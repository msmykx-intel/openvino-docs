
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Post-Training Quantization with TensorFlow Classification Model &#8212; OpenVINO™  documentation</title>
    
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="../_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="../_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/tabs.css" type="text/css" />
    <script src="../_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="../_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="../_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/tabs.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/custom.js"></script>
    <script src="../_static/js/graphs.js"></script>
    <script src="../_static/js/graphs_ov_tf.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/target-highlight.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://docs.openvino.ai/latest/notebooks/301-tensorflow-training-openvino-pot-with-output.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="From Training to Deployment with TensorFlow and OpenVINO™" href="301-tensorflow-training-openvino-with-output.html" />
    <link rel="prev" title="GPT-2 Text Prediction with OpenVINO" href="223-gpt2-text-prediction-with-output.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/get-started-guide.html">
  Get Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/documentation.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api/api_reference.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../model_zoo.html">
  Model Zoo
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/resources.html">
  Resources
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/openvinotoolkit/openvino" rel="noopener" target="_blank" title="GitHub">
            <span><i class="sst-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="language-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">English</button>
  <div class="dropdown-menu" aria-labelledby="language-selector">
    
      
        <a class="dropdown-item font-weight-bold" href="/openvino-docs/index.html">English</a>
      
    
      
        <a  class="dropdown-item" href="/cn/openvino-docs/index.html">Chinese</a>
      
    
  </div>
</div>

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="../search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notebooks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks-installation.html">
   Installation of OpenVINO™ Notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="001-hello-world-with-output.html">
   Hello Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002-openvino-api-with-output.html">
   OpenVINO™ Runtime API Tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003-hello-segmentation-with-output.html">
   Hello Image Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="004-hello-detection-with-output.html">
   Hello Object Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="101-tensorflow-to-openvino-with-output.html">
   Convert a TensorFlow Model to OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="102-pytorch-onnx-to-openvino-with-output.html">
   Convert a PyTorch Model to ONNX and OpenVINO™ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="103-paddle-onnx-to-openvino-classification-with-output.html">
   Convert a PaddlePaddle Model to ONNX and OpenVINO™ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="104-model-tools-with-output.html">
   Working with Open Model Zoo Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="105-language-quantize-bert-with-output.html">
   Quantize NLP models with Post-Training Optimization Tool ​in OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="106-auto-device-with-output.html">
   Automatic Device Selection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="107-speech-recognition-quantization-with-output.html">
   Quantize Speech Recognition Models with OpenVINO™ Post-Training Optimization Tool ​
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-nncf-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="111-detection-quantization-with-output.html">
   Object Detection Quantization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="112-pytorch-post-training-quantization-nncf-with-output.html">
   Post-Training Quantization of PyTorch models with NNCF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="113-image-classification-quantization-with-output.html">
   Quantization of Image Classification Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="114-quantization-simplified-mode-with-output.html">
   INT8 Quantization with Post-training Optimization Tool (POT) in Simplified Mode tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="115-async-api-with-output.html">
   Asynchronous Inference with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="201-vision-monodepth-with-output.html">
   Monodepth Estimation with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-image-with-output.html">
   Single Image Super Resolution with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-video-with-output.html">
   Video Super Resolution with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="203-meter-reader-with-output.html">
   Industrial Meter Reader
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="204-named-entity-recognition-with-output.html">
   Document Entity Extraction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="205-vision-background-removal-with-output.html">
   Image Background Removal with U^2-Net and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="206-vision-paddlegan-anime-with-output.html">
   Photos to Anime with PaddleGAN and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="207-vision-paddlegan-superresolution-with-output.html">
   Super Resolution with PaddleGAN and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="208-optical-character-recognition-with-output.html">
   Optical Character Recognition (OCR) with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="209-handwritten-ocr-with-output.html">
   Handwritten Chinese and Japanese OCR with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="210-ct-scan-live-inference-with-output.html">
   Live Inference and Benchmark CT-scan Data with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="211-speech-to-text-with-output.html">
   Speech to Text with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="212-onnx-style-transfer-with-output.html">
   Style Transfer on ONNX Models with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="214-vision-paddle-classification-with-output.html">
   PaddlePaddle Image Classification with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="215-image-inpainting-with-output.html">
   Image In-painting with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="216-license-plate-recognition-with-output.html">
   License Plate Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="217-vision-deblur-with-output.html">
   Deblur Photos with DeblurGAN-v2 and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="218-vehicle-detection-and-recognition-with-output.html">
   Vehicle Detection And Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="219-knowledge-graphs-conve-with-output.html">
   OpenVINO optimizations for Knowledge graphs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="220-yolov5-accuracy-check-and-quantization-with-output.html">
   Quantize the Ultralytics YOLOv5 model and check accuracy using the OpenVINO POT API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="221-machine-translation-with-output.html">
   Machine translation demo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="222-vision-image-colorization-with-output.html">
   Image Colorization with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="223-gpt2-text-prediction-with-output.html">
   GPT-2 Text Prediction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Post-Training Quantization with TensorFlow Classification Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-with-output.html">
   From Training to Deployment with TensorFlow and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="302-pytorch-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using PyTorch framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="305-tensorflow-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using TensorFlow Framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="401-object-detection-with-output.html">
   Live Object Detection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="402-pose-estimation-with-output.html">
   Live Human Pose Estimation with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="403-action-recognition-webcam-with-output.html">
   Human Action Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="405-paddle-ocr-webcam-with-output.html">
   PaddleOCR with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebook_utils-with-output.html">
   Notebook Utils
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preparation">
   Preparation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#imports">
     Imports
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#settings">
     Settings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-dataloader-class">
     Create DataLoader Class
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-accuracy-metric-class">
     Create Accuracy Metric Class
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pot-optimization">
   POT Optimization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#run-inference-on-quantized-model">
   Run Inference on Quantized Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#compare-inference-speed">
   Compare Inference Speed
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">

<div class="tocsection editthispage">
    <a href="None">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

            
                <div>
                  
  <section id="post-training-quantization-with-tensorflow-classification-model">
<h1>Post-Training Quantization with TensorFlow Classification Model<a class="headerlink" href="#post-training-quantization-with-tensorflow-classification-model" title="Permalink to this headline">¶</a></h1>
<p>This example demonstrates how to quantize the OpenVINO model that was
created in
<a class="reference external" href="301-tensorflow-training-openvino.ipynb">301-tensorflow-training-openvino.ipynb</a>,
to improve inference speed. Quantization is performed with
<a class="reference external" href="https://docs.openvino.ai/nightly/pot_README.html">Post-Training Optimization Tool
(POT)</a>. A custom
dataloader and metric will be defined, and accuracy and performance will
be computed for the original IR model and the quantized model.</p>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline">¶</a></h2>
<p>The notebook requires that the training notebook has been run and that
the Intermediate Representation (IR) models are created. If the IR
models do not exist, running the next cell will run the training
notebook. This will take a while.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from pathlib import Path

import tensorflow as tf

model_xml = Path(&quot;model/flower/flower_ir.xml&quot;)
dataset_url = (
    &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;
)
data_dir = Path(tf.keras.utils.get_file(&quot;flower_photos&quot;, origin=dataset_url, untar=True))

if not model_xml.exists():
    print(&quot;Executing training notebook. This will take a while...&quot;)
    %run 301-tensorflow-training-openvino.ipynb
</pre></div>
</div>
<section id="imports">
<h3>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">¶</a></h3>
<p>The Post Training Optimization API is implemented in the <code class="docutils literal notranslate"><span class="pre">compression</span></code>
library.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import copy
import os
import sys

import cv2
import matplotlib.pyplot as plt
import numpy as np
from addict import Dict
from openvino.tools.pot.api import Metric, DataLoader
from openvino.tools.pot.graph import load_model, save_model
from openvino.tools.pot.graph.model_utils import compress_model_weights
from openvino.tools.pot.engines.ie_engine import IEEngine
from openvino.tools.pot.pipeline.initializer import create_pipeline
from openvino.runtime import Core
from PIL import Image

sys.path.append(&quot;../utils&quot;)
from notebook_utils import benchmark_model, download_file
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/.</span><span class="n">venv</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">defusedxml</span><span class="o">/</span><span class="fm">__init__</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">30</span><span class="p">:</span> <span class="ne">DeprecationWarning</span><span class="p">:</span> <span class="n">defusedxml</span><span class="o">.</span><span class="n">cElementTree</span> <span class="ow">is</span> <span class="n">deprecated</span><span class="p">,</span> <span class="kn">import</span> <span class="nn">from</span> <span class="n">defusedxml</span><span class="o">.</span><span class="n">ElementTree</span> <span class="n">instead</span><span class="o">.</span>
  <span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">cElementTree</span>
</pre></div>
</div>
</section>
<section id="settings">
<h3>Settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h3>
<p>In the next cell, the settings for running quantization are defined. The
default settings use the <em>mixed</em> preset and the <em>DefaultQuantization</em>
algorithm. This enables reasonably fast quantization, with possible drop
in accuracy. The <em>performance</em> preset can result in faster inference on
the quantized model, the <em>AccuracyAwareQuantization</em> algorithm quantizes
the model to a defined maximal accuracy drop, which may not achieve the
greatest performance boost but avoids further drop in accuracy.</p>
<p>See the <a class="reference external" href="https://docs.openvino.ai/latest/pot_docs_BestPractices.html">Post-Training Optimization Best
Practices</a>
page for more information about the configurable parameters and best
practices for post-training quantization.</p>
<p>The POT methods expect configuration dictionaries as arguments. They are
defined in the cell below.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model_config = Dict(
    {
        &quot;model_name&quot;: &quot;flower&quot;,
        &quot;model&quot;: &quot;model/flower/flower_ir.xml&quot;,
        &quot;weights&quot;: &quot;model/flower/flower_ir.bin&quot;,
    }
)

engine_config = Dict({&quot;device&quot;: &quot;CPU&quot;, &quot;stat_requests_number&quot;: 2, &quot;eval_requests_number&quot;: 2})

algorithms = [
    {
        &quot;name&quot;: &quot;DefaultQuantization&quot;,
        &quot;params&quot;: {
            &quot;target_device&quot;: &quot;CPU&quot;,
            &quot;preset&quot;: &quot;performance&quot;,
            &quot;stat_subset_size&quot;: 1000,
        },
    }
]
</pre></div>
</div>
</section>
<section id="create-dataloader-class">
<h3>Create DataLoader Class<a class="headerlink" href="#create-dataloader-class" title="Permalink to this headline">¶</a></h3>
<p>OpenVINO’s compression library contains a DataLoader class. The
DataLoader defines how to load data and annotations. For the TensorFlow
flowers dataset, images are stored in a directory per category. The
DataLoader loads images from a given <em>data_source</em> directory and assigns
a label based on the position of the directory in <em>class_names</em> (where
class_names is a list of directory names in alphabetical order).</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class ClassificationDataLoader(DataLoader):
    &quot;&quot;&quot;
    DataLoader for image data that is stored in a directory per category. For example, for
    categories _rose_ and _daisy_, rose images are expected in data_source/rose, daisy images
    in data_source/daisy.
    &quot;&quot;&quot;

    def __init__(self, data_source):
        &quot;&quot;&quot;
        :param data_source: path to data directory
        &quot;&quot;&quot;
        self.data_source = Path(data_source)
        self.dataset = [p for p in data_dir.glob(&quot;**/*&quot;) if p.suffix in (&quot;.png&quot;, &quot;.jpg&quot;)]
        self.class_names = sorted([item.name for item in Path(data_dir).iterdir() if item.is_dir()])

    def __len__(self):
        &quot;&quot;&quot;
        Returns the number of elements in the dataset
        &quot;&quot;&quot;
        return len(self.dataset)

    def __getitem__(self, index):
        &quot;&quot;&quot;
        Get item from self.dataset at the specified index.
        Returns (annotation, image), where annotation is a tuple (index, class_index)
        and image a preprocessed image in network shape
        &quot;&quot;&quot;
        if index &gt;= len(self):
            raise IndexError
        filepath = self.dataset[index]
        annotation = (index, self.class_names.index(filepath.parent.name))
        image = self._read_image(filepath)
        return annotation, image

    def _read_image(self, index):
        &quot;&quot;&quot;
        Read image at dataset[index] to memory, resize, convert to BGR and to network shape

        :param index: dataset index to read
        :return ndarray representation of image batch
        &quot;&quot;&quot;
        image = cv2.imread(os.path.join(self.data_source, index))[:, :, (2, 1, 0)]
        image = cv2.resize(image, (180, 180)).astype(np.float32)
        return image
</pre></div>
</div>
</section>
<section id="create-accuracy-metric-class">
<h3>Create Accuracy Metric Class<a class="headerlink" href="#create-accuracy-metric-class" title="Permalink to this headline">¶</a></h3>
<p>The accuracy metric is defined as the number of correct predictions
divided by the total number of predictions. It is used to validate the
accuracy of the quantized model.</p>
<p>The Accuracy class in this tutorial implements the <code class="docutils literal notranslate"><span class="pre">Metric</span></code> interface
of the compression library.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class Accuracy(Metric):
    def __init__(self):
        super().__init__()
        self._name = &quot;accuracy&quot;
        self._matches = []

    @property
    def value(self):
        &quot;&quot;&quot;Returns accuracy metric value for the last model output.&quot;&quot;&quot;
        return {self._name: self._matches[-1]}

    @property
    def avg_value(self):
        &quot;&quot;&quot;
        Returns accuracy metric value for all model outputs. Results per image are stored in
        self._matches, where True means a correct prediction and False a wrong prediction.
        Accuracy is computed as the number of correct predictions divided by the total
        number of predictions.
        &quot;&quot;&quot;
        num_correct = np.count_nonzero(self._matches)
        return {self._name: num_correct / len(self._matches)}

    def update(self, output, target):
        &quot;&quot;&quot;Updates prediction matches.

        :param output: model output
        :param target: annotations
        &quot;&quot;&quot;
        predict = np.argmax(output[0], axis=1)
        match = predict == target
        self._matches.append(match)

    def reset(self):
        &quot;&quot;&quot;
        Resets the Accuracy metric. This is a required method that should initialize all
        attributes to their initial value.
        &quot;&quot;&quot;
        self._matches = []

    def get_attributes(self):
        &quot;&quot;&quot;
        Returns a dictionary of metric attributes {metric_name: {attribute_name: value}}.
        Required attributes: &#39;direction&#39;: &#39;higher-better&#39; or &#39;higher-worse&#39;
                             &#39;type&#39;: metric type
        &quot;&quot;&quot;
        return {self._name: {&quot;direction&quot;: &quot;higher-better&quot;, &quot;type&quot;: &quot;accuracy&quot;}}
</pre></div>
</div>
</section>
</section>
<section id="pot-optimization">
<h2>POT Optimization<a class="headerlink" href="#pot-optimization" title="Permalink to this headline">¶</a></h2>
<p>After creating the DataLoader and Metric classes, and defining the
configuration settings for POT, we can start the quantization process.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Step 1: Load the model
model = load_model(model_config=model_config)
original_model = copy.deepcopy(model)

# Step 2: Initialize the data loader
data_loader = ClassificationDataLoader(data_source=data_dir)

# Step 3 (Optional. Required for AccuracyAwareQuantization): Initialize the metric
#        Compute metric results on original model
metric = Accuracy()

# Step 4: Initialize the engine for metric calculation and statistics collection
engine = IEEngine(config=engine_config, data_loader=data_loader, metric=metric)

# Step 5: Create a pipeline of compression algorithms
pipeline = create_pipeline(algo_config=algorithms, engine=engine)

# Step 6: Execute the pipeline
compressed_model = pipeline.run(model=model)

# Step 7 (Optional): Compress model weights quantized precision
#                    in order to reduce the size of final .bin file
compress_model_weights(model=compressed_model)

# Step 8: Save the compressed model and get the path to the model
compressed_model_paths = save_model(
    model=compressed_model, save_path=os.path.join(os.path.curdir, &quot;model/optimized&quot;)
)
compressed_model_xml = Path(compressed_model_paths[0][&quot;model&quot;])
print(f&quot;The quantized model is stored in {compressed_model_xml}&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">quantized</span> <span class="n">model</span> <span class="ow">is</span> <span class="n">stored</span> <span class="ow">in</span> <span class="n">model</span><span class="o">/</span><span class="n">optimized</span><span class="o">/</span><span class="n">flower_ir</span><span class="o">.</span><span class="n">xml</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Step 9 (Optional): Evaluate the original and compressed model. Print the results
original_metric_results = pipeline.evaluate(original_model)
if original_metric_results:
    print(f&quot;Accuracy of the original model:  {next(iter(original_metric_results.values())):.5f}&quot;)

quantized_metric_results = pipeline.evaluate(compressed_model)
if quantized_metric_results:
    print(f&quot;Accuracy of the quantized model: {next(iter(quantized_metric_results.values())):.5f}&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Accuracy</span> <span class="n">of</span> <span class="n">the</span> <span class="n">original</span> <span class="n">model</span><span class="p">:</span>  <span class="mf">0.80218</span>
<span class="n">Accuracy</span> <span class="n">of</span> <span class="n">the</span> <span class="n">quantized</span> <span class="n">model</span><span class="p">:</span> <span class="mf">0.79891</span>
</pre></div>
</div>
</section>
<section id="run-inference-on-quantized-model">
<h2>Run Inference on Quantized Model<a class="headerlink" href="#run-inference-on-quantized-model" title="Permalink to this headline">¶</a></h2>
<p>Copy the preprocess function from the training notebook and run
inference on the quantized model with Inference Engine. See the
<a class="reference external" href="002-openvino-api-with-output.html">OpenVINO API tutorial</a>
for more information about running inference with Inference Engine
Python API.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def pre_process_image(imagePath, img_height=180):
    # Model input format
    n, c, h, w = [1, 3, img_height, img_height]
    image = Image.open(imagePath)
    image = image.resize((h, w), resample=Image.BILINEAR)

    # Convert to array and change data layout from HWC to CHW
    image = np.array(image)

    input_image = image.reshape((n, h, w, c))

    return input_image
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Load the optimized model and get the names of the input and output layer
ie = Core()
model_pot = ie.read_model(model=&quot;model/optimized/flower_ir.xml&quot;)
compiled_model_pot = ie.compile_model(model=model_pot, device_name=&quot;CPU&quot;)
input_layer = compiled_model_pot.input(0)
output_layer = compiled_model_pot.output(0)

# Get the class names: a list of directory names in alphabetical order
class_names = sorted([item.name for item in Path(data_dir).iterdir() if item.is_dir()])

# Run inference on an input image...
inp_img_url = (
    &quot;https://upload.wikimedia.org/wikipedia/commons/4/48/A_Close_Up_Photo_of_a_Dandelion.jpg&quot;
)
directory = &quot;output&quot;
inp_file_name = &quot;A_Close_Up_Photo_of_a_Dandelion.jpg&quot;
file_path = Path(directory)/Path(inp_file_name)
# Download the image if it does not exist yet
if not Path(inp_file_name).exists():
    download_file(inp_img_url, inp_file_name, directory=directory)

# Pre-process the image and get it ready for inference.
input_image = pre_process_image(imagePath=file_path)
print(f&#39;input image shape: {input_image.shape}&#39;)
print(f&#39;input layer shape: {input_layer.shape}&#39;)

res = compiled_model_pot([input_image])[output_layer]

score = tf.nn.softmax(res[0])

# Show the results
image = Image.open(file_path)
plt.imshow(image)
print(
    &quot;This image most likely belongs to {} with a {:.2f} percent confidence.&quot;.format(
        class_names[np.argmax(score)], 100 * np.max(score)
    )
)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;output/A_Close_Up_Photo_of_a_Dandelion.jpg&#39;</span> <span class="n">already</span> <span class="n">exists</span><span class="o">.</span>
<span class="nb">input</span> <span class="n">image</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">input</span> <span class="n">layer</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mi">3</span><span class="p">}</span>
<span class="n">This</span> <span class="n">image</span> <span class="n">most</span> <span class="n">likely</span> <span class="n">belongs</span> <span class="n">to</span> <span class="n">dandelion</span> <span class="k">with</span> <span class="n">a</span> <span class="mf">99.91</span> <span class="n">percent</span> <span class="n">confidence</span><span class="o">.</span>
</pre></div>
</div>
<img alt="../_images/301-tensorflow-training-openvino-pot-with-output_16_1.png" src="../_images/301-tensorflow-training-openvino-pot-with-output_16_1.png" />
</section>
<section id="compare-inference-speed">
<h2>Compare Inference Speed<a class="headerlink" href="#compare-inference-speed" title="Permalink to this headline">¶</a></h2>
<p>Measure inference speed with the <a class="reference external" href="https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html">OpenVINO Benchmark
App</a>.</p>
<p>Benchmark App is a command line tool that measures raw inference
performance for a specified OpenVINO IR model. Run
<code class="docutils literal notranslate"><span class="pre">benchmark_app</span> <span class="pre">--help</span></code> to see a list of available parameters. By
default, Benchmark App tests the performance of the model specified with
the <code class="docutils literal notranslate"><span class="pre">-m</span></code> parameter with asynchronous inference on CPU, for one minute.
Use the <code class="docutils literal notranslate"><span class="pre">-d</span></code> parameter to test performance on a different device, for
example an Intel integrated Graphics (iGPU), and <code class="docutils literal notranslate"><span class="pre">-t</span></code> to set the
number of seconds to run inference. See the
<a class="reference external" href="https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html">documentation</a>
for more information.</p>
<p>In this tutorial, we use a wrapper function from <a class="reference external" href="https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/utils/notebook_utils.ipynb">Notebook
Utils</a>.
It prints the <code class="docutils literal notranslate"><span class="pre">benchmark_app</span></code> command with the chosen parameters.</p>
<p>In the next cells, inference speed will be measured for the original and
quantized model on CPU. If an iGPU is available, inference speed will be
measured for CPU+GPU as well. The number of seconds is set to 15.</p>
<blockquote>
<div><p>NOTE: For the most accurate performance estimation, we recommended
running <code class="docutils literal notranslate"><span class="pre">benchmark_app</span></code> in a terminal/command prompt after closing
other applications.</p>
</div></blockquote>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># print the available devices on this system
ie = Core()
print(&quot;Device information:&quot;)
print(ie.get_property(&quot;CPU&quot;, &quot;FULL_DEVICE_NAME&quot;))
if &quot;GPU&quot; in ie.available_devices:
    print(ie.get_property(&quot;GPU&quot;, &quot;FULL_DEVICE_NAME&quot;))
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Device</span> <span class="n">information</span><span class="p">:</span>
<span class="n">Intel</span><span class="p">(</span><span class="n">R</span><span class="p">)</span> <span class="n">Core</span><span class="p">(</span><span class="n">TM</span><span class="p">)</span> <span class="n">i9</span><span class="o">-</span><span class="mi">10920</span><span class="n">X</span> <span class="n">CPU</span> <span class="o">@</span> <span class="mf">3.50</span><span class="n">GHz</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Original model - CPU
benchmark_model(model_path=model_xml, device=&quot;CPU&quot;, seconds=15, api=&#39;async&#39;)
</pre></div>
</div>
<p><strong>Benchmark flower_ir.xml with CPU for 15 seconds with async inference</strong></p>
<p>Benchmark command:
<code class="docutils literal notranslate"><span class="pre">benchmark_app</span> <span class="pre">-m</span> <span class="pre">model/flower/flower_ir.xml</span> <span class="pre">-d</span> <span class="pre">CPU</span> <span class="pre">-t</span> <span class="pre">15</span> <span class="pre">-api</span> <span class="pre">async</span> <span class="pre">-b</span> <span class="pre">1</span> <span class="pre">-cdir</span> <span class="pre">model_cache</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Count</span><span class="p">:</span>          <span class="mi">53460</span> <span class="n">iterations</span>
<span class="n">Duration</span><span class="p">:</span>       <span class="mf">15002.92</span> <span class="n">ms</span>
<span class="n">Latency</span><span class="p">:</span>
    <span class="n">Median</span><span class="p">:</span>     <span class="mf">3.22</span> <span class="n">ms</span>
    <span class="n">AVG</span><span class="p">:</span>        <span class="mf">3.24</span> <span class="n">ms</span>
    <span class="n">MIN</span><span class="p">:</span>        <span class="mf">1.40</span> <span class="n">ms</span>
    <span class="n">MAX</span><span class="p">:</span>        <span class="mf">10.69</span> <span class="n">ms</span>
<span class="n">Throughput</span><span class="p">:</span> <span class="mf">3563.31</span> <span class="n">FPS</span>

<span class="n">Device</span><span class="p">:</span> <span class="n">Intel</span><span class="p">(</span><span class="n">R</span><span class="p">)</span> <span class="n">Core</span><span class="p">(</span><span class="n">TM</span><span class="p">)</span> <span class="n">i9</span><span class="o">-</span><span class="mi">10920</span><span class="n">X</span> <span class="n">CPU</span> <span class="o">@</span> <span class="mf">3.50</span><span class="n">GHz</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Quantized model - CPU
benchmark_model(model_path=compressed_model_xml, device=&quot;CPU&quot;, seconds=15, api=&#39;async&#39;)
</pre></div>
</div>
<p><strong>Benchmark flower_ir.xml with CPU for 15 seconds with async inference</strong></p>
<p>Benchmark command:
<code class="docutils literal notranslate"><span class="pre">benchmark_app</span> <span class="pre">-m</span> <span class="pre">model/optimized/flower_ir.xml</span> <span class="pre">-d</span> <span class="pre">CPU</span> <span class="pre">-t</span> <span class="pre">15</span> <span class="pre">-api</span> <span class="pre">async</span> <span class="pre">-b</span> <span class="pre">1</span> <span class="pre">-cdir</span> <span class="pre">model_cache</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Count</span><span class="p">:</span>          <span class="mi">199104</span> <span class="n">iterations</span>
<span class="n">Duration</span><span class="p">:</span>       <span class="mf">15000.77</span> <span class="n">ms</span>
<span class="n">Latency</span><span class="p">:</span>
    <span class="n">Median</span><span class="p">:</span>     <span class="mf">0.87</span> <span class="n">ms</span>
    <span class="n">AVG</span><span class="p">:</span>        <span class="mf">0.87</span> <span class="n">ms</span>
    <span class="n">MIN</span><span class="p">:</span>        <span class="mf">0.59</span> <span class="n">ms</span>
    <span class="n">MAX</span><span class="p">:</span>        <span class="mf">6.97</span> <span class="n">ms</span>
<span class="n">Throughput</span><span class="p">:</span> <span class="mf">13272.92</span> <span class="n">FPS</span>

<span class="n">Device</span><span class="p">:</span> <span class="n">Intel</span><span class="p">(</span><span class="n">R</span><span class="p">)</span> <span class="n">Core</span><span class="p">(</span><span class="n">TM</span><span class="p">)</span> <span class="n">i9</span><span class="o">-</span><span class="mi">10920</span><span class="n">X</span> <span class="n">CPU</span> <span class="o">@</span> <span class="mf">3.50</span><span class="n">GHz</span>
</pre></div>
</div>
<p><strong>Benchmark on MULTI:CPU,GPU</strong></p>
<p>With a recent Intel CPU, the best performance can often be achieved by
doing inference on both the CPU and the iGPU, with OpenVINO’s <a class="reference external" href="https://docs.openvino.ai/2021.4/openvino_docs_IE_DG_supported_plugins_MULTI.html">Multi
Device
Plugin</a>.
It takes a bit longer to load a model on GPU than on CPU, so this
benchmark will take a bit longer to complete than the CPU benchmark,
when run for the first time. Benchmark App supports caching, by
specifying the <code class="docutils literal notranslate"><span class="pre">--cdir</span></code> parameter. In the cells below, the model will
cached to the <code class="docutils literal notranslate"><span class="pre">model_cache</span></code> directory.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Original model - MULTI:CPU,GPU
if &quot;GPU&quot; in ie.available_devices:
    benchmark_model(model_path=model_xml, device=&quot;MULTI:CPU,GPU&quot;, seconds=15, api=&#39;async&#39;)
else:
    print(&quot;A supported integrated GPU is not available on this system.&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="n">supported</span> <span class="n">integrated</span> <span class="n">GPU</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">available</span> <span class="n">on</span> <span class="n">this</span> <span class="n">system</span><span class="o">.</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Quantized model - MULTI:CPU,GPU
if &quot;GPU&quot; in ie.available_devices:
    benchmark_model(model_path=compressed_model_xml, device=&quot;MULTI:CPU,GPU&quot;, seconds=15, api=&#39;async&#39;)
else:
    print(&quot;A supported integrated GPU is not available on this system.&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="n">supported</span> <span class="n">integrated</span> <span class="n">GPU</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">available</span> <span class="n">on</span> <span class="n">this</span> <span class="n">system</span><span class="o">.</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># print the available devices on this system
print(&quot;Device information:&quot;)
print(ie.get_property(&quot;CPU&quot;, &quot;FULL_DEVICE_NAME&quot;))
if &quot;GPU&quot; in ie.available_devices:
    print(ie.get_property(&quot;GPU&quot;, &quot;FULL_DEVICE_NAME&quot;))
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Device</span> <span class="n">information</span><span class="p">:</span>
<span class="n">Intel</span><span class="p">(</span><span class="n">R</span><span class="p">)</span> <span class="n">Core</span><span class="p">(</span><span class="n">TM</span><span class="p">)</span> <span class="n">i9</span><span class="o">-</span><span class="mi">10920</span><span class="n">X</span> <span class="n">CPU</span> <span class="o">@</span> <span class="mf">3.50</span><span class="n">GHz</span>
</pre></div>
</div>
<p><strong>Original IR model - CPU</strong></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>benchmark_output = %sx benchmark_app -m $model_xml -t 15 -api async
# Remove logging info from benchmark_app output and show only the results
benchmark_result = [line for line in benchmark_output if not (line.startswith(r&quot;[&quot;) or line.startswith(&quot;  &quot;) or line==&quot;&quot;)]
print(&quot;\n&quot;.join(benchmark_result))
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Count</span><span class="p">:</span>          <span class="mi">57816</span> <span class="n">iterations</span>
<span class="n">Duration</span><span class="p">:</span>       <span class="mf">15001.82</span> <span class="n">ms</span>
<span class="n">Latency</span><span class="p">:</span>
<span class="n">Throughput</span><span class="p">:</span> <span class="mf">3853.93</span> <span class="n">FPS</span>
</pre></div>
</div>
<p><strong>Quantized IR model - CPU</strong></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>benchmark_output = %sx benchmark_app -m $compressed_model_xml -t 15 -api async
# Remove logging info from benchmark_app output and show only the results
benchmark_result = [line for line in benchmark_output if not (line.startswith(r&quot;[&quot;) or line.startswith(&quot;  &quot;) or line==&quot;&quot;)]
print(&quot;\n&quot;.join(benchmark_result))
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Count</span><span class="p">:</span>          <span class="mi">159030</span> <span class="n">iterations</span>
<span class="n">Duration</span><span class="p">:</span>       <span class="mf">15000.94</span> <span class="n">ms</span>
<span class="n">Latency</span><span class="p">:</span>
<span class="n">Throughput</span><span class="p">:</span> <span class="mf">10601.34</span> <span class="n">FPS</span>
</pre></div>
</div>
<p><strong>Original IR model - MULTI:CPU,GPU</strong></p>
<p>With a recent Intel CPU, the best performance can often be achieved by
doing inference on both the CPU and the iGPU, with OpenVINO’s <a class="reference external" href="https://docs.openvino.ai/latest/openvino_docs_OV_UG_Running_on_multiple_devices.html">Multi
Device
Plugin</a>.
It takes a bit longer to load a model on GPU than on CPU, so this
benchmark will take a bit longer to complete than the CPU benchmark.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>ie = Core()
if &quot;GPU&quot; in ie.available_devices:
    benchmark_output = %sx benchmark_app -m $model_xml -d MULTI:CPU,GPU -t 15 -api async
    # Remove logging info from benchmark_app output and show only the results
    benchmark_result = [line for line in benchmark_output if not (line.startswith(r&quot;[&quot;) or line.startswith(&quot;  &quot;) or line==&quot;&quot;)]
    print(&quot;\n&quot;.join(benchmark_result))
else:
    print(&quot;An integrated GPU is not available on this system.&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">An</span> <span class="n">integrated</span> <span class="n">GPU</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">available</span> <span class="n">on</span> <span class="n">this</span> <span class="n">system</span><span class="o">.</span>
</pre></div>
</div>
<p><strong>Quantized IR model - MULTI:CPU,GPU</strong></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>ie = Core()
if &quot;GPU&quot; in ie.available_devices:
    benchmark_output = %sx benchmark_app -m $compressed_model_xml -d MULTI:CPU,GPU -t 15 -api async
    # Remove logging info from benchmark_app output and show only the results
    benchmark_result = [line for line in benchmark_output if not (line.startswith(r&quot;[&quot;) or line.startswith(&quot;  &quot;) or line==&quot;&quot;)]
    print(&quot;\n&quot;.join(benchmark_result))
else:
    print(&quot;An integrated GPU is not available on this system.&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">An</span> <span class="n">integrated</span> <span class="n">GPU</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">available</span> <span class="n">on</span> <span class="n">this</span> <span class="n">system</span><span class="o">.</span>
</pre></div>
</div>
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  
    <a class='button bttn-sec button-size-l' id="prev-link" href="223-gpt2-text-prediction-with-output.html" title="previous page">Prev</a>
    <a class='button bttn-sec button-size-l' id="next-link" href="301-tensorflow-training-openvino-with-output.html" title="next page">Next</a>

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel®.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>