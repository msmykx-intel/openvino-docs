
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>OpenVINO optimizations for Knowledge graphs &#8212; OpenVINO™  documentation</title>
    
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="../_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="../_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/tabs.css" type="text/css" />
    <script src="../_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="../_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="../_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/tabs.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/custom.js"></script>
    <script src="../_static/js/graphs.js"></script>
    <script src="../_static/js/graphs_ov_tf.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/target-highlight.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://docs.openvino.ai/latest/notebooks/219-knowledge-graphs-conve-with-output.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Quantize the Ultralytics YOLOv5 model and check accuracy using the OpenVINO POT API" href="220-yolov5-accuracy-check-and-quantization-with-output.html" />
    <link rel="prev" title="Vehicle Detection And Recognition with OpenVINO™" href="218-vehicle-detection-and-recognition-with-output.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/get-started-guide.html">
  Get Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/documentation.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api/api_reference.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../model_zoo.html">
  Model Zoo
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/resources.html">
  Resources
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/openvinotoolkit/openvino" rel="noopener" target="_blank" title="GitHub">
            <span><i class="sst-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="language-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">English</button>
  <div class="dropdown-menu" aria-labelledby="language-selector">
    
      
        <a class="dropdown-item font-weight-bold" href="/openvino-docs/index.html">English</a>
      
    
      
        <a  class="dropdown-item" href="/cn/openvino-docs/index.html">Chinese</a>
      
    
  </div>
</div>

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="../search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notebooks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks-installation.html">
   Installation of OpenVINO™ Notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="001-hello-world-with-output.html">
   Hello Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002-openvino-api-with-output.html">
   OpenVINO™ Runtime API Tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003-hello-segmentation-with-output.html">
   Hello Image Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="004-hello-detection-with-output.html">
   Hello Object Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="101-tensorflow-to-openvino-with-output.html">
   Convert a TensorFlow Model to OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="102-pytorch-onnx-to-openvino-with-output.html">
   Convert a PyTorch Model to ONNX and OpenVINO™ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="103-paddle-onnx-to-openvino-classification-with-output.html">
   Convert a PaddlePaddle Model to ONNX and OpenVINO™ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="104-model-tools-with-output.html">
   Working with Open Model Zoo Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="105-language-quantize-bert-with-output.html">
   Quantize NLP models with Post-Training Optimization Tool ​in OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="106-auto-device-with-output.html">
   Automatic Device Selection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="107-speech-recognition-quantization-with-output.html">
   Quantize Speech Recognition Models with OpenVINO™ Post-Training Optimization Tool ​
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-nncf-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="111-detection-quantization-with-output.html">
   Object Detection Quantization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="112-pytorch-post-training-quantization-nncf-with-output.html">
   Post-Training Quantization of PyTorch models with NNCF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="113-image-classification-quantization-with-output.html">
   Quantization of Image Classification Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="114-quantization-simplified-mode-with-output.html">
   INT8 Quantization with Post-training Optimization Tool (POT) in Simplified Mode tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="115-async-api-with-output.html">
   Asynchronous Inference with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="201-vision-monodepth-with-output.html">
   Monodepth Estimation with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-image-with-output.html">
   Single Image Super Resolution with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-video-with-output.html">
   Video Super Resolution with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="203-meter-reader-with-output.html">
   Industrial Meter Reader
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="204-named-entity-recognition-with-output.html">
   Document Entity Extraction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="205-vision-background-removal-with-output.html">
   Image Background Removal with U^2-Net and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="206-vision-paddlegan-anime-with-output.html">
   Photos to Anime with PaddleGAN and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="207-vision-paddlegan-superresolution-with-output.html">
   Super Resolution with PaddleGAN and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="208-optical-character-recognition-with-output.html">
   Optical Character Recognition (OCR) with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="209-handwritten-ocr-with-output.html">
   Handwritten Chinese and Japanese OCR with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="210-ct-scan-live-inference-with-output.html">
   Live Inference and Benchmark CT-scan Data with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="211-speech-to-text-with-output.html">
   Speech to Text with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="212-onnx-style-transfer-with-output.html">
   Style Transfer on ONNX Models with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="214-vision-paddle-classification-with-output.html">
   PaddlePaddle Image Classification with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="215-image-inpainting-with-output.html">
   Image In-painting with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="216-license-plate-recognition-with-output.html">
   License Plate Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="217-vision-deblur-with-output.html">
   Deblur Photos with DeblurGAN-v2 and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="218-vehicle-detection-and-recognition-with-output.html">
   Vehicle Detection And Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   OpenVINO optimizations for Knowledge graphs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="220-yolov5-accuracy-check-and-quantization-with-output.html">
   Quantize the Ultralytics YOLOv5 model and check accuracy using the OpenVINO POT API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="221-machine-translation-with-output.html">
   Machine translation demo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="222-vision-image-colorization-with-output.html">
   Image Colorization with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="223-gpt2-text-prediction-with-output.html">
   GPT-2 Text Prediction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-pot-with-output.html">
   Post-Training Quantization with TensorFlow Classification Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-with-output.html">
   From Training to Deployment with TensorFlow and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="302-pytorch-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using PyTorch framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="305-tensorflow-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using TensorFlow Framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="401-object-detection-with-output.html">
   Live Object Detection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="402-pose-estimation-with-output.html">
   Live Human Pose Estimation with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="403-action-recognition-webcam-with-output.html">
   Human Action Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="405-paddle-ocr-webcam-with-output.html">
   PaddleOCR with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebook_utils-with-output.html">
   Notebook Utils
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   OpenVINO optimizations for Knowledge graphs
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#windows-specific-settings">
   Windows specific settings
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#import-the-packages-needed-for-successful-execution">
   Import the packages needed for successful execution
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#settings-including-path-to-the-serialized-model-files-and-input-data-files">
     Settings: Including path to the serialized model files and input data files
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#defining-the-conve-model-class">
     Defining the ConvE model class
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#defining-the-dataloader">
     Defining the dataloader
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluate-the-trained-conve-model">
     Evaluate the trained ConvE model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prediction-on-the-knowledge-graph">
     Prediction on the Knowledge graph.
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convert-the-trained-pytorch-model-to-onnx-format-for-openvino-inference">
     Convert the trained PyTorch model to ONNX format for OpenVINO inference
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluate-the-model-performance-with-openvino">
     Evaluate the model performance with OpenVINO
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#determine-the-platform-specific-speedup-obtained-through-openvino-graph-optimizations">
     Determine the platform specific speedup obtained through OpenVINO graph optimizations
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#benchmark-the-converted-openvino-model-using-benchmark-app">
     Benchmark the converted OpenVINO model using benchmark app
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conclusions">
     Conclusions
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#references">
     References
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">

<div class="tocsection editthispage">
    <a href="None">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

            
                <div>
                  
  <section id="openvino-optimizations-for-knowledge-graphs">
<h1>OpenVINO optimizations for Knowledge graphs<a class="headerlink" href="#openvino-optimizations-for-knowledge-graphs" title="Permalink to this headline">¶</a></h1>
<p>The goal of this notebook is to showcase performance optimizations for
the ConvE knowledge graph embeddings model using the Intel® Distribution
of OpenVINO™ Toolkit. The optimizations process contains the following
steps: 1. Export the trained model to a format suitable for OpenVINO
optimizations and inference 2. Report the inference performance speedup
obtained with the optimized OpenVINO model</p>
<p>The ConvE model we use is an implementation of the paper Convolutional
2D Knowledge Graph Embeddings (<a class="reference external" href="https://arxiv.org/abs/1707.01476">https://arxiv.org/abs/1707.01476</a>). The
sample dataset was downloaded from:
<a class="reference external" href="https://github.com/TimDettmers/ConvE/tree/master/countries/countries_S1">https://github.com/TimDettmers/ConvE/tree/master/countries/countries_S1</a></p>
</section>
<section id="windows-specific-settings">
<h1>Windows specific settings<a class="headerlink" href="#windows-specific-settings" title="Permalink to this headline">¶</a></h1>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># On Windows, add the directory that contains cl.exe to the PATH
# to enable PyTorch to find the required C++ tools.
# This code assumes that Visual Studio 2019 is installed in the default directory.
# If you have a different C++ compiler, please add the correct path
# to os.environ[&quot;PATH&quot;] directly.
# Note that the C++ Redistributable is not enough to run this notebook.

# Adding the path to os.environ[&quot;LIB&quot;] is not always required
# - it depends on the system&#39;s configuration

import sys

if sys.platform == &quot;win32&quot;:
    import distutils.command.build_ext
    import os
    from pathlib import Path

    VS_INSTALL_DIR = r&quot;C:/Program Files (x86)/Microsoft Visual Studio&quot;
    cl_paths = sorted(list(Path(VS_INSTALL_DIR).glob(&quot;**/Hostx86/x64/cl.exe&quot;)))
    if len(cl_paths) == 0:
        raise ValueError(
            &quot;Cannot find Visual Studio. This notebook requires a C++ compiler. If you installed &quot;
            &quot;a C++ compiler, please add the directory that contains&quot;
            &quot;cl.exe to `os.environ[&#39;PATH&#39;]`.&quot;
        )
    else:
        # If multiple versions of MSVC are installed, get the most recent version
        cl_path = cl_paths[-1]
        vs_dir = str(cl_path.parent)
        os.environ[&quot;PATH&quot;] += f&quot;{os.pathsep}{vs_dir}&quot;
        # Code for finding the library dirs from
        # https://stackoverflow.com/questions/47423246/get-pythons-lib-path
        d = distutils.core.Distribution()
        b = distutils.command.build_ext.build_ext(d)
        b.finalize_options()
        os.environ[&quot;LIB&quot;] = os.pathsep.join(b.library_dirs)
        print(f&quot;Added {vs_dir} to PATH&quot;)
</pre></div>
</div>
</section>
<section id="import-the-packages-needed-for-successful-execution">
<h1>Import the packages needed for successful execution<a class="headerlink" href="#import-the-packages-needed-for-successful-execution" title="Permalink to this headline">¶</a></h1>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import time
import json

import torch
from torch.nn import functional as F, Parameter
from torch.nn.init import xavier_normal_

from pathlib import Path

from sklearn.metrics import accuracy_score

from openvino.runtime import Core
</pre></div>
</div>
<section id="settings-including-path-to-the-serialized-model-files-and-input-data-files">
<h2>Settings: Including path to the serialized model files and input data files<a class="headerlink" href="#settings-including-path-to-the-serialized-model-files-and-input-data-files" title="Permalink to this headline">¶</a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
print(f&quot;Using {device} device&quot;)

# Path to the trained model
modelpath = Path(&#39;models/conve.pt&#39;)

# Path to the file containing the entities and entity IDs
entdatapath = Path(&#39;data/countries_S1/kg_training_entids.txt&#39;)

# Path to the file containing the relations and relation IDs
reldatapath = Path(&#39;data/countries_S1/kg_training_relids.txt&#39;)

# Path to the test data file
testdatapath = Path(&#39;data/countries_S1/e1rel_to_e2_ranking_test.json&#39;)

# Entity and relation embedding dimensions
EMB_DIM = 300

# Top K vals to consider from the predictions
TOP_K = 2

# Required for OpenVINO conversion
output_dir = Path(&quot;models&quot;)
base_model_name = &quot;conve&quot;

output_dir.mkdir(exist_ok=True)

# Paths where PyTorch, ONNX and OpenVINO IR models will be stored
fp32_onnx_path = Path(output_dir / (base_model_name + &quot;_fp32&quot;)).with_suffix(&quot;.onnx&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Using</span> <span class="n">cpu</span> <span class="n">device</span>
</pre></div>
</div>
</section>
<section id="defining-the-conve-model-class">
<h2>Defining the ConvE model class<a class="headerlink" href="#defining-the-conve-model-class" title="Permalink to this headline">¶</a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Model implementation reference: https://github.com/TimDettmers/ConvE
class ConvE(torch.nn.Module):
    def __init__(self, num_entities, num_relations, emb_dim):
        super(ConvE, self).__init__()
        # Embedding tables for entity and relations with num_uniq_ent in y-dim, emb_dim in x-dim
        self.emb_e = torch.nn.Embedding(num_entities, emb_dim, padding_idx=0)
        self.ent_weights_matrix = torch.ones([num_entities, emb_dim], dtype=torch.float64)
        self.emb_rel = torch.nn.Embedding(num_relations, emb_dim, padding_idx=0)
        self.ne = num_entities
        self.nr = num_relations
        self.inp_drop = torch.nn.Dropout(0.2)
        self.hidden_drop = torch.nn.Dropout(0.3)
        self.feature_map_drop = torch.nn.Dropout2d(0.2)
        self.loss = torch.nn.BCELoss()
        self.conv1 = torch.nn.Conv2d(1, 32, (3, 3), 1, 0, bias=True)
        self.bn0 = torch.nn.BatchNorm2d(1)
        self.bn1 = torch.nn.BatchNorm2d(32)
        self.ln0 = torch.nn.LayerNorm(emb_dim)
        self.register_parameter(&#39;b&#39;, Parameter(torch.zeros(num_entities)))
        self.fc = torch.nn.Linear(16128, emb_dim)

    def init(self):
        &quot;&quot;&quot; Initializes the model &quot;&quot;&quot;
        # Xavier initialization
        xavier_normal_(self.emb_e.weight.data)
        xavier_normal_(self.emb_rel.weight.data)

    def forward(self, e1, rel):
        &quot;&quot;&quot; Forward pass on the model.
        :param e1: source entity
        :param rel: relation between the source and target entities
        Returns the model predictions for the target entities
        &quot;&quot;&quot;
        e1_embedded = self.emb_e(e1).view(-1, 1, 10, 30)
        rel_embedded = self.emb_rel(rel).view(-1, 1, 10, 30)
        stacked_inputs = torch.cat([e1_embedded, rel_embedded], 2)
        stacked_inputs = self.bn0(stacked_inputs)
        x = self.inp_drop(stacked_inputs)
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.feature_map_drop(x)
        x = x.view(1, -1)
        x = self.fc(x)
        x = self.hidden_drop(x)
        x = self.ln0(x)
        x = F.relu(x)
        x = torch.mm(x, self.emb_e.weight.transpose(1, 0))
        x = self.hidden_drop(x)
        x += self.b.expand_as(x)
        pred = torch.nn.functional.softmax(x, dim=1)
        return pred
</pre></div>
</div>
</section>
<section id="defining-the-dataloader">
<h2>Defining the dataloader<a class="headerlink" href="#defining-the-dataloader" title="Permalink to this headline">¶</a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class DataLoader():
    def __init__(self):
        super(DataLoader, self).__init__()

        self.ent_path = entdatapath
        self.rel_path = reldatapath
        self.test_file = testdatapath
        self.entity_ids, self.ids2entities = self.load_data(data_path=self.ent_path)
        self.rel_ids, self.ids2rel = self.load_data(data_path=self.rel_path)
        self.test_triples_list = self.convert_triples(data_path=self.test_file)

    def load_data(self, data_path):
        &quot;&quot;&quot; Creates a dictionary of data items with corresponding ids &quot;&quot;&quot;
        item_dict, ids_dict = {}, {}
        fp = open(data_path, &quot;r&quot;)
        lines = fp.readlines()
        for line in lines:
            name, id = line.strip().split(&#39;\t&#39;)
            item_dict[name] = int(id)
            ids_dict[int(id)] = name
        fp.close()
        return item_dict, ids_dict

    def convert_triples(self, data_path):
        &quot;&quot;&quot; Creates a triple of source entity, relation and target entities&quot;&quot;&quot;
        triples_list = []
        dp = open(data_path, &quot;r&quot;)
        lines = dp.readlines()
        for line in lines:
            item_dict = json.loads(line.strip())
            h = item_dict[&#39;e1&#39;]
            r = item_dict[&#39;rel&#39;]
            t = item_dict[&#39;e2_multi1&#39;].split(&#39;\t&#39;)
            hrt_list = []
            hrt_list.append(self.entity_ids[h])
            hrt_list.append(self.rel_ids[r])
            t_ents = []
            for t_idx in t:
                t_ents.append(self.entity_ids[t_idx])
            hrt_list.append(t_ents)
            triples_list.append(hrt_list)
        dp.close()
        return triples_list
</pre></div>
</div>
</section>
<section id="evaluate-the-trained-conve-model">
<h2>Evaluate the trained ConvE model<a class="headerlink" href="#evaluate-the-trained-conve-model" title="Permalink to this headline">¶</a></h2>
<p>We will first evaluate the model performance using PyTorch. The goal is
to make sure there are no accuracy differences between the original
model inference and the model converted to OpenVINO intermediate
representation inference results. Here, we use a simple accuracy metric
to evaluate the model performance on a test dataset. However, it is
typical to use metrics such as Mean Reciprocal Rank, <a class="reference external" href="mailto:Hits&#37;&#52;&#48;10">Hits<span>&#64;</span>10</a> etc.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>data = DataLoader()
num_entities = len(data.entity_ids)
num_relations = len(data.rel_ids)

model = ConvE(num_entities=num_entities, num_relations=num_relations, emb_dim=EMB_DIM)
model.load_state_dict(torch.load(modelpath))
model.eval()

pt_inf_times = []

triples_list = data.test_triples_list
num_test_samples = len(triples_list)
pt_acc = 0.0
for i in range(num_test_samples):
    test_sample = triples_list[i]
    h, r, t = test_sample
    start_time = time.time()
    logits = model.forward(e1=torch.tensor(h), rel=torch.tensor(r))
    end_time = time.time()
    pt_inf_times.append(end_time - start_time)
    score, pred = torch.topk(logits, TOP_K, 1)

    gt = np.array(sorted(t))
    pred = np.array(sorted(pred[0].cpu().detach()))
    pt_acc += accuracy_score(gt, pred)

avg_pt_time = np.mean(pt_inf_times) * 1000
print(f&#39;Average time taken for inference: {avg_pt_time} ms&#39;)
print(f&#39;Mean accuracy of the model on the test dataset: {pt_acc/num_test_samples}&#39;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Average</span> <span class="n">time</span> <span class="n">taken</span> <span class="k">for</span> <span class="n">inference</span><span class="p">:</span> <span class="mf">0.6341437498728434</span> <span class="n">ms</span>
<span class="n">Mean</span> <span class="n">accuracy</span> <span class="n">of</span> <span class="n">the</span> <span class="n">model</span> <span class="n">on</span> <span class="n">the</span> <span class="n">test</span> <span class="n">dataset</span><span class="p">:</span> <span class="mf">0.875</span>
</pre></div>
</div>
</section>
<section id="prediction-on-the-knowledge-graph">
<h2>Prediction on the Knowledge graph.<a class="headerlink" href="#prediction-on-the-knowledge-graph" title="Permalink to this headline">¶</a></h2>
<p>As a sample evaluation, we perform the entity prediction task on the
knowledge graph. We pass the source entity ‘san_marino’ and relation
‘locatedIn’ to the knowledge graph and obtain the target entity
predictions. We expect to see as predictions, target entities that form
a factual triple with the entity and relation passed as inputs to the
knowledge graph.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>entitynames_dict = data.ids2entities

ent = &#39;san_marino&#39;
rel = &#39;locatedin&#39;

h_idx = data.entity_ids[ent]
r_idx = data.rel_ids[rel]

logits = model.forward(torch.tensor(h_idx), torch.tensor(r_idx))
score, pred = torch.topk(logits, TOP_K, 1)

for j, id in enumerate(pred[0].cpu().detach().numpy()):
    pred_entity = entitynames_dict[id]
    print(f&#39;Source Entity: {ent}, Relation: {rel}, Target entity prediction: {pred_entity}&#39;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Source</span> <span class="n">Entity</span><span class="p">:</span> <span class="n">san_marino</span><span class="p">,</span> <span class="n">Relation</span><span class="p">:</span> <span class="n">locatedin</span><span class="p">,</span> <span class="n">Target</span> <span class="n">entity</span> <span class="n">prediction</span><span class="p">:</span> <span class="n">southern_europe</span>
<span class="n">Source</span> <span class="n">Entity</span><span class="p">:</span> <span class="n">san_marino</span><span class="p">,</span> <span class="n">Relation</span><span class="p">:</span> <span class="n">locatedin</span><span class="p">,</span> <span class="n">Target</span> <span class="n">entity</span> <span class="n">prediction</span><span class="p">:</span> <span class="n">europe</span>
</pre></div>
</div>
</section>
<section id="convert-the-trained-pytorch-model-to-onnx-format-for-openvino-inference">
<h2>Convert the trained PyTorch model to ONNX format for OpenVINO inference<a class="headerlink" href="#convert-the-trained-pytorch-model-to-onnx-format-for-openvino-inference" title="Permalink to this headline">¶</a></h2>
<p>To evaluate performance with OpenVINO, we can either convert the trained
PyTorch model to an intermediate representation (IR) format or to an
ONNX representation. In this notebook, we use the ONNX format. For more
details on model optimization, refer to:
<a class="reference external" href="https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html">https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html</a></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(&#39;Converting the trained conve model to ONNX format&#39;)
torch.onnx.export(model, (torch.tensor(1), torch.tensor(1)),
                  fp32_onnx_path, verbose=False, opset_version=11, training=False)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Converting</span> <span class="n">the</span> <span class="n">trained</span> <span class="n">conve</span> <span class="n">model</span> <span class="n">to</span> <span class="n">ONNX</span> <span class="nb">format</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/.</span><span class="n">venv</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">torch</span><span class="o">/</span><span class="n">onnx</span><span class="o">/</span><span class="n">utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">305</span><span class="p">:</span> <span class="ne">UserWarning</span><span class="p">:</span> <span class="n">It</span> <span class="ow">is</span> <span class="n">recommended</span> <span class="n">that</span> <span class="n">constant</span> <span class="n">folding</span> <span class="n">be</span> <span class="n">turned</span> <span class="n">off</span> <span class="p">(</span><span class="s1">&#39;do_constant_folding=False&#39;</span><span class="p">)</span> <span class="n">when</span> <span class="n">exporting</span> <span class="n">the</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">training</span><span class="o">-</span><span class="n">amenable</span> <span class="n">mode</span><span class="p">,</span> <span class="n">i</span><span class="o">.</span><span class="n">e</span><span class="o">.</span> <span class="k">with</span> <span class="s1">&#39;training=TrainingMode.TRAIN&#39;</span> <span class="ow">or</span> <span class="s1">&#39;training=TrainingMode.PRESERVE&#39;</span> <span class="p">(</span><span class="n">when</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">in</span> <span class="n">training</span> <span class="n">mode</span><span class="p">)</span><span class="o">.</span> <span class="n">Otherwise</span><span class="p">,</span> <span class="n">some</span> <span class="n">learnable</span> <span class="n">model</span> <span class="n">parameters</span> <span class="n">may</span> <span class="ow">not</span> <span class="n">translate</span> <span class="n">correctly</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">exported</span> <span class="n">ONNX</span> <span class="n">model</span> <span class="n">because</span> <span class="n">constant</span> <span class="n">folding</span> <span class="n">mutates</span> <span class="n">model</span> <span class="n">parameters</span><span class="o">.</span> <span class="n">Please</span> <span class="n">consider</span> <span class="n">turning</span> <span class="n">off</span> <span class="n">constant</span> <span class="n">folding</span> <span class="ow">or</span> <span class="n">setting</span> <span class="n">the</span> <span class="n">training</span><span class="o">=</span><span class="n">TrainingMode</span><span class="o">.</span><span class="n">EVAL</span><span class="o">.</span>
  <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;It is recommended that constant folding be turned off (&#39;do_constant_folding=False&#39;) &quot;</span>
</pre></div>
</div>
</section>
<section id="evaluate-the-model-performance-with-openvino">
<h2>Evaluate the model performance with OpenVINO<a class="headerlink" href="#evaluate-the-model-performance-with-openvino" title="Permalink to this headline">¶</a></h2>
<p>Now, we evaluate the model performance with the OpenVINO framework. In
order to do so, we make three main API calls: 1. Initialize the
Inference engine with Core() 2. Load the model with read_model() 3.
Compile the model with compile_model()</p>
<p>The model can then be inferred on using by using the
create_infer_request() API call.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>ie = Core()
ir_net = ie.read_model(model=fp32_onnx_path)
compiled_model = ie.compile_model(model=ir_net)
input_layer_source = compiled_model.input(&#39;input.1&#39;)
input_layer_relation = compiled_model.input(&#39;input.2&#39;)
output_layer = compiled_model.output(0)

ov_acc = 0.0
ov_inf_times = []
for i in range(num_test_samples):
    test_sample = triples_list[i]
    source, relation, target = test_sample
    model_inputs = {input_layer_source: source, input_layer_relation: relation}
    start_time = time.time()
    result = compiled_model(model_inputs)[output_layer]
    end_time = time.time()
    ov_inf_times.append(end_time - start_time)
    top_k_idxs = list(np.argpartition(result[0], -TOP_K)[-TOP_K:])

    gt = np.array(sorted(t))
    pred = np.array(sorted(top_k_idxs))
    ov_acc += accuracy_score(gt, pred)

avg_ov_time = np.mean(ov_inf_times) * 1000
print(f&#39;Average time taken for inference: {avg_ov_time} ms&#39;)
print(f&#39;Mean accuracy of the model on the test dataset: {ov_acc/num_test_samples}&#39;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Average</span> <span class="n">time</span> <span class="n">taken</span> <span class="k">for</span> <span class="n">inference</span><span class="p">:</span> <span class="mf">1.7911593119303386</span> <span class="n">ms</span>
<span class="n">Mean</span> <span class="n">accuracy</span> <span class="n">of</span> <span class="n">the</span> <span class="n">model</span> <span class="n">on</span> <span class="n">the</span> <span class="n">test</span> <span class="n">dataset</span><span class="p">:</span> <span class="mf">0.10416666666666667</span>
</pre></div>
</div>
</section>
<section id="determine-the-platform-specific-speedup-obtained-through-openvino-graph-optimizations">
<h2>Determine the platform specific speedup obtained through OpenVINO graph optimizations<a class="headerlink" href="#determine-the-platform-specific-speedup-obtained-through-openvino-graph-optimizations" title="Permalink to this headline">¶</a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(f&#39;Speedup with OpenVINO optimizations: {round(float(avg_pt_time)/float(avg_ov_time),2)} X&#39;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Speedup</span> <span class="k">with</span> <span class="n">OpenVINO</span> <span class="n">optimizations</span><span class="p">:</span> <span class="mf">0.35</span> <span class="n">X</span>
</pre></div>
</div>
</section>
<section id="benchmark-the-converted-openvino-model-using-benchmark-app">
<h2>Benchmark the converted OpenVINO model using benchmark app<a class="headerlink" href="#benchmark-the-converted-openvino-model-using-benchmark-app" title="Permalink to this headline">¶</a></h2>
<p>The OpenVINO toolkit provides a benchmarking application to gauge the
platform specific runtime performance that can be obtained under optimal
configuration parameters for a given model. For more details refer to:
<a class="reference external" href="https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html">https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html</a></p>
<p>Here, we use the benchmark application to obtain performance estimates
under optimal configuration for the knowledge graph model inference. We
obtain the average (AVG), minimum (MIN) as well as maximum (MAX) latency
as well as the throughput performance (in samples/s) observed while
running the benchmark application. The platform specific optimal
configuration parameters determined by the benchmarking app for OpenVINO
inference can also be obtained by looking at the benchmark app results.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(&#39;Benchmark OpenVINO model using the benchmark app&#39;)
! benchmark_app -m &quot;$fp32_onnx_path&quot; -d CPU -api async -t 10 -shape &quot;input.1[1],input.2[1]&quot;
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Benchmark OpenVINO model using the benchmark app
[Step 1/11] Parsing and validating input arguments
[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README.
[Step 2/11] Loading OpenVINO
[ WARNING ] PerformanceMode was not explicitly specified in command line. Device CPU performance hint will be set to THROUGHPUT.
[ INFO ] OpenVINO:
         API version............. 2022.1.0-7019-cdb9bec7210-releases/2022/1
[ INFO ] Device info
         CPU
         openvino_intel_cpu_plugin version 2022.1
         Build................... 2022.1.0-7019-cdb9bec7210-releases/2022/1

[Step 3/11] Setting device configuration
[ WARNING ] -nstreams default value is determined automatically for CPU device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README.
[Step 4/11] Reading network files
[ INFO ] Read model took 27.51 ms
[Step 5/11] Resizing network to match image sizes and given batch
[ INFO ] Reshaping model: &#39;input.1&#39;: {1}, &#39;input.2&#39;: {1}
[ INFO ] Reshape model took 0.82 ms
[ INFO ] Network batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model input &#39;input.1&#39; precision i64, dimensions ([...]): 1
[ INFO ] Model input &#39;input.2&#39; precision i64, dimensions ([...]): 1
[ INFO ] Model output &#39;51&#39; precision f32, dimensions ([...]): 1 271
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 50.77 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] DEVICE: CPU
[ INFO ]   AVAILABLE_DEVICES  , [&#39;&#39;]
[ INFO ]   RANGE_FOR_ASYNC_INFER_REQUESTS  , (1, 1, 1)
[ INFO ]   RANGE_FOR_STREAMS  , (1, 24)
[ INFO ]   FULL_DEVICE_NAME  , Intel(R) Core(TM) i9-10920X CPU @ 3.50GHz
[ INFO ]   OPTIMIZATION_CAPABILITIES  , [&#39;WINOGRAD&#39;, &#39;FP32&#39;, &#39;FP16&#39;, &#39;INT8&#39;, &#39;BIN&#39;, &#39;EXPORT_IMPORT&#39;]
[ INFO ]   CACHE_DIR  ,
[ INFO ]   NUM_STREAMS  , 6
[ INFO ]   INFERENCE_NUM_THREADS  , 0
[ INFO ]   PERF_COUNT  , False
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS  , 0
[Step 9/11] Creating infer requests and preparing input data
[ INFO ] Create 6 infer requests took 1.78 ms
[ WARNING ] No input files were given for input &#39;input.1&#39;!. This input will be filled with random values!
[ WARNING ] No input files were given for input &#39;input.2&#39;!. This input will be filled with random values!
[ INFO ] Fill input &#39;input.1&#39; with random values
[ INFO ] Fill input &#39;input.2&#39; with random values
[Step 10/11] Measuring performance (Start inference asynchronously, 6 inference requests using 6 streams for CPU, inference only: True, limits: 10000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 4.07 ms
[Step 11/11] Dumping statistics report
Count:          84108 iterations
Duration:       10000.84 ms
Latency:
    Median:     0.62 ms
    AVG:        0.64 ms
    MIN:        0.41 ms
    MAX:        2.45 ms
Throughput: 8410.09 FPS
</pre></div>
</div>
</section>
<section id="conclusions">
<h2>Conclusions<a class="headerlink" href="#conclusions" title="Permalink to this headline">¶</a></h2>
<p>In this notebook, we converted the trained PyTorch knowledge graph
embeddings model to the OpenVINO format. We confirmed that there were no
accuracy differences post conversion. We also performed a sample
evaluation on the knowledge graph! We then determined the platform
specific speedup in runtime performance that can be obtained through
OpenVINO graph optimizations. To learn more about the OpenVINO
performance optimizations, refer to:
<a class="reference external" href="https://docs.openvino.ai/latest/openvino_docs_optimization_guide_dldt_optimization_guide.html">https://docs.openvino.ai/latest/openvino_docs_optimization_guide_dldt_optimization_guide.html</a></p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Convolutional 2D Knowledge Graph Embeddings, Tim Dettmers et
al. (<a class="reference external" href="https://arxiv.org/abs/1707.01476">https://arxiv.org/abs/1707.01476</a>)</p></li>
<li><p>Model implementation: <a class="reference external" href="https://github.com/TimDettmers/ConvE">https://github.com/TimDettmers/ConvE</a></p></li>
</ol>
<p>The ConvE model implementation used in this notebook licensed under the
MIT License. The license is displayed below: MIT License</p>
<p>Copyright (c) 2017 Tim Dettmers</p>
<p>Permission is hereby granted, free of charge, to any person obtaining a
copy of this software and associated documentation files (the
“Software”), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:</p>
<p>The above copyright notice and this permission notice shall be included
in all copies or substantial portions of the Software.</p>
<p>THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS
OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  
    <a class='button bttn-sec button-size-l' id="prev-link" href="218-vehicle-detection-and-recognition-with-output.html" title="previous page">Prev</a>
    <a class='button bttn-sec button-size-l' id="next-link" href="220-yolov5-accuracy-check-and-quantization-with-output.html" title="next page">Next</a>

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel®.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>