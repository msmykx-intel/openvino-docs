
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Industrial Meter Reader &#8212; OpenVINO™  documentation</title>
    
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="../_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="../_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/tabs.css" type="text/css" />
    <script src="../_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="../_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="../_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/tabs.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/custom.js"></script>
    <script src="../_static/js/graphs.js"></script>
    <script src="../_static/js/graphs_ov_tf.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/target-highlight.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://docs.openvino.ai/latest/notebooks/203-meter-reader-with-output.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Document Entity Extraction with OpenVINO" href="204-named-entity-recognition-with-output.html" />
    <link rel="prev" title="Video Super Resolution with OpenVINO™" href="202-vision-superresolution-video-with-output.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/get-started-guide.html">
  Get Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/documentation.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api/api_reference.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../model_zoo.html">
  Model Zoo
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/resources.html">
  Resources
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/openvinotoolkit/openvino" rel="noopener" target="_blank" title="GitHub">
            <span><i class="sst-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="language-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">English</button>
  <div class="dropdown-menu" aria-labelledby="language-selector">
    
      
        <a class="dropdown-item font-weight-bold" href="/openvino-docs/index.html">English</a>
      
    
      
        <a  class="dropdown-item" href="/cn/openvino-docs/index.html">Chinese</a>
      
    
  </div>
</div>

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="../search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notebooks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks-installation.html">
   Installation of OpenVINO™ Notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="001-hello-world-with-output.html">
   Hello Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002-openvino-api-with-output.html">
   OpenVINO™ Runtime API Tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003-hello-segmentation-with-output.html">
   Hello Image Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="004-hello-detection-with-output.html">
   Hello Object Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="101-tensorflow-to-openvino-with-output.html">
   Convert a TensorFlow Model to OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="102-pytorch-onnx-to-openvino-with-output.html">
   Convert a PyTorch Model to ONNX and OpenVINO™ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="103-paddle-onnx-to-openvino-classification-with-output.html">
   Convert a PaddlePaddle Model to ONNX and OpenVINO™ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="104-model-tools-with-output.html">
   Working with Open Model Zoo Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="105-language-quantize-bert-with-output.html">
   Quantize NLP models with Post-Training Optimization Tool ​in OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="106-auto-device-with-output.html">
   Automatic Device Selection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="107-speech-recognition-quantization-with-output.html">
   Quantize Speech Recognition Models with OpenVINO™ Post-Training Optimization Tool ​
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-nncf-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="111-detection-quantization-with-output.html">
   Object Detection Quantization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="112-pytorch-post-training-quantization-nncf-with-output.html">
   Post-Training Quantization of PyTorch models with NNCF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="113-image-classification-quantization-with-output.html">
   Quantization of Image Classification Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="114-quantization-simplified-mode-with-output.html">
   INT8 Quantization with Post-training Optimization Tool (POT) in Simplified Mode tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="115-async-api-with-output.html">
   Asynchronous Inference with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="201-vision-monodepth-with-output.html">
   Monodepth Estimation with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-image-with-output.html">
   Single Image Super Resolution with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-video-with-output.html">
   Video Super Resolution with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Industrial Meter Reader
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="204-named-entity-recognition-with-output.html">
   Document Entity Extraction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="205-vision-background-removal-with-output.html">
   Image Background Removal with U^2-Net and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="206-vision-paddlegan-anime-with-output.html">
   Photos to Anime with PaddleGAN and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="207-vision-paddlegan-superresolution-with-output.html">
   Super Resolution with PaddleGAN and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="208-optical-character-recognition-with-output.html">
   Optical Character Recognition (OCR) with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="209-handwritten-ocr-with-output.html">
   Handwritten Chinese and Japanese OCR with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="210-ct-scan-live-inference-with-output.html">
   Live Inference and Benchmark CT-scan Data with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="211-speech-to-text-with-output.html">
   Speech to Text with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="212-onnx-style-transfer-with-output.html">
   Style Transfer on ONNX Models with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="214-vision-paddle-classification-with-output.html">
   PaddlePaddle Image Classification with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="215-image-inpainting-with-output.html">
   Image In-painting with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="216-license-plate-recognition-with-output.html">
   License Plate Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="217-vision-deblur-with-output.html">
   Deblur Photos with DeblurGAN-v2 and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="218-vehicle-detection-and-recognition-with-output.html">
   Vehicle Detection And Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="219-knowledge-graphs-conve-with-output.html">
   OpenVINO optimizations for Knowledge graphs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="220-yolov5-accuracy-check-and-quantization-with-output.html">
   Quantize the Ultralytics YOLOv5 model and check accuracy using the OpenVINO POT API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="221-machine-translation-with-output.html">
   Machine translation demo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="222-vision-image-colorization-with-output.html">
   Image Colorization with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="223-gpt2-text-prediction-with-output.html">
   GPT-2 Text Prediction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-pot-with-output.html">
   Post-Training Quantization with TensorFlow Classification Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-with-output.html">
   From Training to Deployment with TensorFlow and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="302-pytorch-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using PyTorch framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="305-tensorflow-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using TensorFlow Framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="401-object-detection-with-output.html">
   Live Object Detection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="402-pose-estimation-with-output.html">
   Live Human Pose Estimation with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="403-action-recognition-webcam-with-output.html">
   Human Action Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="405-paddle-ocr-webcam-with-output.html">
   PaddleOCR with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebook_utils-with-output.html">
   Notebook Utils
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#import">
   Import
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prepare-the-model-and-test-image">
   Prepare the Model and Test Image
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#configuration">
   Configuration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-the-models">
   Load the Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#predictor">
   Predictor
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-process">
   Data Process
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#main-function">
   Main Function
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intialize-the-model-and-parameters">
     Intialize the model and parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#run-meter-detection-model">
     Run meter detection model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#run-meter-segmentation-model">
     Run meter segmentation model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#postprocess-the-models-result-and-calculate-the-final-readings">
     Postprocess the models result and calculate the final readings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#get-the-reading-result-on-the-meter-picture">
     Get the reading result on the meter picture
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">

<div class="tocsection editthispage">
    <a href="None">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

            
                <div>
                  
  <section id="industrial-meter-reader">
<h1>Industrial Meter Reader<a class="headerlink" href="#industrial-meter-reader" title="Permalink to this headline">¶</a></h1>
<p>This notebook shows how to create a industrial meter reader with
OpenVINO Runtime. We use the PaddlePaddle pre-trained model
<a class="reference external" href="https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/configs/ppyolo">PPYOLOv2</a>
and
<a class="reference external" href="https://github.com/PaddlePaddle/PaddleSeg/tree/release/2.5/configs/deeplabv3p">DeepLabV3P</a>
to built-up a multiple inference task pipeline:</p>
<ol class="arabic simple">
<li><p>Run detection model to find the meters, and crop them from the origin
photo.</p></li>
<li><p>Run segmentation model on these cropped meters to get the pointer and
scale instance.</p></li>
<li><p>Find the location of the pointer in scale map.</p></li>
</ol>
<figure class="align-default" id="id1">
<img alt="workflow" src="https://user-images.githubusercontent.com/91237924/166137115-67284fa5-f703-4468-98f4-c43d2c584763.png" />
<figcaption>
<p><span class="caption-text">workflow</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="import">
<h2>Import<a class="headerlink" href="#import" title="Permalink to this headline">¶</a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import os
import sys
from pathlib import Path
import numpy as np
import math
import cv2
import tarfile
import matplotlib.pyplot as plt
from openvino.runtime import Core

sys.path.append(&quot;../utils&quot;)
from notebook_utils import download_file, segmentation_map_to_image
</pre></div>
</div>
</section>
<section id="prepare-the-model-and-test-image">
<h2>Prepare the Model and Test Image<a class="headerlink" href="#prepare-the-model-and-test-image" title="Permalink to this headline">¶</a></h2>
<p>Download PPYolov2 and DeepLabV3P pre-trained models from PaddlePaddle
community.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>MODEL_DIR = &quot;model&quot;
DATA_DIR = &quot;data&quot;
DET_MODEL_LINK = &quot;https://bj.bcebos.com/paddlex/examples2/meter_reader/meter_det_model.tar.gz&quot;
SEG_MODEL_LINK = &quot;https://bj.bcebos.com/paddlex/examples2/meter_reader/meter_seg_model.tar.gz&quot;
DET_FILE_NAME = DET_MODEL_LINK.split(&quot;/&quot;)[-1]
SEG_FILE_NAME = SEG_MODEL_LINK.split(&quot;/&quot;)[-1]
IMG_LINK = &quot;https://user-images.githubusercontent.com/91237924/170696219-f68699c6-1e82-46bf-aaed-8e2fc3fa5f7b.jpg&quot;
IMG_FILE_NAME = IMG_LINK.split(&quot;/&quot;)[-1]
IMG_PATH = Path(f&quot;{DATA_DIR}/{IMG_FILE_NAME}&quot;)

os.makedirs(MODEL_DIR, exist_ok=True)

download_file(DET_MODEL_LINK, directory=MODEL_DIR, show_progress=True)
file = tarfile.open(f&quot;model/{DET_FILE_NAME}&quot;)
res = file.extractall(&quot;model&quot;)
if not res:
    print(f&quot;Detection Model Extracted to \&quot;./{MODEL_DIR}\&quot;.&quot;)
else:
    print(&quot;Error Extracting the Detection model. Please check the network.&quot;)

download_file(SEG_MODEL_LINK, directory=MODEL_DIR, show_progress=True)
file = tarfile.open(f&quot;model/{SEG_FILE_NAME}&quot;)
res = file.extractall(&quot;model&quot;)
if not res:
    print(f&quot;Segmentation Model Extracted to \&quot;./{MODEL_DIR}\&quot;.&quot;)
else:
    print(&quot;Error Extracting the Segmentation model. Please check the network.&quot;)

download_file(IMG_LINK, directory=DATA_DIR, show_progress=True)
if IMG_PATH.is_file():
    print(f&quot;Test Image Saved to \&quot;./{DATA_DIR}\&quot;.&quot;)
else:
    print(&quot;Error Downloading the Test Image. Please check the network.&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>model/meter_det_model.tar.gz:   0%|          | 0.00/192M [00:00&lt;?, ?B/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Detection</span> <span class="n">Model</span> <span class="n">Extracted</span> <span class="n">to</span> <span class="s2">&quot;./model&quot;</span><span class="o">.</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>model/meter_seg_model.tar.gz:   0%|          | 0.00/94.9M [00:00&lt;?, ?B/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Segmentation</span> <span class="n">Model</span> <span class="n">Extracted</span> <span class="n">to</span> <span class="s2">&quot;./model&quot;</span><span class="o">.</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>data/170696219-f68699c6-1e82-46bf-aaed-8e2fc3fa5f7b.jpg:   0%|          | 0.00/183k [00:00&lt;?, ?B/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Test</span> <span class="n">Image</span> <span class="n">Saved</span> <span class="n">to</span> <span class="s2">&quot;./data&quot;</span><span class="o">.</span>
</pre></div>
</div>
</section>
<section id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h2>
<p>Add parameter configuration for reading calculation.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>METER_SHAPE = [512, 512]
CIRCLE_CENTER = [256, 256]
CIRCLE_RADIUS = 250
PI = math.pi
RECTANGLE_HEIGHT = 120
RECTANGLE_WIDTH = 1570
TYPE_THRESHOLD = 40
COLORMAP = np.array([[28, 28, 28], [238, 44, 44], [250, 250, 250]])

# There are 2 types of meters in test image datasets
METER_CONFIG = [{
    &#39;scale_interval_value&#39;: 25.0 / 50.0,
    &#39;range&#39;: 25.0,
    &#39;unit&#39;: &quot;(MPa)&quot;
}, {
    &#39;scale_interval_value&#39;: 1.6 / 32.0,
    &#39;range&#39;: 1.6,
    &#39;unit&#39;: &quot;(MPa)&quot;
}]

SEG_LABEL = {&#39;background&#39;: 0, &#39;pointer&#39;: 1, &#39;scale&#39;: 2}
</pre></div>
</div>
</section>
<section id="load-the-models">
<h2>Load the Models<a class="headerlink" href="#load-the-models" title="Permalink to this headline">¶</a></h2>
<p>Initialize the OpenVINO compiled model.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Initialize OpenVINO Runtime
ie_core = Core()


def model_init(det_model_path, seg_model_path):
    &quot;&quot;&quot;
    Initialize the Detection and Segmentation models

    :param:
            model (str): model path *.pdmodel
    :retuns:
            detector: detection compiled model
            segmenter: segmentation compiled model

    &quot;&quot;&quot;

    # Load the PaddlePaddle detection model directly
    det_model = ie_core.read_model(model=det_model_path)
    det_model.reshape({&#39;image&#39;: [1, 3, 608, 608], &#39;im_shape&#39;: [1, 2], &#39;scale_factor&#39;: [1, 2]})
    detector = ie_core.compile_model(model=det_model, device_name=&quot;CPU&quot;)

    # Load the PaddlePaddle segmentation model directly, the input batch size is dynamic
    seg_model = ie_core.read_model(model=seg_model_path)
    seg_model.reshape({&#39;image&#39;: [-1, 3, 512, 512]})
    segmenter = ie_core.compile_model(model=seg_model, device_name=&quot;CPU&quot;)

    return detector, segmenter
</pre></div>
</div>
</section>
<section id="predictor">
<h2>Predictor<a class="headerlink" href="#predictor" title="Permalink to this headline">¶</a></h2>
<p>Define a common predictor function for both detection and segmentation
models</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def predictor(input, compiled_model):
    &quot;&quot;&quot;
    A predictor to run inference

    :param:
            input: input data
            compiled_model: a IE detector or segmenter
    :retuns:
            result (np.array): model output data

    &quot;&quot;&quot;
    output_layer = compiled_model.output(0)
    result = compiled_model(input)[output_layer]
    return result
</pre></div>
</div>
</section>
<section id="data-process">
<h2>Data Process<a class="headerlink" href="#data-process" title="Permalink to this headline">¶</a></h2>
<p>Including the preprocessing and postprocessing tasks of each model.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def det_preprocess(input_image, target_size):
    &quot;&quot;&quot;
    Preprocessing the input data for detection task

    :param:
            input_image (np.array): input data
            size (int): the image size required by model input layer
    :retuns:
            img.astype (np.float32): preprocessed image

    &quot;&quot;&quot;
    img = cv2.resize(input_image, (target_size, target_size))
    img = np.transpose(img, [2, 0, 1]) / 255
    img = np.expand_dims(img, 0)
    img_mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))
    img_std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))
    img -= img_mean
    img /= img_std
    return img.astype(np.float32)


def filter_bboxes(det_results, score_threshold):
    &quot;&quot;&quot;
    filter out the detection results with low confidence

    :param：
        det_results (list[dict]): detection results
        score_threshold (float)： confidence threshold

    :retuns：
        filtered_results (list[dict]): filter detection results

    &quot;&quot;&quot;
    filtered_results = []
    for i in range(len(det_results)):
        if det_results[i, 1] &gt; score_threshold:
            filtered_results.append(det_results[i])
    return filtered_results


def roi_crop(image, results, scale_x, scale_y):
    &quot;&quot;&quot;
    crop the area of detected meter of original image

    :param：
        img (np.array)：original image。
        det_results (list[dict]): detection results
        scale_x (float): the scale value in x axis
        scale_y (float): the scale value in y axis

    :retuns：
        roi_imgs (list[np.array]): the list of meter images
        loc (list[int]): the list of meter locations

    &quot;&quot;&quot;
    roi_imgs = []
    loc = []
    for result in results:
        bbox = result[2:]
        xmin, ymin, xmax, ymax = [int(bbox[0] * scale_x), int(bbox[1] * scale_y), int(bbox[2] * scale_x), int(bbox[3] * scale_y)]
        sub_img = image[ymin:(ymax + 1), xmin:(xmax + 1), :]
        roi_imgs.append(sub_img)
        loc.append([xmin, ymin, xmax, ymax])
    return roi_imgs, loc


def roi_process(input_images, target_size, interp=cv2.INTER_LINEAR):
    &quot;&quot;&quot;
    Prepare the roi image of detection results data
    Preprocessing the input data for segmentation task

    :param：
        input_images (list[np.array])：the list of meter images
        target_size (list|tuple)： height and width of resized image， e.g [heigh,width]
        interp (int)：the interp method for image reszing

    :retuns：
        img_list (list[np.array])：the list of processed images
        resize_img (list[np.array]): for visualization

    &quot;&quot;&quot;
    img_list = list()
    resize_list = list()
    for img in input_images:
        img_shape = img.shape
        scale_x = float(target_size[1]) / float(img_shape[1])
        scale_y = float(target_size[0]) / float(img_shape[0])
        resize_img = cv2.resize(img, None, None, fx=scale_x, fy=scale_y, interpolation=interp)
        resize_list.append(resize_img)
        resize_img = resize_img.transpose(2, 0, 1) / 255
        img_mean = np.array([0.5, 0.5, 0.5]).reshape((3, 1, 1))
        img_std = np.array([0.5, 0.5, 0.5]).reshape((3, 1, 1))
        resize_img -= img_mean
        resize_img /= img_std
        img_list.append(resize_img)
    return img_list, resize_list


def erode(seg_results, erode_kernel):
    &quot;&quot;&quot;
    erode the segmentation result to get the more clear instance of pointer and scale

    :param：
        seg_results (list[dict])：segmentation results
        erode_kernel (int): size of erode_kernel

    :return：
        eroded_results (list[dict])： the lab map of eroded_results
    &quot;&quot;&quot;
    kernel = np.ones((erode_kernel, erode_kernel), np.uint8)
    eroded_results = seg_results
    for i in range(len(seg_results)):
        eroded_results[i] = cv2.erode(seg_results[i].astype(np.uint8), kernel)
    return eroded_results


def circle_to_rectangle(seg_results):
    &quot;&quot;&quot;
    switch the shape of label_map from circle to rectangle

    :param：
        seg_results (list[dict])：segmentation results

    :return：
        rectangle_meters (list[np.array])：the rectangle of label map

    &quot;&quot;&quot;
    rectangle_meters = list()
    for i, seg_result in enumerate(seg_results):
        label_map = seg_result

        # The size of rectangle_meter is determined by RECTANGLE_HEIGHT and RECTANGLE_WIDTH
        rectangle_meter = np.zeros((RECTANGLE_HEIGHT, RECTANGLE_WIDTH), dtype=np.uint8)
        for row in range(RECTANGLE_HEIGHT):
            for col in range(RECTANGLE_WIDTH):
                theta = PI * 2 * (col + 1) / RECTANGLE_WIDTH

                # The radius of meter circle will be mapped to the height of rectangle
                rho = CIRCLE_RADIUS - row - 1
                y = int(CIRCLE_CENTER[0] + rho * math.cos(theta) + 0.5)
                x = int(CIRCLE_CENTER[1] - rho * math.sin(theta) + 0.5)
                rectangle_meter[row, col] = label_map[y, x]
        rectangle_meters.append(rectangle_meter)
    return rectangle_meters


def rectangle_to_line(rectangle_meters):
    &quot;&quot;&quot;
    switch the dimension of rectangle label map from 2D to 1D

    :param：
        rectangle_meters (list[np.array])：2D rectangle OF label_map。

    :return：
        line_scales (list[np.array])： the list of scales value
        line_pointers (list[np.array])：the list of pointers value

    &quot;&quot;&quot;
    line_scales = list()
    line_pointers = list()
    for rectangle_meter in rectangle_meters:
        height, width = rectangle_meter.shape[0:2]
        line_scale = np.zeros((width), dtype=np.uint8)
        line_pointer = np.zeros((width), dtype=np.uint8)
        for col in range(width):
            for row in range(height):
                if rectangle_meter[row, col] == SEG_LABEL[&#39;pointer&#39;]:
                    line_pointer[col] += 1
                elif rectangle_meter[row, col] == SEG_LABEL[&#39;scale&#39;]:
                    line_scale[col] += 1
        line_scales.append(line_scale)
        line_pointers.append(line_pointer)
    return line_scales, line_pointers


def mean_binarization(data_list):
    &quot;&quot;&quot;
    binarize the data

    :param：
        data_list (list[np.array])：input data

    :return：
        binaried_data_list (list[np.array])：output data。

    &quot;&quot;&quot;
    batch_size = len(data_list)
    binaried_data_list = data_list
    for i in range(batch_size):
        mean_data = np.mean(data_list[i])
        width = data_list[i].shape[0]
        for col in range(width):
            if data_list[i][col] &lt; mean_data:
                binaried_data_list[i][col] = 0
            else:
                binaried_data_list[i][col] = 1
    return binaried_data_list


def locate_scale(line_scales):
    &quot;&quot;&quot;
    find out the location of center of each scale

    :param：
        line_scales (list[np.array])：the list of binaried scales value

    :return：
        scale_locations (list[list])：location of each scale

    &quot;&quot;&quot;
    batch_size = len(line_scales)
    scale_locations = list()
    for i in range(batch_size):
        line_scale = line_scales[i]
        width = line_scale.shape[0]
        find_start = False
        one_scale_start = 0
        one_scale_end = 0
        locations = list()
        for j in range(width - 1):
            if line_scale[j] &gt; 0 and line_scale[j + 1] &gt; 0:
                if not find_start:
                    one_scale_start = j
                    find_start = True
            if find_start:
                if line_scale[j] == 0 and line_scale[j + 1] == 0:
                    one_scale_end = j - 1
                    one_scale_location = (one_scale_start + one_scale_end) / 2
                    locations.append(one_scale_location)
                    one_scale_start = 0
                    one_scale_end = 0
                    find_start = False
        scale_locations.append(locations)
    return scale_locations


def locate_pointer(line_pointers):
    &quot;&quot;&quot;
    find out the location of center of pointer

    :param：
        line_scales (list[np.array])：the list of binaried pointer value

    :return：
        scale_locations (list[list])：location of pointer

    &quot;&quot;&quot;
    batch_size = len(line_pointers)
    pointer_locations = list()
    for i in range(batch_size):
        line_pointer = line_pointers[i]
        find_start = False
        pointer_start = 0
        pointer_end = 0
        location = 0
        width = line_pointer.shape[0]
        for j in range(width - 1):
            if line_pointer[j] &gt; 0 and line_pointer[j + 1] &gt; 0:
                if not find_start:
                    pointer_start = j
                    find_start = True
            if find_start:
                if line_pointer[j] == 0 and line_pointer[j + 1] == 0 :
                    pointer_end = j - 1
                    location = (pointer_start + pointer_end) / 2
                    find_start = False
                    break
        pointer_locations.append(location)
    return pointer_locations


def get_relative_location(scale_locations, pointer_locations):
    &quot;&quot;&quot;
    match the location of pointer and scales

    :param：
        scale_locations (list[list])：location of each scale
        pointer_locations (list[list])：location of pointer

    :return：
        pointed_scales (list[dict])： a list of dict with:
                                     &#39;num_scales&#39;: total number of scales
                                     &#39;pointed_scale&#39;: predicted number of scales

    &quot;&quot;&quot;
    pointed_scales = list()
    for scale_location, pointer_location in zip(scale_locations,
                                                pointer_locations):
        num_scales = len(scale_location)
        pointed_scale = -1
        if num_scales &gt; 0:
            for i in range(num_scales - 1):
                if scale_location[i] &lt;= pointer_location &lt; scale_location[i + 1]:
                    pointed_scale = i + (pointer_location - scale_location[i]) / (scale_location[i + 1] - scale_location[i] + 1e-05) + 1
        result = {&#39;num_scales&#39;: num_scales, &#39;pointed_scale&#39;: pointed_scale}
        pointed_scales.append(result)
    return pointed_scales


def calculate_reading(pointed_scales):
    &quot;&quot;&quot;
    calculate the value of meter according to the type of meter

    :param：
        pointed_scales (list[list])：predicted number of scales

    :return：
        readings (list[float])： the list of values read from meter

    &quot;&quot;&quot;
    readings = list()
    batch_size = len(pointed_scales)
    for i in range(batch_size):
        pointed_scale = pointed_scales[i]
        # find out the type of meter according the total number of scales
        if pointed_scale[&#39;num_scales&#39;] &gt; TYPE_THRESHOLD:
            reading = pointed_scale[&#39;pointed_scale&#39;] * METER_CONFIG[0][&#39;scale_interval_value&#39;]
        else:
            reading = pointed_scale[&#39;pointed_scale&#39;] * METER_CONFIG[1][&#39;scale_interval_value&#39;]
        readings.append(reading)
    return readings
</pre></div>
</div>
</section>
<section id="main-function">
<h2>Main Function<a class="headerlink" href="#main-function" title="Permalink to this headline">¶</a></h2>
<section id="intialize-the-model-and-parameters">
<h3>Intialize the model and parameters<a class="headerlink" href="#intialize-the-model-and-parameters" title="Permalink to this headline">¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>img_file = f&quot;{DATA_DIR}/{IMG_FILE_NAME}&quot;
det_model_path = f&quot;{MODEL_DIR}/meter_det_model/model.pdmodel&quot;
seg_model_path = f&quot;{MODEL_DIR}/meter_seg_model/model.pdmodel&quot;
erode_kernel = 4
score_threshold = 0.5
seg_batch_size = 2
input_shape = 608
detector, segmenter = model_init(det_model_path, seg_model_path)
image = cv2.imread(img_file)
plt.imshow(image)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">AxesImage</span> <span class="n">at</span> <span class="mh">0x7f06267cbc70</span><span class="o">&gt;</span>
</pre></div>
</div>
<img alt="../_images/203-meter-reader-with-output_15_1.png" src="../_images/203-meter-reader-with-output_15_1.png" />
</section>
<section id="run-meter-detection-model">
<h3>Run meter detection model<a class="headerlink" href="#run-meter-detection-model" title="Permalink to this headline">¶</a></h3>
<p>Detect the location of the meter and prepare the ROI images for
segmentation.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Prepare the input data for meter detection model
im_shape = np.array([[input_shape, input_shape]]).astype(&#39;float32&#39;)
scale_factor = np.array([[1, 2]]).astype(&#39;float32&#39;)
input_image = det_preprocess(image, input_shape)
inputs_dict = {&#39;image&#39;: input_image, &quot;im_shape&quot;: im_shape, &quot;scale_factor&quot;: scale_factor}

# Run meter detection model
det_results = predictor(inputs_dict, detector)

# Filter out the bounding box with low confidence
filtered_results = filter_bboxes(det_results, score_threshold)

# Prepare the input data for meter segmentation model
scale_x = image.shape[1] / input_shape * 2
scale_y = image.shape[0] / input_shape

# Create the individual photo for each detected meter
roi_imgs, loc = roi_crop(image, filtered_results, scale_x, scale_y)
roi_imgs, resize_imgs = roi_process(roi_imgs, METER_SHAPE)

# Create the photos of detection results
if len(resize_imgs) == 2:
    roi_stack = np.hstack([resize_imgs[0], resize_imgs[1]])
else:
    roi_stack = resize_imgs[0]

if cv2.imwrite(f&quot;{DATA_DIR}/detection_results.jpg&quot;, roi_stack):
    print(&quot;The detection result image has been saved as \&quot;detection_results.jpg\&quot; in data&quot;)
    plt.imshow(roi_stack)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">detection</span> <span class="n">result</span> <span class="n">image</span> <span class="n">has</span> <span class="n">been</span> <span class="n">saved</span> <span class="k">as</span> <span class="s2">&quot;detection_results.jpg&quot;</span> <span class="ow">in</span> <span class="n">data</span>
</pre></div>
</div>
<img alt="../_images/203-meter-reader-with-output_17_1.png" src="../_images/203-meter-reader-with-output_17_1.png" />
</section>
<section id="run-meter-segmentation-model">
<h3>Run meter segmentation model<a class="headerlink" href="#run-meter-segmentation-model" title="Permalink to this headline">¶</a></h3>
<p>Get the results of segmentation task on detected ROI.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>seg_results = list()
num_imgs = len(roi_imgs)

# Run meter segmentation model on all meters detected
for i in range(0, num_imgs, seg_batch_size):
    batch = roi_imgs[i : min(num_imgs, i + seg_batch_size)]
    seg_result = predictor([batch], segmenter)
    seg_results.extend(seg_result)
results = []
for i in range(len(seg_results)):
    results.append(np.argmax(seg_results[i], axis=0))
seg_results = erode(results, erode_kernel)

# Create the photos of segmentation results
if len(resize_imgs) == 2:
    mask_stack = np.hstack([segmentation_map_to_image(seg_results[0], COLORMAP),
                            segmentation_map_to_image(seg_results[1], COLORMAP)])
else:
    mask_stack = segmentation_map_to_image(seg_results[0], COLORMAP)

if cv2.imwrite(f&quot;{DATA_DIR}/segmentation_results.jpg&quot;, mask_stack):
    print(&quot;The segmentation result image has been saved as \&quot;segmentation_results.jpg\&quot; in data&quot;)
    plt.imshow(mask_stack)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">segmentation</span> <span class="n">result</span> <span class="n">image</span> <span class="n">has</span> <span class="n">been</span> <span class="n">saved</span> <span class="k">as</span> <span class="s2">&quot;segmentation_results.jpg&quot;</span> <span class="ow">in</span> <span class="n">data</span>
</pre></div>
</div>
<img alt="../_images/203-meter-reader-with-output_19_1.png" src="../_images/203-meter-reader-with-output_19_1.png" />
</section>
<section id="postprocess-the-models-result-and-calculate-the-final-readings">
<h3>Postprocess the models result and calculate the final readings<a class="headerlink" href="#postprocess-the-models-result-and-calculate-the-final-readings" title="Permalink to this headline">¶</a></h3>
<p>Use OpenCV function to find the location of the pointer in scale map.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Find the pointer location in scale map and calculate the meters reading
rectangle_meters = circle_to_rectangle(seg_results)
line_scales, line_pointers = rectangle_to_line(rectangle_meters)
binaried_scales = mean_binarization(line_scales)
binaried_pointers = mean_binarization(line_pointers)
scale_locations = locate_scale(binaried_scales)
pointer_locations = locate_pointer(binaried_pointers)
pointed_scales = get_relative_location(scale_locations, pointer_locations)
meter_readings = calculate_reading(pointed_scales)

# Plot the rectangle meters
if len(rectangle_meters) == 2:
    rectangle_meters_stack = np.hstack([segmentation_map_to_image(rectangle_meters[0], COLORMAP),
                                        segmentation_map_to_image(rectangle_meters[1], COLORMAP)])
else:
    rectangle_meters_stack = segmentation_map_to_image(rectangle_meters[0], COLORMAP)

if cv2.imwrite(f&quot;{DATA_DIR}/rectangle_meters.jpg&quot;, rectangle_meters_stack):
    print(&quot;The rectangle_meters result image has been saved as \&quot;rectangle_meters.jpg\&quot; in data&quot;)
    plt.imshow(rectangle_meters_stack)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">rectangle_meters</span> <span class="n">result</span> <span class="n">image</span> <span class="n">has</span> <span class="n">been</span> <span class="n">saved</span> <span class="k">as</span> <span class="s2">&quot;rectangle_meters.jpg&quot;</span> <span class="ow">in</span> <span class="n">data</span>
</pre></div>
</div>
<img alt="../_images/203-meter-reader-with-output_21_1.png" src="../_images/203-meter-reader-with-output_21_1.png" />
</section>
<section id="get-the-reading-result-on-the-meter-picture">
<h3>Get the reading result on the meter picture<a class="headerlink" href="#get-the-reading-result-on-the-meter-picture" title="Permalink to this headline">¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create final result photo with meter value
for i in range(len(meter_readings)):
    print(&quot;Meter {}: {:.3f}&quot;.format(i + 1, meter_readings[i]))

result_image = image.copy()
for i in range(len(loc)):
    cv2.rectangle(result_image,(loc[i][0], loc[i][1]), (loc[i][2], loc[i][3]), (0, 150, 0), 3)
    font = cv2.FONT_HERSHEY_SIMPLEX
    cv2.rectangle(result_image, (loc[i][0], loc[i][1]), (loc[i][0] + 100, loc[i][1] + 40), (0, 150, 0), -1)
    cv2.putText(result_image, &quot;#{:.3f}&quot;.format(meter_readings[i]), (loc[i][0],loc[i][1] + 25), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)
if cv2.imwrite(f&quot;{DATA_DIR}/reading_results.jpg&quot;, result_image):
    print(&quot;The reading results image has been saved as \&quot;reading_results.jpg\&quot; in data&quot;)
    plt.imshow(result_image)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Meter</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">1.100</span>
<span class="n">Meter</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">6.185</span>
<span class="n">The</span> <span class="n">reading</span> <span class="n">results</span> <span class="n">image</span> <span class="n">has</span> <span class="n">been</span> <span class="n">saved</span> <span class="k">as</span> <span class="s2">&quot;reading_results.jpg&quot;</span> <span class="ow">in</span> <span class="n">data</span>
</pre></div>
</div>
<img alt="../_images/203-meter-reader-with-output_23_1.png" src="../_images/203-meter-reader-with-output_23_1.png" />
<p>## Try it with your meter photos!</p>
</section>
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  
    <a class='button bttn-sec button-size-l' id="prev-link" href="202-vision-superresolution-video-with-output.html" title="previous page">Prev</a>
    <a class='button bttn-sec button-size-l' id="next-link" href="204-named-entity-recognition-with-output.html" title="next page">Next</a>

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel®.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>