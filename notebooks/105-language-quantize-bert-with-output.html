
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Quantize NLP models with Post-Training Optimization Tool ​in OpenVINO™ &#8212; OpenVINO™  documentation</title>
    
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="../_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="../_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/tabs.css" type="text/css" />
    <script src="../_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="../_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="../_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/tabs.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/custom.js"></script>
    <script src="../_static/js/graphs.js"></script>
    <script src="../_static/js/graphs_ov_tf.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/target-highlight.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://docs.openvino.ai/latest/notebooks/105-language-quantize-bert-with-output.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Automatic Device Selection with OpenVINO™" href="106-auto-device-with-output.html" />
    <link rel="prev" title="Working with Open Model Zoo Models" href="104-model-tools-with-output.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/get-started-guide.html">
  Get Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/documentation.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api/api_reference.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../model_zoo.html">
  Model Zoo
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/resources.html">
  Resources
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/openvinotoolkit/openvino" rel="noopener" target="_blank" title="GitHub">
            <span><i class="sst-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="language-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">English</button>
  <div class="dropdown-menu" aria-labelledby="language-selector">
    
      
        <a class="dropdown-item font-weight-bold" href="/openvino-docs/index.html">English</a>
      
    
      
        <a  class="dropdown-item" href="/cn/openvino-docs/index.html">Chinese</a>
      
    
  </div>
</div>

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="../search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notebooks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks-installation.html">
   Installation of OpenVINO™ Notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="001-hello-world-with-output.html">
   Hello Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002-openvino-api-with-output.html">
   OpenVINO™ Runtime API Tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003-hello-segmentation-with-output.html">
   Hello Image Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="004-hello-detection-with-output.html">
   Hello Object Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="101-tensorflow-to-openvino-with-output.html">
   Convert a TensorFlow Model to OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="102-pytorch-onnx-to-openvino-with-output.html">
   Convert a PyTorch Model to ONNX and OpenVINO™ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="103-paddle-onnx-to-openvino-classification-with-output.html">
   Convert a PaddlePaddle Model to ONNX and OpenVINO™ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="104-model-tools-with-output.html">
   Working with Open Model Zoo Models
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Quantize NLP models with Post-Training Optimization Tool ​in OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="106-auto-device-with-output.html">
   Automatic Device Selection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="107-speech-recognition-quantization-with-output.html">
   Quantize Speech Recognition Models with OpenVINO™ Post-Training Optimization Tool ​
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-nncf-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="111-detection-quantization-with-output.html">
   Object Detection Quantization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="112-pytorch-post-training-quantization-nncf-with-output.html">
   Post-Training Quantization of PyTorch models with NNCF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="113-image-classification-quantization-with-output.html">
   Quantization of Image Classification Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="114-quantization-simplified-mode-with-output.html">
   INT8 Quantization with Post-training Optimization Tool (POT) in Simplified Mode tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="115-async-api-with-output.html">
   Asynchronous Inference with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="201-vision-monodepth-with-output.html">
   Monodepth Estimation with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-image-with-output.html">
   Single Image Super Resolution with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-video-with-output.html">
   Video Super Resolution with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="203-meter-reader-with-output.html">
   Industrial Meter Reader
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="204-named-entity-recognition-with-output.html">
   Document Entity Extraction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="205-vision-background-removal-with-output.html">
   Image Background Removal with U^2-Net and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="206-vision-paddlegan-anime-with-output.html">
   Photos to Anime with PaddleGAN and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="207-vision-paddlegan-superresolution-with-output.html">
   Super Resolution with PaddleGAN and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="208-optical-character-recognition-with-output.html">
   Optical Character Recognition (OCR) with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="209-handwritten-ocr-with-output.html">
   Handwritten Chinese and Japanese OCR with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="210-ct-scan-live-inference-with-output.html">
   Live Inference and Benchmark CT-scan Data with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="211-speech-to-text-with-output.html">
   Speech to Text with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="212-onnx-style-transfer-with-output.html">
   Style Transfer on ONNX Models with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="214-vision-paddle-classification-with-output.html">
   PaddlePaddle Image Classification with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="215-image-inpainting-with-output.html">
   Image In-painting with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="216-license-plate-recognition-with-output.html">
   License Plate Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="217-vision-deblur-with-output.html">
   Deblur Photos with DeblurGAN-v2 and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="218-vehicle-detection-and-recognition-with-output.html">
   Vehicle Detection And Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="219-knowledge-graphs-conve-with-output.html">
   OpenVINO optimizations for Knowledge graphs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="220-yolov5-accuracy-check-and-quantization-with-output.html">
   Quantize the Ultralytics YOLOv5 model and check accuracy using the OpenVINO POT API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="221-machine-translation-with-output.html">
   Machine translation demo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="222-vision-image-colorization-with-output.html">
   Image Colorization with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="223-gpt2-text-prediction-with-output.html">
   GPT-2 Text Prediction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-pot-with-output.html">
   Post-Training Quantization with TensorFlow Classification Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-with-output.html">
   From Training to Deployment with TensorFlow and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="302-pytorch-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using PyTorch framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="305-tensorflow-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using TensorFlow Framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="401-object-detection-with-output.html">
   Live Object Detection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="402-pose-estimation-with-output.html">
   Live Human Pose Estimation with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="403-action-recognition-webcam-with-output.html">
   Human Action Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="405-paddle-ocr-webcam-with-output.html">
   PaddleOCR with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebook_utils-with-output.html">
   Notebook Utils
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#settings">
   Settings
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prepare-the-model">
   Prepare the Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convert-the-onnx-model-to-openvino-ir">
   Convert the ONNX Model to OpenVINO IR
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prepare-mrpc-task-dataset">
   Prepare MRPC Task Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#define-dataloader-for-pot">
   Define DataLoader for POT
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#define-accuracy-metric-calculation">
   Define Accuracy Metric Calculation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#run-quantization-pipeline">
   Run Quantization Pipeline
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-and-test-openvino-model">
   Load and Test OpenVINO Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#compare-performance-of-the-original-converted-and-quantized-models">
   Compare Performance of the Original, Converted and Quantized Models
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">

<div class="tocsection editthispage">
    <a href="None">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

            
                <div>
                  
  <section id="quantize-nlp-models-with-post-training-optimization-tool-in-openvino">
<h1>Quantize NLP models with Post-Training Optimization Tool ​in OpenVINO™<a class="headerlink" href="#quantize-nlp-models-with-post-training-optimization-tool-in-openvino" title="Permalink to this headline">¶</a></h1>
<p>This tutorial demonstrates how to apply <code class="docutils literal notranslate"><span class="pre">INT8</span></code> quantization to the
Natural Language Processing model known as
<a class="reference external" href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a>, using
the <a class="reference external" href="https://docs.openvino.ai/latest/pot_compression_api_README.html">Post-Training Optimization Tool
API</a>
(part of the <a class="reference external" href="https://docs.openvino.ai/">OpenVINO Toolkit</a>). A
fine-tuned <a class="reference external" href="https://huggingface.co/transformers/model_doc/bert.html">HuggingFace
BERT</a>
<a class="reference external" href="https://pytorch.org/">PyTorch</a> model, trained on the <a class="reference external" href="https://www.microsoft.com/en-us/download/details.aspx?id=52398">Microsoft
Research Paraphrase Corpus
(MRPC)</a>,
will be used. The tutorial is designed to be extendable to custom models
and datasets. It consists of the following steps:</p>
<ul class="simple">
<li><p>Download and prepare the BERT model and MRPC dataset.</p></li>
<li><p>Define data loading and accuracy validation functionality.</p></li>
<li><p>Prepare the model for quantization.</p></li>
<li><p>Run optimization pipeline.</p></li>
<li><p>Load and test quantized model.</p></li>
<li><p>Compare the performance of the original, converted and quantized
models.</p></li>
</ul>
<section id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">¶</a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import os
import sys
import time
import warnings
from pathlib import Path
from zipfile import ZipFile

import numpy as np
import torch
from addict import Dict
from compression.api import DataLoader as POTDataLoader
from compression.api import Metric
from compression.engines.ie_engine import IEEngine
from compression.graph import load_model, save_model
from compression.graph.model_utils import compress_model_weights
from compression.pipeline.initializer import create_pipeline
from openvino import runtime as ov
from torch.utils.data import TensorDataset
from transformers import BertForSequenceClassification, BertTokenizer
from transformers import (
    glue_convert_examples_to_features as convert_examples_to_features,
)
from transformers import glue_output_modes as output_modes
from transformers import glue_processors as processors

sys.path.append(&quot;../utils&quot;)
from notebook_utils import download_file
</pre></div>
</div>
</section>
<section id="settings">
<h2>Settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Set the data and model directories, source URL and the filename of the model.
DATA_DIR = &quot;data&quot;
MODEL_DIR = &quot;model&quot;
MODEL_LINK = &quot;https://download.pytorch.org/tutorial/MRPC.zip&quot;
FILE_NAME = MODEL_LINK.split(&quot;/&quot;)[-1]

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)
</pre></div>
</div>
</section>
<section id="prepare-the-model">
<h2>Prepare the Model<a class="headerlink" href="#prepare-the-model" title="Permalink to this headline">¶</a></h2>
<p>Perform the following: - Download and unpack pre-trained BERT model for
MRPC by PyTorch. - Convert the model to the ONNX. - Run Model Optimizer
to convert the model from the ONNX representation to the OpenVINO
Intermediate Representation (OpenVINO IR)</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>download_file(MODEL_LINK, directory=MODEL_DIR, show_progress=True)
with ZipFile(f&quot;{MODEL_DIR}/{FILE_NAME}&quot;, &quot;r&quot;) as zip_ref:
    zip_ref.extractall(MODEL_DIR)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>model/MRPC.zip:   0%|          | 0.00/387M [00:00&lt;?, ?B/s]
</pre></div>
</div>
<p>Import all dependencies to load the original PyTorch model and convert
it to the ONNX representation.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>BATCH_SIZE = 1
MAX_SEQ_LENGTH = 128


def export_model_to_onnx(model, path):
    with torch.no_grad():
        default_input = torch.ones(1, MAX_SEQ_LENGTH, dtype=torch.int64)
        inputs = {
            &quot;input_ids&quot;: default_input,
            &quot;attention_mask&quot;: default_input,
            &quot;token_type_ids&quot;: default_input,
        }
        symbolic_names = {0: &quot;batch_size&quot;, 1: &quot;max_seq_len&quot;}
        torch.onnx.export(
            model,
            (inputs[&quot;input_ids&quot;], inputs[&quot;attention_mask&quot;], inputs[&quot;token_type_ids&quot;]),
            path,
            opset_version=11,
            do_constant_folding=True,
            input_names=[&quot;input_ids&quot;, &quot;input_mask&quot;, &quot;segment_ids&quot;],
            output_names=[&quot;output&quot;],
            dynamic_axes={
                &quot;input_ids&quot;: symbolic_names,
                &quot;input_mask&quot;: symbolic_names,
                &quot;segment_ids&quot;: symbolic_names,
            },
        )
        print(&quot;ONNX model saved to {}&quot;.format(path))


torch_model = BertForSequenceClassification.from_pretrained(os.path.join(MODEL_DIR, &quot;MRPC&quot;))
onnx_model_path = Path(MODEL_DIR) / &quot;bert_mrpc.onnx&quot;
if not onnx_model_path.exists():
    export_model_to_onnx(torch_model, onnx_model_path)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ONNX</span> <span class="n">model</span> <span class="n">saved</span> <span class="n">to</span> <span class="n">model</span><span class="o">/</span><span class="n">bert_mrpc</span><span class="o">.</span><span class="n">onnx</span>
</pre></div>
</div>
</section>
<section id="convert-the-onnx-model-to-openvino-ir">
<h2>Convert the ONNX Model to OpenVINO IR<a class="headerlink" href="#convert-the-onnx-model-to-openvino-ir" title="Permalink to this headline">¶</a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>ir_model_xml = onnx_model_path.with_suffix(&quot;.xml&quot;)
ir_model_bin = onnx_model_path.with_suffix(&quot;.bin&quot;)

# Convert the ONNX model to OpenVINO IR FP32.
if not ir_model_xml.exists():
    !mo --input_model $onnx_model_path --output_dir $MODEL_DIR --model_name $ir_model_xml.stem --input input_ids,input_mask,segment_ids --input_shape [1,128],[1,128],[1,128] --output output --data_type FP32
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Model</span> <span class="n">Optimizer</span> <span class="n">arguments</span><span class="p">:</span>
<span class="n">Common</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="o">-</span> <span class="n">Path</span> <span class="n">to</span> <span class="n">the</span> <span class="n">Input</span> <span class="n">Model</span><span class="p">:</span>  <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">105</span><span class="o">-</span><span class="n">language</span><span class="o">-</span><span class="n">quantize</span><span class="o">-</span><span class="n">bert</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">bert_mrpc</span><span class="o">.</span><span class="n">onnx</span>
    <span class="o">-</span> <span class="n">Path</span> <span class="k">for</span> <span class="n">generated</span> <span class="n">IR</span><span class="p">:</span>    <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">105</span><span class="o">-</span><span class="n">language</span><span class="o">-</span><span class="n">quantize</span><span class="o">-</span><span class="n">bert</span><span class="o">/</span><span class="n">model</span>
    <span class="o">-</span> <span class="n">IR</span> <span class="n">output</span> <span class="n">name</span><span class="p">:</span>   <span class="n">bert_mrpc</span>
    <span class="o">-</span> <span class="n">Log</span> <span class="n">level</span><span class="p">:</span>    <span class="n">ERROR</span>
    <span class="o">-</span> <span class="n">Batch</span><span class="p">:</span>    <span class="n">Not</span> <span class="n">specified</span><span class="p">,</span> <span class="n">inherited</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">model</span>
    <span class="o">-</span> <span class="n">Input</span> <span class="n">layers</span><span class="p">:</span>     <span class="n">input_ids</span><span class="p">,</span><span class="n">input_mask</span><span class="p">,</span><span class="n">segment_ids</span>
    <span class="o">-</span> <span class="n">Output</span> <span class="n">layers</span><span class="p">:</span>    <span class="n">output</span>
    <span class="o">-</span> <span class="n">Input</span> <span class="n">shapes</span><span class="p">:</span>     <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">128</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">128</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">128</span><span class="p">]</span>
    <span class="o">-</span> <span class="n">Source</span> <span class="n">layout</span><span class="p">:</span>    <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Target</span> <span class="n">layout</span><span class="p">:</span>    <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Layout</span><span class="p">:</span>   <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Mean</span> <span class="n">values</span><span class="p">:</span>  <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Scale</span> <span class="n">values</span><span class="p">:</span>     <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Scale</span> <span class="n">factor</span><span class="p">:</span>     <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Precision</span> <span class="n">of</span> <span class="n">IR</span><span class="p">:</span>  <span class="n">FP32</span>
    <span class="o">-</span> <span class="n">Enable</span> <span class="n">fusing</span><span class="p">:</span>    <span class="kc">True</span>
    <span class="o">-</span> <span class="n">User</span> <span class="n">transformations</span><span class="p">:</span>     <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Reverse</span> <span class="nb">input</span> <span class="n">channels</span><span class="p">:</span>   <span class="kc">False</span>
    <span class="o">-</span> <span class="n">Enable</span> <span class="n">IR</span> <span class="n">generation</span> <span class="k">for</span> <span class="n">fixed</span> <span class="nb">input</span> <span class="n">shape</span><span class="p">:</span>   <span class="kc">False</span>
    <span class="o">-</span> <span class="n">Use</span> <span class="n">the</span> <span class="n">transformations</span> <span class="n">config</span> <span class="n">file</span><span class="p">:</span>  <span class="kc">None</span>
<span class="n">Advanced</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="o">-</span> <span class="n">Force</span> <span class="n">the</span> <span class="n">usage</span> <span class="n">of</span> <span class="n">legacy</span> <span class="n">Frontend</span> <span class="n">of</span> <span class="n">Model</span> <span class="n">Optimizer</span> <span class="k">for</span> <span class="n">model</span> <span class="n">conversion</span> <span class="n">into</span> <span class="n">IR</span><span class="p">:</span>   <span class="kc">False</span>
    <span class="o">-</span> <span class="n">Force</span> <span class="n">the</span> <span class="n">usage</span> <span class="n">of</span> <span class="n">new</span> <span class="n">Frontend</span> <span class="n">of</span> <span class="n">Model</span> <span class="n">Optimizer</span> <span class="k">for</span> <span class="n">model</span> <span class="n">conversion</span> <span class="n">into</span> <span class="n">IR</span><span class="p">:</span>  <span class="kc">False</span>
<span class="n">OpenVINO</span> <span class="n">runtime</span> <span class="n">found</span> <span class="ow">in</span><span class="p">:</span>  <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/.</span><span class="n">venv</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">openvino</span>
<span class="n">OpenVINO</span> <span class="n">runtime</span> <span class="n">version</span><span class="p">:</span>   <span class="mf">2022.1.0</span><span class="o">-</span><span class="mi">7019</span><span class="o">-</span><span class="n">cdb9bec7210</span><span class="o">-</span><span class="n">releases</span><span class="o">/</span><span class="mi">2022</span><span class="o">/</span><span class="mi">1</span>
<span class="n">Model</span> <span class="n">Optimizer</span> <span class="n">version</span><span class="p">:</span>    <span class="mf">2022.1.0</span><span class="o">-</span><span class="mi">7019</span><span class="o">-</span><span class="n">cdb9bec7210</span><span class="o">-</span><span class="n">releases</span><span class="o">/</span><span class="mi">2022</span><span class="o">/</span><span class="mi">1</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">Generated</span> <span class="n">IR</span> <span class="n">version</span> <span class="mi">11</span> <span class="n">model</span><span class="o">.</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">XML</span> <span class="n">file</span><span class="p">:</span> <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">105</span><span class="o">-</span><span class="n">language</span><span class="o">-</span><span class="n">quantize</span><span class="o">-</span><span class="n">bert</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">bert_mrpc</span><span class="o">.</span><span class="n">xml</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">BIN</span> <span class="n">file</span><span class="p">:</span> <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">105</span><span class="o">-</span><span class="n">language</span><span class="o">-</span><span class="n">quantize</span><span class="o">-</span><span class="n">bert</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">bert_mrpc</span><span class="o">.</span><span class="n">bin</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">Total</span> <span class="n">execution</span> <span class="n">time</span><span class="p">:</span> <span class="mf">1.88</span> <span class="n">seconds</span><span class="o">.</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">Memory</span> <span class="n">consumed</span><span class="p">:</span> <span class="mi">938</span> <span class="n">MB</span><span class="o">.</span>
<span class="n">It</span><span class="s1">&#39;s been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&amp;source=prod&amp;campid=ww_2022_bu_IOTG_OpenVINO-2022-1&amp;content=upg_all&amp;medium=organic or on the GitHub*</span>
<span class="p">[</span> <span class="n">INFO</span> <span class="p">]</span> <span class="n">The</span> <span class="n">model</span> <span class="n">was</span> <span class="n">converted</span> <span class="n">to</span> <span class="n">IR</span> <span class="n">v11</span><span class="p">,</span> <span class="n">the</span> <span class="n">latest</span> <span class="n">model</span> <span class="nb">format</span> <span class="n">that</span> <span class="n">corresponds</span> <span class="n">to</span> <span class="n">the</span> <span class="n">source</span> <span class="n">DL</span> <span class="n">framework</span> <span class="nb">input</span><span class="o">/</span><span class="n">output</span> <span class="nb">format</span><span class="o">.</span> <span class="n">While</span> <span class="n">IR</span> <span class="n">v11</span> <span class="ow">is</span> <span class="n">backwards</span> <span class="n">compatible</span> <span class="k">with</span> <span class="n">OpenVINO</span> <span class="n">Inference</span> <span class="n">Engine</span> <span class="n">API</span> <span class="n">v1</span><span class="mf">.0</span><span class="p">,</span> <span class="n">please</span> <span class="n">use</span> <span class="n">API</span> <span class="n">v2</span><span class="mf">.0</span> <span class="p">(</span><span class="k">as</span> <span class="n">of</span> <span class="mf">2022.1</span><span class="p">)</span> <span class="n">to</span> <span class="n">take</span> <span class="n">advantage</span> <span class="n">of</span> <span class="n">the</span> <span class="n">latest</span> <span class="n">improvements</span> <span class="ow">in</span> <span class="n">IR</span> <span class="n">v11</span><span class="o">.</span>
<span class="n">Find</span> <span class="n">more</span> <span class="n">information</span> <span class="n">about</span> <span class="n">API</span> <span class="n">v2</span><span class="mf">.0</span> <span class="ow">and</span> <span class="n">IR</span> <span class="n">v11</span> <span class="n">at</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">docs</span><span class="o">.</span><span class="n">openvino</span><span class="o">.</span><span class="n">ai</span>
</pre></div>
</div>
</section>
<section id="prepare-mrpc-task-dataset">
<h2>Prepare MRPC Task Dataset<a class="headerlink" href="#prepare-mrpc-task-dataset" title="Permalink to this headline">¶</a></h2>
<p>To run this tutorial, you will need to download the General Language
Understanding Evaluation (GLUE) data for the MRPC task from HuggingFace.
Use the code below to download a script that fetches the MRPC dataset.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>download_file(
    &quot;https://raw.githubusercontent.com/huggingface/transformers/f98ef14d161d7bcdc9808b5ec399981481411cc1/utils/download_glue_data.py&quot;,
    show_progress=False,
)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PosixPath</span><span class="p">(</span><span class="s1">&#39;/opt/home/k8sworker/cibuilds/ov-notebook/OVNotebookOps-231/.workspace/scm/ov-notebook/notebooks/105-language-quantize-bert/download_glue_data.py&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from download_glue_data import format_mrpc

format_mrpc(DATA_DIR, &quot;&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Processing MRPC...
Local MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt
    Completed!
</pre></div>
</div>
</section>
<section id="define-dataloader-for-pot">
<h2>Define DataLoader for POT<a class="headerlink" href="#define-dataloader-for-pot" title="Permalink to this headline">¶</a></h2>
<p>In this step, you define <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> based on POT API. It will be
used to collect statistics for quantization and run model evaluation.
Use helper functions from the HuggingFace Transformers to do the data
preprocessing. It takes raw text data and encodes sentences and words,
producing three model inputs. For more details about the data
preprocessing and tokenization, refer to this
<a class="reference external" href="https://medium.com/&#64;dhartidhami/understanding-bert-word-embeddings-7dc4d2ea54ca">description</a>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class MRPCDataLoader(POTDataLoader):
    # Required methods
    def __init__(self, config):
        &quot;&quot;&quot;Constructor
        :param config: data loader specific config
        &quot;&quot;&quot;
        if not isinstance(config, Dict):
            config = Dict(config)
        super().__init__(config)
        self._task = config[&quot;task&quot;].lower()
        self._model_dir = config[&quot;model_dir&quot;]
        self._data_dir = config[&quot;data_source&quot;]
        self._batch_size = config[&quot;batch_size&quot;]
        self._max_length = config[&quot;max_length&quot;]
        self.examples = []
        self._prepare_dataset()

    def __len__(self):
        &quot;&quot;&quot;Returns size of the dataset&quot;&quot;&quot;
        return len(self.dataset)

    def __getitem__(self, index):
        &quot;&quot;&quot;
        Returns annotation, data and metadata at the specified index.
        Possible formats:
        (index, annotation), data
        (index, annotation), data, metadata
        &quot;&quot;&quot;
        if index &gt;= len(self):
            raise IndexError

        batch = self.dataset[index]
        batch = tuple(t.detach().cpu().numpy() for t in batch)
        inputs = {&quot;input_ids&quot;: batch[0], &quot;input_mask&quot;: batch[1], &quot;segment_ids&quot;: batch[2]}
        labels = batch[3]
        return (index, labels), inputs

    # Methods specific to the current implementation
    def _prepare_dataset(self):
        &quot;&quot;&quot;Prepare dataset&quot;&quot;&quot;
        tokenizer = BertTokenizer.from_pretrained(self._model_dir, do_lower_case=True)
        processor = processors[self._task]()
        output_mode = output_modes[self._task]
        label_list = processor.get_labels()
        examples = processor.get_dev_examples(self._data_dir)
        features = convert_examples_to_features(
            examples,
            tokenizer,
            label_list=label_list,
            max_length=self._max_length,
            output_mode=output_mode,
        )
        all_input_ids = torch.unsqueeze(torch.tensor([f.input_ids for f in features], dtype=torch.long), 1)
        all_attention_mask = torch.unsqueeze(torch.tensor([f.attention_mask for f in features], dtype=torch.long), 1)
        all_token_type_ids = torch.unsqueeze(torch.tensor([f.token_type_ids for f in features], dtype=torch.long), 1)
        all_labels = torch.unsqueeze(torch.tensor([f.label for f in features], dtype=torch.long), 1)
        self.dataset = TensorDataset(
            all_input_ids, all_attention_mask, all_token_type_ids, all_labels
        )
        self.examples = examples
</pre></div>
</div>
</section>
<section id="define-accuracy-metric-calculation">
<h2>Define Accuracy Metric Calculation<a class="headerlink" href="#define-accuracy-metric-calculation" title="Permalink to this headline">¶</a></h2>
<p>In this step the <code class="docutils literal notranslate"><span class="pre">Metric</span></code> interface for MRPC task metrics is
implemented. It is used for validating the accuracy of the models.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class Accuracy(Metric):

    # Required methods
    def __init__(self):
        super().__init__()
        self._name = &quot;Accuracy&quot;
        self._matches = []

    @property
    def value(self):
        &quot;&quot;&quot;Returns accuracy metric value for the last model output.&quot;&quot;&quot;
        return {self._name: self._matches[-1]}

    @property
    def avg_value(self):
        &quot;&quot;&quot;Returns accuracy metric value for all model outputs.&quot;&quot;&quot;
        return {self._name: np.ravel(self._matches).mean()}

    def update(self, output, target):
        &quot;&quot;&quot;
        Updates prediction matches.

        :param output: model output
        :param target: annotations
        &quot;&quot;&quot;
        if len(output) &gt; 1:
            raise Exception(
                &quot;The accuracy metric cannot be calculated &quot; &quot;for a model with multiple outputs&quot;
            )
        output = np.argmax(output)
        match = output == target[0]
        self._matches.append(match)

    def reset(self):
        &quot;&quot;&quot;
        Resets collected matches
        &quot;&quot;&quot;
        self._matches = []

    def get_attributes(self):
        &quot;&quot;&quot;
        Returns a dictionary of metric attributes {metric_name: {attribute_name: value}}.
        Required attributes: &#39;direction&#39;: &#39;higher-better&#39; or &#39;higher-worse&#39;
                             &#39;type&#39;: metric type
        &quot;&quot;&quot;
        return {self._name: {&quot;direction&quot;: &quot;higher-better&quot;, &quot;type&quot;: &quot;accuracy&quot;}}
</pre></div>
</div>
</section>
<section id="run-quantization-pipeline">
<h2>Run Quantization Pipeline<a class="headerlink" href="#run-quantization-pipeline" title="Permalink to this headline">¶</a></h2>
<p>Define a configuration for the quantization pipeline and run it. Keep in
mind that built-in <code class="docutils literal notranslate"><span class="pre">IEEngine</span></code> implementation of <code class="docutils literal notranslate"><span class="pre">Engine</span></code> interface
from the POT API for model inference is used here.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>warnings.filterwarnings(&quot;ignore&quot;)  # Suppress accuracychecker warnings.

model_config = Dict({&quot;model_name&quot;: &quot;bert_mrpc&quot;, &quot;model&quot;: ir_model_xml, &quot;weights&quot;: ir_model_bin})
engine_config = Dict({&quot;device&quot;: &quot;CPU&quot;})
dataset_config = {
    &quot;task&quot;: &quot;mrpc&quot;,
    &quot;data_source&quot;: os.path.join(DATA_DIR, &quot;MRPC&quot;),
    &quot;model_dir&quot;: os.path.join(MODEL_DIR, &quot;MRPC&quot;),
    &quot;batch_size&quot;: BATCH_SIZE,
    &quot;max_length&quot;: MAX_SEQ_LENGTH,
}
algorithms = [
    {
        &quot;name&quot;: &quot;DefaultQuantization&quot;,
        &quot;params&quot;: {
            &quot;target_device&quot;: &quot;ANY&quot;,
            &quot;model_type&quot;: &quot;transformer&quot;,
            &quot;preset&quot;: &quot;performance&quot;,
            &quot;stat_subset_size&quot;: 250,
        },
    }
]


# Step 1: Load the model.
model = load_model(model_config=model_config)

# Step 2: Initialize the data loader.
data_loader = MRPCDataLoader(config=dataset_config)

# Step 3 (Optional. Required for AccuracyAwareQuantization): Initialize the metric.
metric = Accuracy()

# Step 4: Initialize the engine for metric calculation and statistics collection.
engine = IEEngine(config=engine_config, data_loader=data_loader, metric=metric)

# Step 5: Create a pipeline of compression algorithms.
pipeline = create_pipeline(algo_config=algorithms, engine=engine)

# Step 6 (Optional): Evaluate the original model. Print the results.
fp_results = pipeline.evaluate(model=model)
if fp_results:
    print(&quot;FP32 model results:&quot;)
    for name, value in fp_results.items():
        print(f&quot;{name}: {value:.5f}&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">FP32</span> <span class="n">model</span> <span class="n">results</span><span class="p">:</span>
<span class="n">Accuracy</span><span class="p">:</span> <span class="mf">0.86029</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Step 7: Execute the pipeline.
warnings.filterwarnings(&quot;ignore&quot;)  # Suppress accuracychecker warnings.
print(
    f&quot;Quantizing model with {algorithms[0][&#39;params&#39;][&#39;preset&#39;]} preset and {algorithms[0][&#39;name&#39;]}&quot;
)
start_time = time.perf_counter()
compressed_model = pipeline.run(model=model)
end_time = time.perf_counter()
print(f&quot;Quantization finished in {end_time - start_time:.2f} seconds&quot;)

# Step 8 (Optional): Compress model weights to quantized precision
#                    in order to reduce the size of the final .bin file.
compress_model_weights(model=compressed_model)

# Step 9: Save the compressed model to the desired path.
compressed_model_paths = save_model(model=compressed_model, save_path=MODEL_DIR, model_name=&quot;quantized_bert_mrpc&quot;
)
compressed_model_xml = compressed_model_paths[0][&quot;model&quot;]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Quantizing</span> <span class="n">model</span> <span class="k">with</span> <span class="n">performance</span> <span class="n">preset</span> <span class="ow">and</span> <span class="n">DefaultQuantization</span>
<span class="n">Quantization</span> <span class="n">finished</span> <span class="ow">in</span> <span class="mf">58.65</span> <span class="n">seconds</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Step 10 (Optional): Evaluate the compressed model and print the results.
int_results = pipeline.evaluate(model=compressed_model)

if int_results:
    print(&quot;INT8 model results:&quot;)
    for name, value in int_results.items():
        print(f&quot;{name}: {value:.5f}&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">INT8</span> <span class="n">model</span> <span class="n">results</span><span class="p">:</span>
<span class="n">Accuracy</span><span class="p">:</span> <span class="mf">0.86029</span>
</pre></div>
</div>
</section>
<section id="load-and-test-openvino-model">
<h2>Load and Test OpenVINO Model<a class="headerlink" href="#load-and-test-openvino-model" title="Permalink to this headline">¶</a></h2>
<p>To load and test converted model, perform the following: * Load the
model and compile it for CPU. * Prepare the input. * Run the
inference. * Get the answer from the model output.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>core = ov.Core()

# Read the model from files.
model = core.read_model(model=compressed_model_xml)

# Assign dynamic shapes to every input layer.
for input_layer in model.inputs:
    input_shape = input_layer.partial_shape
    input_shape[1] = -1
    model.reshape({input_layer: input_shape})

# Compile the model for a specific device.
compiled_model_int8 = core.compile_model(model=model, device_name=&quot;CPU&quot;)

output_layer = compiled_model_int8.outputs[0]
</pre></div>
</div>
<p>The Data Loader returns a pair of sentences (indicated by
<code class="docutils literal notranslate"><span class="pre">sample_idx</span></code>) and the inference compares these sentences and outputs
whether their meaning is the same. You can test other sentences by
changing <code class="docutils literal notranslate"><span class="pre">sample_idx</span></code> to another value (from 0 to 407).</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>sample_idx = 5

sample = data_loader.examples[sample_idx]
inputs = data_loader[sample_idx][1]

result = compiled_model_int8(inputs)[output_layer]
result = np.argmax(result)

print(f&quot;Text 1: {sample.text_a}&quot;)
print(f&quot;Text 2: {sample.text_b}&quot;)
print(f&quot;The same meaning: {&#39;yes&#39; if result == 1 else &#39;no&#39;}&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Text</span> <span class="mi">1</span><span class="p">:</span> <span class="n">Wal</span><span class="o">-</span><span class="n">Mart</span> <span class="n">said</span> <span class="n">it</span> <span class="n">would</span> <span class="n">check</span> <span class="nb">all</span> <span class="n">of</span> <span class="n">its</span> <span class="n">million</span><span class="o">-</span><span class="n">plus</span> <span class="n">domestic</span> <span class="n">workers</span> <span class="n">to</span> <span class="n">ensure</span> <span class="n">they</span> <span class="n">were</span> <span class="n">legally</span> <span class="n">employed</span> <span class="o">.</span>
<span class="n">Text</span> <span class="mi">2</span><span class="p">:</span> <span class="n">It</span> <span class="n">has</span> <span class="n">also</span> <span class="n">said</span> <span class="n">it</span> <span class="n">would</span> <span class="n">review</span> <span class="nb">all</span> <span class="n">of</span> <span class="n">its</span> <span class="n">domestic</span> <span class="n">employees</span> <span class="n">more</span> <span class="n">than</span> <span class="mi">1</span> <span class="n">million</span> <span class="n">to</span> <span class="n">ensure</span> <span class="n">they</span> <span class="n">have</span> <span class="n">legal</span> <span class="n">status</span> <span class="o">.</span>
<span class="n">The</span> <span class="n">same</span> <span class="n">meaning</span><span class="p">:</span> <span class="n">yes</span>
</pre></div>
</div>
</section>
<section id="compare-performance-of-the-original-converted-and-quantized-models">
<h2>Compare Performance of the Original, Converted and Quantized Models<a class="headerlink" href="#compare-performance-of-the-original-converted-and-quantized-models" title="Permalink to this headline">¶</a></h2>
<p>Compare the original PyTorch model with OpenVINO converted and quantized
models (<code class="docutils literal notranslate"><span class="pre">FP32</span></code>, <code class="docutils literal notranslate"><span class="pre">INT8</span></code>) to see the difference in performance. It is
expressed in Sentences Per Second (SPS) measure, which is the same as
Frames Per Second (FPS) for images.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model = core.read_model(model=ir_model_xml)

# Assign dynamic shapes to every input layer.
for input_layer in model.inputs:
    input_shape = input_layer.partial_shape
    input_shape[1] = -1
    model.reshape({input_layer: input_shape})

# Compile the model for a specific device.
compiled_model_fp32 = core.compile_model(model=model, device_name=&quot;CPU&quot;)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>num_samples = 50
inputs = data_loader[0][1]

with torch.no_grad():
    start = time.perf_counter()
    for _ in range(num_samples):
        torch_model(torch.as_tensor(list(inputs.values())).squeeze())
    end = time.perf_counter()
    time_torch = end - start
print(
    f&quot;PyTorch model on CPU: {time_torch / num_samples:.3f} seconds per sentence, &quot;
    f&quot;SPS: {num_samples / time_torch:.2f}&quot;
)

start = time.perf_counter()
for _ in range(num_samples):
    compiled_model_fp32(inputs)
end = time.perf_counter()
time_ir = end - start
print(
    f&quot;IR FP32 model in OpenVINO Runtime/CPU: {time_ir / num_samples:.3f} &quot;
    f&quot;seconds per sentence, SPS: {num_samples / time_ir:.2f}&quot;
)

start = time.perf_counter()
for _ in range(num_samples):
    compiled_model_int8(inputs)
end = time.perf_counter()
time_ir = end - start
print(
    f&quot;OpenVINO IR INT8 model in OpenVINO Runtime/CPU: {time_ir / num_samples:.3f} &quot;
    f&quot;seconds per sentence, SPS: {num_samples / time_ir:.2f}&quot;
)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PyTorch</span> <span class="n">model</span> <span class="n">on</span> <span class="n">CPU</span><span class="p">:</span> <span class="mf">0.170</span> <span class="n">seconds</span> <span class="n">per</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">SPS</span><span class="p">:</span> <span class="mf">5.87</span>
<span class="n">IR</span> <span class="n">FP32</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">OpenVINO</span> <span class="n">Runtime</span><span class="o">/</span><span class="n">CPU</span><span class="p">:</span> <span class="mf">0.025</span> <span class="n">seconds</span> <span class="n">per</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">SPS</span><span class="p">:</span> <span class="mf">40.24</span>
<span class="n">OpenVINO</span> <span class="n">IR</span> <span class="n">INT8</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">OpenVINO</span> <span class="n">Runtime</span><span class="o">/</span><span class="n">CPU</span><span class="p">:</span> <span class="mf">0.011</span> <span class="n">seconds</span> <span class="n">per</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">SPS</span><span class="p">:</span> <span class="mf">87.17</span>
</pre></div>
</div>
<p>Finally, measure the inference performance of OpenVINO <code class="docutils literal notranslate"><span class="pre">FP32</span></code> and
<code class="docutils literal notranslate"><span class="pre">INT8</span></code> models. For this purpose, use <a class="reference external" href="https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html">Benchmark
Tool</a>
in OpenVINO.</p>
<blockquote>
<div><p><strong>Note</strong>: The <code class="docutils literal notranslate"><span class="pre">benchmark_app</span></code> tool is able to measure the
performance of the OpenVINO Intermediate Representation (OpenVINO IR)
models only. For more accurate performance, run <code class="docutils literal notranslate"><span class="pre">benchmark_app</span></code> in
a terminal/command prompt after closing other applications. Run
<code class="docutils literal notranslate"><span class="pre">benchmark_app</span> <span class="pre">-m</span> <span class="pre">model.xml</span> <span class="pre">-d</span> <span class="pre">CPU</span></code> to benchmark async inference on
CPU for one minute. Change <code class="docutils literal notranslate"><span class="pre">CPU</span></code> to <code class="docutils literal notranslate"><span class="pre">GPU</span></code> to benchmark on GPU.
Run <code class="docutils literal notranslate"><span class="pre">benchmark_app</span> <span class="pre">--help</span></code> to see an overview of all command-line
options.</p>
</div></blockquote>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Inference FP32 model (OpenVINO IR)
! benchmark_app -m $ir_model_xml -d CPU -api sync
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[Step 1/11] Parsing and validating input arguments
[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README.
[Step 2/11] Loading OpenVINO
[ WARNING ] PerformanceMode was not explicitly specified in command line. Device CPU performance hint will be set to LATENCY.
[ INFO ] OpenVINO:
         API version............. 2022.1.0-7019-cdb9bec7210-releases/2022/1
[ INFO ] Device info
         CPU
         openvino_intel_cpu_plugin version 2022.1
         Build................... 2022.1.0-7019-cdb9bec7210-releases/2022/1

[Step 3/11] Setting device configuration
[Step 4/11] Reading network files
[ INFO ] Read model took 175.40 ms
[Step 5/11] Resizing network to match image sizes and given batch
[ INFO ] Network batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model input &#39;input_ids&#39; precision i64, dimensions ([...]): 1 128
[ INFO ] Model input &#39;input_mask&#39; precision i64, dimensions ([...]): 1 128
[ INFO ] Model input &#39;segment_ids&#39; precision i64, dimensions ([...]): 1 128
[ INFO ] Model output &#39;output&#39; precision f32, dimensions ([...]): 1 2
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 168.16 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] DEVICE: CPU
[ INFO ]   AVAILABLE_DEVICES  , [&#39;&#39;]
[ INFO ]   RANGE_FOR_ASYNC_INFER_REQUESTS  , (1, 1, 1)
[ INFO ]   RANGE_FOR_STREAMS  , (1, 24)
[ INFO ]   FULL_DEVICE_NAME  , Intel(R) Core(TM) i9-10920X CPU @ 3.50GHz
[ INFO ]   OPTIMIZATION_CAPABILITIES  , [&#39;WINOGRAD&#39;, &#39;FP32&#39;, &#39;FP16&#39;, &#39;INT8&#39;, &#39;BIN&#39;, &#39;EXPORT_IMPORT&#39;]
[ INFO ]   CACHE_DIR  ,
[ INFO ]   NUM_STREAMS  , 1
[ INFO ]   INFERENCE_NUM_THREADS  , 0
[ INFO ]   PERF_COUNT  , False
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS  , 0
[Step 9/11] Creating infer requests and preparing input data
[ INFO ] Create 1 infer requests took 0.12 ms
[ WARNING ] No input files were given for input &#39;input_ids&#39;!. This input will be filled with random values!
[ WARNING ] No input files were given for input &#39;input_mask&#39;!. This input will be filled with random values!
[ WARNING ] No input files were given for input &#39;segment_ids&#39;!. This input will be filled with random values!
[ INFO ] Fill input &#39;input_ids&#39; with random values
[ INFO ] Fill input &#39;input_mask&#39; with random values
[ INFO ] Fill input &#39;segment_ids&#39; with random values
[Step 10/11] Measuring performance (Start inference synchronously, inference only: True, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 21.71 ms
[Step 11/11] Dumping statistics report
Count:          2417 iterations
Duration:       60025.84 ms
Latency:
    Median:     23.95 ms
    AVG:        24.76 ms
    MIN:        20.94 ms
    MAX:        28.82 ms
Throughput: 41.75 FPS
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Inference INT8 model (OpenVINO IR)
! benchmark_app -m $compressed_model_xml -d CPU -api sync
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[Step 1/11] Parsing and validating input arguments
[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README.
[Step 2/11] Loading OpenVINO
[ WARNING ] PerformanceMode was not explicitly specified in command line. Device CPU performance hint will be set to LATENCY.
[ INFO ] OpenVINO:
         API version............. 2022.1.0-7019-cdb9bec7210-releases/2022/1
[ INFO ] Device info
         CPU
         openvino_intel_cpu_plugin version 2022.1
         Build................... 2022.1.0-7019-cdb9bec7210-releases/2022/1

[Step 3/11] Setting device configuration
[Step 4/11] Reading network files
[ INFO ] Read model took 94.03 ms
[Step 5/11] Resizing network to match image sizes and given batch
[ INFO ] Network batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model input &#39;input_ids&#39; precision i64, dimensions ([...]): 1 128
[ INFO ] Model input &#39;input_mask&#39; precision i64, dimensions ([...]): 1 128
[ INFO ] Model input &#39;segment_ids&#39; precision i64, dimensions ([...]): 1 128
[ INFO ] Model output &#39;output&#39; precision f32, dimensions ([...]): 1 2
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 244.16 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] DEVICE: CPU
[ INFO ]   AVAILABLE_DEVICES  , [&#39;&#39;]
[ INFO ]   RANGE_FOR_ASYNC_INFER_REQUESTS  , (1, 1, 1)
[ INFO ]   RANGE_FOR_STREAMS  , (1, 24)
[ INFO ]   FULL_DEVICE_NAME  , Intel(R) Core(TM) i9-10920X CPU @ 3.50GHz
[ INFO ]   OPTIMIZATION_CAPABILITIES  , [&#39;WINOGRAD&#39;, &#39;FP32&#39;, &#39;FP16&#39;, &#39;INT8&#39;, &#39;BIN&#39;, &#39;EXPORT_IMPORT&#39;]
[ INFO ]   CACHE_DIR  ,
[ INFO ]   NUM_STREAMS  , 1
[ INFO ]   INFERENCE_NUM_THREADS  , 0
[ INFO ]   PERF_COUNT  , False
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS  , 0
[Step 9/11] Creating infer requests and preparing input data
[ INFO ] Create 1 infer requests took 0.12 ms
[ WARNING ] No input files were given for input &#39;input_ids&#39;!. This input will be filled with random values!
[ WARNING ] No input files were given for input &#39;input_mask&#39;!. This input will be filled with random values!
[ WARNING ] No input files were given for input &#39;segment_ids&#39;!. This input will be filled with random values!
[ INFO ] Fill input &#39;input_ids&#39; with random values
[ INFO ] Fill input &#39;input_mask&#39; with random values
[ INFO ] Fill input &#39;segment_ids&#39; with random values
[Step 10/11] Measuring performance (Start inference synchronously, inference only: True, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 11.09 ms
[Step 11/11] Dumping statistics report
Count:          6041 iterations
Duration:       60005.38 ms
Latency:
    Median:     9.93 ms
    AVG:        9.86 ms
    MIN:        8.59 ms
    MAX:        11.61 ms
Throughput: 100.69 FPS
</pre></div>
</div>
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  
    <a class='button bttn-sec button-size-l' id="prev-link" href="104-model-tools-with-output.html" title="previous page">Prev</a>
    <a class='button bttn-sec button-size-l' id="next-link" href="106-auto-device-with-output.html" title="next page">Next</a>

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel®.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>