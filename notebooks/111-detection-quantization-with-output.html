
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Object Detection Quantization &#8212; OpenVINO™  documentation</title>
    
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="../_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="../_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/tabs.css" type="text/css" />
    <script src="../_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="../_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="../_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/tabs.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/custom.js"></script>
    <script src="../_static/js/graphs.js"></script>
    <script src="../_static/js/graphs_ov_tf.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/target-highlight.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://docs.openvino.ai/latest/notebooks/111-detection-quantization-with-output.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Post-Training Quantization of PyTorch models with NNCF" href="112-pytorch-post-training-quantization-nncf-with-output.html" />
    <link rel="prev" title="Quantize a Segmentation Model and Show Live Inference" href="110-ct-segmentation-quantize-with-output.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/get-started-guide.html">
  Get Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/documentation.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api/api_reference.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../model_zoo.html">
  Model Zoo
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/resources.html">
  Resources
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/openvinotoolkit/openvino" rel="noopener" target="_blank" title="GitHub">
            <span><i class="sst-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="language-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">English</button>
  <div class="dropdown-menu" aria-labelledby="language-selector">
    
      
        <a class="dropdown-item font-weight-bold" href="/openvino-docs/index.html">English</a>
      
    
      
        <a  class="dropdown-item" href="/cn/openvino-docs/index.html">Chinese</a>
      
    
  </div>
</div>

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="../search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notebooks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks-installation.html">
   Installation of OpenVINO™ Notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="001-hello-world-with-output.html">
   Hello Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002-openvino-api-with-output.html">
   OpenVINO™ Runtime API Tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003-hello-segmentation-with-output.html">
   Hello Image Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="004-hello-detection-with-output.html">
   Hello Object Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="101-tensorflow-to-openvino-with-output.html">
   Convert a TensorFlow Model to OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="102-pytorch-onnx-to-openvino-with-output.html">
   Convert a PyTorch Model to ONNX and OpenVINO™ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="103-paddle-onnx-to-openvino-classification-with-output.html">
   Convert a PaddlePaddle Model to ONNX and OpenVINO™ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="104-model-tools-with-output.html">
   Working with Open Model Zoo Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="105-language-quantize-bert-with-output.html">
   Quantize NLP models with Post-Training Optimization Tool ​in OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="106-auto-device-with-output.html">
   Automatic Device Selection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="107-speech-recognition-quantization-with-output.html">
   Quantize Speech Recognition Models with OpenVINO™ Post-Training Optimization Tool ​
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-nncf-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Object Detection Quantization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="112-pytorch-post-training-quantization-nncf-with-output.html">
   Post-Training Quantization of PyTorch models with NNCF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="113-image-classification-quantization-with-output.html">
   Quantization of Image Classification Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="114-quantization-simplified-mode-with-output.html">
   INT8 Quantization with Post-training Optimization Tool (POT) in Simplified Mode tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="115-async-api-with-output.html">
   Asynchronous Inference with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="201-vision-monodepth-with-output.html">
   Monodepth Estimation with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-image-with-output.html">
   Single Image Super Resolution with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-video-with-output.html">
   Video Super Resolution with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="203-meter-reader-with-output.html">
   Industrial Meter Reader
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="204-named-entity-recognition-with-output.html">
   Document Entity Extraction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="205-vision-background-removal-with-output.html">
   Image Background Removal with U^2-Net and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="206-vision-paddlegan-anime-with-output.html">
   Photos to Anime with PaddleGAN and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="207-vision-paddlegan-superresolution-with-output.html">
   Super Resolution with PaddleGAN and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="208-optical-character-recognition-with-output.html">
   Optical Character Recognition (OCR) with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="209-handwritten-ocr-with-output.html">
   Handwritten Chinese and Japanese OCR with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="210-ct-scan-live-inference-with-output.html">
   Live Inference and Benchmark CT-scan Data with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="211-speech-to-text-with-output.html">
   Speech to Text with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="212-onnx-style-transfer-with-output.html">
   Style Transfer on ONNX Models with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="214-vision-paddle-classification-with-output.html">
   PaddlePaddle Image Classification with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="215-image-inpainting-with-output.html">
   Image In-painting with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="216-license-plate-recognition-with-output.html">
   License Plate Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="217-vision-deblur-with-output.html">
   Deblur Photos with DeblurGAN-v2 and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="218-vehicle-detection-and-recognition-with-output.html">
   Vehicle Detection And Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="219-knowledge-graphs-conve-with-output.html">
   OpenVINO optimizations for Knowledge graphs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="220-yolov5-accuracy-check-and-quantization-with-output.html">
   Quantize the Ultralytics YOLOv5 model and check accuracy using the OpenVINO POT API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="221-machine-translation-with-output.html">
   Machine translation demo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="222-vision-image-colorization-with-output.html">
   Image Colorization with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="223-gpt2-text-prediction-with-output.html">
   GPT-2 Text Prediction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-pot-with-output.html">
   Post-Training Quantization with TensorFlow Classification Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-with-output.html">
   From Training to Deployment with TensorFlow and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="302-pytorch-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using PyTorch framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="305-tensorflow-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using TensorFlow Framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="401-object-detection-with-output.html">
   Live Object Detection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="402-pose-estimation-with-output.html">
   Live Human Pose Estimation with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="403-action-recognition-webcam-with-output.html">
   Human Action Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="405-paddle-ocr-webcam-with-output.html">
   PaddleOCR with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebook_utils-with-output.html">
   Notebook Utils
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preparation">
   Preparation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#imports">
     Imports
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#download-model">
     Download Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-model">
     Load Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#post-training-optimization-tool-pot-quantization">
   Post-Training Optimization Tool (POT) Quantization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#configuration">
     Configuration
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dataset">
       Dataset
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#metric">
       Metric
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#quantization-config">
       Quantization Config
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#run-quantization-pipeline">
     Run Quantization Pipeline
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#compare-metric-of-floating-point-and-quantized-model">
   Compare Metric of Floating Point and Quantized Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#visualize-results">
   Visualize Results
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#compare-the-size-of-the-original-and-quantized-models">
   Compare the Size of the Original and Quantized Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#compare-performance-of-the-original-and-quantized-models">
   Compare Performance of the Original and Quantized Models
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">

<div class="tocsection editthispage">
    <a href="None">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

            
                <div>
                  
  <section id="object-detection-quantization">
<h1>Object Detection Quantization<a class="headerlink" href="#object-detection-quantization" title="Permalink to this headline">¶</a></h1>
<p>This tutorial shows how to quantize an object detection model, using
<a class="reference external" href="https://docs.openvino.ai/2021.4/pot_compression_api_README.html">Post-Training Optimization Tool
API</a>
in OpenVINO™.</p>
<p>For demonstration purposes, a very small dataset of 10 images presenting
people at the airport is used. The images have been resized from the
original resolution of 1920x1080 to 960x540. For any real use cases, a
representative dataset of about 300 images is recommended. The tutorial
uses the
<a class="reference external" href="https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/intel/person-detection-retail-0013">person-detection-retail-0013</a>
model.</p>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline">¶</a></h2>
<section id="imports">
<h3>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import json
import sys
import time
from pathlib import Path
from typing import Sequence, Tuple

import addict
import cv2
import matplotlib.pyplot as plt
import numpy as np
import torch
import torchmetrics
from compression.api import DataLoader, Metric
from compression.engines.ie_engine import IEEngine
from compression.graph import load_model, save_model
from compression.graph.model_utils import compress_model_weights
from compression.pipeline.initializer import create_pipeline
from openvino.runtime import Core
from yaspin import yaspin

sys.path.append(&quot;../utils&quot;)
from notebook_utils import benchmark_model
</pre></div>
</div>
</section>
<section id="download-model">
<h3>Download Model<a class="headerlink" href="#download-model" title="Permalink to this headline">¶</a></h3>
<p>Download the model from Open Model Zoo if it is not already in the
system.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>ir_path = Path(&quot;intel/person-detection-retail-0013/FP32/person-detection-retail-0013.xml&quot;)

if not ir_path.exists():
    ! omz_downloader --name &quot;person-detection-retail-0013&quot; --precisions FP32
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">################|| Downloading person-detection-retail-0013 ||################</span>

<span class="o">==========</span> <span class="n">Downloading</span> <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">111</span><span class="o">-</span><span class="n">detection</span><span class="o">-</span><span class="n">quantization</span><span class="o">/</span><span class="n">intel</span><span class="o">/</span><span class="n">person</span><span class="o">-</span><span class="n">detection</span><span class="o">-</span><span class="n">retail</span><span class="o">-</span><span class="mi">0013</span><span class="o">/</span><span class="n">FP32</span><span class="o">/</span><span class="n">person</span><span class="o">-</span><span class="n">detection</span><span class="o">-</span><span class="n">retail</span><span class="o">-</span><span class="mf">0013.</span><span class="n">xml</span>


<span class="o">==========</span> <span class="n">Downloading</span> <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">111</span><span class="o">-</span><span class="n">detection</span><span class="o">-</span><span class="n">quantization</span><span class="o">/</span><span class="n">intel</span><span class="o">/</span><span class="n">person</span><span class="o">-</span><span class="n">detection</span><span class="o">-</span><span class="n">retail</span><span class="o">-</span><span class="mi">0013</span><span class="o">/</span><span class="n">FP32</span><span class="o">/</span><span class="n">person</span><span class="o">-</span><span class="n">detection</span><span class="o">-</span><span class="n">retail</span><span class="o">-</span><span class="mf">0013.</span><span class="n">bin</span>
</pre></div>
</div>
</section>
<section id="load-model">
<h3>Load Model<a class="headerlink" href="#load-model" title="Permalink to this headline">¶</a></h3>
<p>Load the OpenVINO IR model, and get information about network inputs and
outputs.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>ie = Core()
model = ie.read_model(model=ir_path)
compiled_model = ie.compile_model(model=model, device_name=&quot;CPU&quot;)
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)
input_size = input_layer.shape
_, _, input_height, input_width = input_size
</pre></div>
</div>
</section>
</section>
<section id="post-training-optimization-tool-pot-quantization">
<h2>Post-Training Optimization Tool (POT) Quantization<a class="headerlink" href="#post-training-optimization-tool-pot-quantization" title="Permalink to this headline">¶</a></h2>
<p>The Post-Training Optimization Tool (POT) <code class="docutils literal notranslate"><span class="pre">compression</span></code> API defines
base classes for <code class="docutils literal notranslate"><span class="pre">Metric</span></code> and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>. This notebook uses a
custom <code class="docutils literal notranslate"><span class="pre">Metric</span></code> and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> class that implement all the
required methods.</p>
<p>To implement <code class="docutils literal notranslate"><span class="pre">Metric</span></code> and <code class="docutils literal notranslate"><span class="pre">Dataloader</span></code>, you need to know the outputs
of the model and the annotation format.</p>
<p>The dataset in this example uses annotations in <code class="docutils literal notranslate"><span class="pre">JSON</span></code> format, with
keys: <code class="docutils literal notranslate"><span class="pre">['categories',</span> <span class="pre">'annotations',</span> <span class="pre">'images']</span></code>. The <code class="docutils literal notranslate"><span class="pre">annotations</span></code>
key is a list of dictionaries, with one item per annotation. Such item
contains a <code class="docutils literal notranslate"><span class="pre">boxes</span></code> key, which holds the prediction boxes, in the
<code class="docutils literal notranslate"><span class="pre">[xmin,</span> <span class="pre">xmax,</span> <span class="pre">ymin,</span> <span class="pre">ymax]</span></code> format. In this dataset, there is only one
label: “person”.</p>
<p>The <a class="reference external" href="https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/intel/person-detection-retail-0013">model
documentation</a>
specifies that the model returns an array of shape <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">1,</span> <span class="pre">200,</span> <span class="pre">7]</span></code>
where 200 is the number of detected boxes. Each detection has the format
of <code class="docutils literal notranslate"><span class="pre">[image_id,</span> <span class="pre">label,</span> <span class="pre">conf,</span> <span class="pre">x_min,</span> <span class="pre">y_min,</span> <span class="pre">x_max,</span> <span class="pre">y_max]</span></code>. For this
dataset, the label of <code class="docutils literal notranslate"><span class="pre">1</span></code> indicates a person.</p>
<section id="configuration">
<h3>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h3>
<section id="dataset">
<h4>Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">DetectionDataLoader</span></code> class follows <code class="docutils literal notranslate"><span class="pre">compression.api.DataLoader</span></code>
interface in POT, which should implement <code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>
and <code class="docutils literal notranslate"><span class="pre">__len__</span></code>, where <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> should return data as
<code class="docutils literal notranslate"><span class="pre">(annotation,</span> <span class="pre">image)</span></code> or optionally <code class="docutils literal notranslate"><span class="pre">(annotation,</span> <span class="pre">image,</span> <span class="pre">metadata)</span></code>,
with the annotation as <code class="docutils literal notranslate"><span class="pre">(index,</span> <span class="pre">label)</span></code>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class DetectionDataLoader(DataLoader):
    def __init__(self, basedir: str, target_size: Tuple[int, int]):
        &quot;&quot;&quot;
        :param basedir: Directory that contains images and annotation as &quot;annotation.json&quot;
        :param target_size: Tuple of (width, height) to resize images to.
        &quot;&quot;&quot;
        self.images = sorted(Path(basedir).glob(&quot;*.jpg&quot;))
        self.target_size = target_size
        with open(f&quot;{basedir}/annotation_person_train.json&quot;) as f:
            self.annotations = json.load(f)
        self.image_ids = {
            Path(item[&quot;file_name&quot;]).name: item[&quot;id&quot;]
            for item in self.annotations[&quot;images&quot;]
        }

        for image_filename in self.images:
            annotations = [
                item
                for item in self.annotations[&quot;annotations&quot;]
                if item[&quot;image_id&quot;] == self.image_ids[Path(image_filename).name]
            ]
            assert (
                len(annotations) != 0
            ), f&quot;No annotations found for image id {image_filename}&quot;

        print(
            f&quot;Created dataset with {len(self.images)} items. Data directory: {basedir}&quot;
        )

    def __getitem__(self, index):
        &quot;&quot;&quot;
        Get an item from the dataset at the specified index.
        Detection boxes are converted from absolute coordinates to relative coordinates
        between 0 and 1 by dividing xmin, xmax by image width and ymin, ymax by image height.

        :return: (annotation, input_image, metadata) where annotation is (index, target_annotation)
                 with target_annotation as a dictionary with keys category_id, image_width, image_height
                 and bbox, containing the relative bounding box coordinates [xmin, ymin, xmax, ymax]
                 (with values between 0 and 1) and metadata a dictionary: {&quot;filename&quot;: path_to_image}
        &quot;&quot;&quot;
        image_path = self.images[index]
        image = cv2.imread(str(image_path))
        image = cv2.resize(image, self.target_size)
        image_id = self.image_ids[Path(image_path).name]

        # The `image_info` key contains height and width of the annotated image.
        image_info = [
            image for image in self.annotations[&quot;images&quot;] if image[&quot;id&quot;] == image_id
        ][0]
        # The `image_annotations` key contains the boxes and labels for the image.
        image_annotations = [
            item
            for item in self.annotations[&quot;annotations&quot;]
            if item[&quot;image_id&quot;] == image_id
        ]

        # The annotations are in xmin, ymin, width, height format. Convert to
        # xmin, ymin, xmax, ymax and normalize to image width and height as
        # stored in the annotation.
        target_annotations = []
        for annotation in image_annotations:
            xmin, ymin, width, height = annotation[&quot;bbox&quot;]
            xmax = xmin + width
            ymax = ymin + height
            xmin /= image_info[&quot;width&quot;]
            ymin /= image_info[&quot;height&quot;]
            xmax /= image_info[&quot;width&quot;]
            ymax /= image_info[&quot;height&quot;]
            target_annotation = {}
            target_annotation[&quot;category_id&quot;] = annotation[&quot;category_id&quot;]
            target_annotation[&quot;image_width&quot;] = image_info[&quot;width&quot;]
            target_annotation[&quot;image_height&quot;] = image_info[&quot;height&quot;]
            target_annotation[&quot;bbox&quot;] = [xmin, ymin, xmax, ymax]
            target_annotations.append(target_annotation)

        item_annotation = (index, target_annotations)
        input_image = np.expand_dims(image.transpose(2, 0, 1), axis=0).astype(
            np.float32
        )
        return (
            item_annotation,
            input_image,
            {&quot;filename&quot;: str(image_path), &quot;shape&quot;: image.shape},
        )

    def __len__(self):
        return len(self.images)
</pre></div>
</div>
</section>
<section id="metric">
<h4>Metric<a class="headerlink" href="#metric" title="Permalink to this headline">¶</a></h4>
<p>Define a metric to determine the performance of a model. For the Default
Quantization algorithm used in this notebook, defining a metric is
optional, but it can be used to compare the quantized <code class="docutils literal notranslate"><span class="pre">INT8</span></code> model
with the original FP OpenVINO IR model.</p>
<p>This tutorial uses the Mean Average Precision (MAP) metric from
<a class="reference external" href="https://torchmetrics.readthedocs.io/en/latest/references/modules.html#detection-metrics">TorchMetrics</a>.</p>
<p>A metric for POT inherits from <code class="docutils literal notranslate"><span class="pre">compression.api.Metric</span></code> and should
implement all the methods in this example.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class MAPMetric(Metric):
    def __init__(self, map_value=&quot;map&quot;):
        &quot;&quot;&quot;
        Mean Average Precision Metric. Wraps torchmetrics implementation, see
        https://torchmetrics.readthedocs.io/en/latest/references/modules.html#map

        :map_value: specific metric to return. Default: &quot;map&quot;
                    Change `to one of the values in the list below to return a different value
                    [&#39;mar_1&#39;, &#39;mar_10&#39;, &#39;mar_100&#39;, &#39;mar_small&#39;, &#39;mar_medium&#39;, &#39;mar_large&#39;,
                     &#39;map&#39;, &#39;map_50&#39;, &#39;map_75&#39;, &#39;map_small&#39;, &#39;map_medium&#39;, &#39;map_large&#39;]
                    See torchmetrics documentation for more details.
        &quot;&quot;&quot;
        assert (
            map_value
            in torchmetrics.detection.map.MARMetricResults.__slots__
            + torchmetrics.detection.map.MAPMetricResults.__slots__
        )

        self._name = map_value
        self.metric = torchmetrics.detection.map.MAP()
        super().__init__()

    @property
    def value(self):
        &quot;&quot;&quot;
        Returns metric value for the last model output.
        Possible format: {metric_name: [metric_values_per_image]}
        &quot;&quot;&quot;
        return {self._name: [0]}

    @property
    def avg_value(self):
        &quot;&quot;&quot;
        Returns average metric value for all model outputs.
        Possible format: {metric_name: metric_value}
        &quot;&quot;&quot;
        return {self._name: self.metric.compute()[self._name].item()}

    def update(self, output, target):
        &quot;&quot;&quot;
        Convert network output and labels to the format that torchmetrics&#39; MAP
        implementation expects, and call `metric.update()`.

        :param output: model output
        :param target: annotations for model output
        &quot;&quot;&quot;
        targetboxes = []
        targetlabels = []
        predboxes = []
        predlabels = []
        scores = []

        image_width = target[0][0][&quot;image_width&quot;]
        image_height = target[0][0][&quot;image_height&quot;]

        for single_target in target[0]:
            txmin, tymin, txmax, tymax = single_target[&quot;bbox&quot;]
            category = single_target[&quot;category_id&quot;]
            txmin *= image_width
            txmax *= image_width
            tymin *= image_height
            tymax *= image_height

            targetbox = [round(txmin), round(tymin), round(txmax), round(tymax)]
            targetboxes.append(targetbox)
            targetlabels.append(category)

        for single_output in output:
            for pred in single_output[0, 0, ::]:
                image_id, label, conf, xmin, ymin, xmax, ymax = pred
                xmin *= image_width
                xmax *= image_width
                ymin *= image_height
                ymax *= image_height

                predbox = [round(xmin), round(ymin), round(xmax), round(ymax)]
                predboxes.append(predbox)
                predlabels.append(label)
                scores.append(conf)

        preds = [
            dict(
                boxes=torch.Tensor(predboxes).float(),
                labels=torch.Tensor(predlabels).short(),
                scores=torch.Tensor(scores),
            )
        ]
        targets = [
            dict(
                boxes=torch.Tensor(targetboxes).float(),
                labels=torch.Tensor(targetlabels).short(),
            )
        ]
        self.metric.update(preds, targets)

    def reset(self):
        &quot;&quot;&quot;
        Resets metric
        &quot;&quot;&quot;
        self.metric.reset()

    def get_attributes(self):
        &quot;&quot;&quot;
        Returns a dictionary of metric attributes {metric_name: {attribute_name: value}}.
        Required attributes: &#39;direction&#39;: &#39;higher-better&#39; or &#39;higher-worse&#39;
                             &#39;type&#39;: metric type
        &quot;&quot;&quot;
        return {self._name: {&quot;direction&quot;: &quot;higher-better&quot;, &quot;type&quot;: &quot;mAP&quot;}}
</pre></div>
</div>
</section>
<section id="quantization-config">
<h4>Quantization Config<a class="headerlink" href="#quantization-config" title="Permalink to this headline">¶</a></h4>
<p>The POT methods expect configuration dictionaries as arguments, which
are defined in the cell below. The <code class="docutils literal notranslate"><span class="pre">ir_path</span></code> variable points to the
<code class="docutils literal notranslate"><span class="pre">.xml</span></code> file of the OpenVINO IR model. It is defined at the top of the
notebook. This tutorial uses the <code class="docutils literal notranslate"><span class="pre">DefaultQuantization</span></code> algorithm.</p>
<p>For more information about the settings and best practices, refer to the
<a class="reference external" href="https://docs.openvino.ai/2021.4/pot_docs_BestPractices.html">Post-Training Optimization Best
Practices</a>
and the main <a class="reference external" href="https://docs.openvino.ai/2021.4/pot_README.html">POT
documentation</a>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Model config specifies the name of the model and paths to `.xml` and `.bin` files of the model.
model_config = addict.Dict(
    {
        &quot;model_name&quot;: ir_path.stem,
        &quot;model&quot;: ir_path,
        &quot;weights&quot;: ir_path.with_suffix(&quot;.bin&quot;),
    }
)

# Engine config
engine_config = addict.Dict({&quot;device&quot;: &quot;CPU&quot;})

# Standard DefaultQuantization config. For this tutorial, stat_subset_size is ignored
# because there are fewer than 300 images. For production use, 300 is recommended.
default_algorithms = [
    {
        &quot;name&quot;: &quot;DefaultQuantization&quot;,
        &quot;stat_subset_size&quot;: 300,
        &quot;params&quot;: {
            &quot;target_device&quot;: &quot;ANY&quot;,
            &quot;preset&quot;: &quot;mixed&quot;,  # Choose between &quot;mixed&quot; and &quot;performance&quot;.
        },
    }
]

print(f&quot;model_config: {model_config}&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_config</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;model_name&#39;</span><span class="p">:</span> <span class="s1">&#39;person-detection-retail-0013&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="n">PosixPath</span><span class="p">(</span><span class="s1">&#39;intel/person-detection-retail-0013/FP32/person-detection-retail-0013.xml&#39;</span><span class="p">),</span> <span class="s1">&#39;weights&#39;</span><span class="p">:</span> <span class="n">PosixPath</span><span class="p">(</span><span class="s1">&#39;intel/person-detection-retail-0013/FP32/person-detection-retail-0013.bin&#39;</span><span class="p">)}</span>
</pre></div>
</div>
</section>
</section>
<section id="run-quantization-pipeline">
<h3>Run Quantization Pipeline<a class="headerlink" href="#run-quantization-pipeline" title="Permalink to this headline">¶</a></h3>
<p>The POT pipeline uses the functions: <code class="docutils literal notranslate"><span class="pre">load_model()</span></code>, <code class="docutils literal notranslate"><span class="pre">IEEngine</span></code>, and
<code class="docutils literal notranslate"><span class="pre">create_pipeline()</span></code>. The <code class="docutils literal notranslate"><span class="pre">load_model()</span></code> function loads an OpenVINO
IR model specified in <code class="docutils literal notranslate"><span class="pre">model_config</span></code>. <code class="docutils literal notranslate"><span class="pre">IEEngine</span></code> is a POT
implementation of OpenVINO Runtime that will be passed to the POT
pipeline created by the <code class="docutils literal notranslate"><span class="pre">create_pipeline()</span></code> function. The POT classes
and functions expect a config argument. These configs are created in the
Config section in the cell above. <code class="docutils literal notranslate"><span class="pre">MAPMetric</span></code> metric and
<code class="docutils literal notranslate"><span class="pre">DetectionDataLoader</span></code> have been defined earlier in this notebook.</p>
<p>Creating and running the POT quantization pipeline takes just two lines
of code. First, create the pipeline with the <code class="docutils literal notranslate"><span class="pre">create_pipeline</span></code>
function, and then run that pipeline with <code class="docutils literal notranslate"><span class="pre">pipeline.run()</span></code>. To reuse
the quantized model later, compress the model weights and save the
compressed model to a disk.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Step 1: Create the data loader.
data_loader = DetectionDataLoader(
    basedir=&quot;data&quot;, target_size=(input_width, input_height)
)

# Step 2: Load the model.
ir_model = load_model(model_config=model_config)

# Step 3: Initialize the metric.
# For DefaultQuantization, specifying a metric is optional: metric can be set to None.
metric = MAPMetric(map_value=&quot;map&quot;)

# Step 4: Initialize the engine for metric calculation and statistics collection.
engine = IEEngine(config=engine_config, data_loader=data_loader, metric=metric)

# Step 5: Create a pipeline of compression algorithms.
# The `default_algorithms` parameter is defined in the Config cell above this cell.
pipeline = create_pipeline(default_algorithms, engine)

# Step 6: Execute the pipeline to quantize the model.
algorithm_name = pipeline.algo_seq[0].name
with yaspin(
    text=f&quot;Executing POT pipeline on {model_config[&#39;model&#39;]} with {algorithm_name}&quot;
) as sp:
    start_time = time.perf_counter()
    compressed_model = pipeline.run(ir_model)
    end_time = time.perf_counter()
    sp.ok(&quot;✔&quot;)
print(f&quot;Quantization finished in {end_time - start_time:.2f} seconds&quot;)

# Step 7 (Optional): Compress model weights to quantized precision
#                    in order to reduce the size of the final `.bin` file.
compress_model_weights(compressed_model)

# Step 8: Save the compressed model to the desired path.
# Set `save_path` to the directory where the compressed model should be stored.
preset = pipeline._algo_seq[0].config[&quot;preset&quot;]
algorithm_name = pipeline.algo_seq[0].name
compressed_model_paths = save_model(
    model=compressed_model,
    save_path=&quot;optimized_model&quot;,
    model_name=f&quot;{ir_model.name}_{preset}_{algorithm_name}&quot;,
)

compressed_model_path = compressed_model_paths[0][&quot;model&quot;]
print(&quot;The quantized model is stored at&quot;, compressed_model_path)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Created dataset with 10 items. Data directory: data
✔ Executing POT pipeline on intel/person-detection-retail-0013/FP32/person-detection-retail-0013.xml with DefaultQuantization
Quantization finished in 36.70 seconds
The quantized model is stored at optimized_model/person-detection-retail-0013_mixed_DefaultQuantization.xml
</pre></div>
</div>
</section>
</section>
<section id="compare-metric-of-floating-point-and-quantized-model">
<h2>Compare Metric of Floating Point and Quantized Model<a class="headerlink" href="#compare-metric-of-floating-point-and-quantized-model" title="Permalink to this headline">¶</a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Compute the mAP on the quantized model and compare with the mAP on the FP16 OpenVINO IR model.
ir_model = load_model(model_config=model_config)
evaluation_pipeline = create_pipeline(algo_config=dict(), engine=engine)

with yaspin(text=&quot;Evaluating original IR model&quot;) as sp:
    original_metric = evaluation_pipeline.evaluate(ir_model)

with yaspin(text=&quot;Evaluating quantized IR model&quot;) as sp:
    quantized_metric = pipeline.evaluate(compressed_model)

if original_metric:
    for key, value in original_metric.items():
        print(f&quot;The {key} score of the original FP16 model is {value:.5f}&quot;)

if quantized_metric:
    for key, value in quantized_metric.items():
        print(f&quot;The {key} score of the quantized INT8 model is {value:.5f}&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="nb">map</span> <span class="n">score</span> <span class="n">of</span> <span class="n">the</span> <span class="n">original</span> <span class="n">FP16</span> <span class="n">model</span> <span class="ow">is</span> <span class="mf">0.67329</span>
<span class="n">The</span> <span class="nb">map</span> <span class="n">score</span> <span class="n">of</span> <span class="n">the</span> <span class="n">quantized</span> <span class="n">INT8</span> <span class="n">model</span> <span class="ow">is</span> <span class="mf">0.65735</span>
</pre></div>
</div>
</section>
<section id="visualize-results">
<h2>Visualize Results<a class="headerlink" href="#visualize-results" title="Permalink to this headline">¶</a></h2>
<p>Compare the annotated boxes (green) with the results of the floating
point (red) and quantized (green) models. First, define a helper
function to draw the boxes on an image, using the specified color. Then,
do inference on five images and show the results. The figure shows three
images for every input image: the left image shows the annotation and
both <code class="docutils literal notranslate"><span class="pre">FP</span></code> and <code class="docutils literal notranslate"><span class="pre">INT8</span></code> predictions, the middle image shows the
floating point model prediction separately, and the image to the right
shows the quantized model prediction. The mAP score of the prediction is
shown with each prediction. Predicted boxes with a confidence value of
at least 0.5 will be shown.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def draw_boxes_on_image(
    box: Sequence[float], image: np.ndarray, color: str, scale: bool = True
):
    &quot;&quot;&quot;
    Draw a `box` on an `image` with a `color`, optionally scaling the box from normalized
    coordinates (between 0 and 1) to image coordinates.
    This is a utility function for binary detection where all boxes belong to one category.

    :param box: Box coordinates as [xmin, ymin, xmax, ymax]
    :param image: numpy array of RGB image
    :param color: Box color, &quot;red&quot;, &quot;green&quot; or &quot;blue&quot;
    &quot;param scale: If True, scale normalized box coordinates to absolute coordinates based
                  on the image size.
    &quot;&quot;&quot;
    colors = {&quot;red&quot;: (255, 0, 64), &quot;green&quot;: (0, 255, 0), &quot;yellow&quot;: (255, 255, 128)}
    assert color in colors, f&quot;{color} is not defined yet. Defined colors are: {colors}&quot;
    image_height, image_width, _ = image.shape
    x_min, y_min, x_max, y_max = box
    if scale:
        x_min *= image_width
        x_max *= image_width
        y_min *= image_height
        y_max *= image_height

    image = cv2.rectangle(
        img=image,
        pt1=(round(x_min), round(y_min)),
        pt2=(round(x_max), round(y_max)),
        color=colors[color],
        thickness=2,
    )
    return image
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Change `map_value` to one of the values in the list below to show a different metric.
# See https://torchmetrics.readthedocs.io/en/latest/references/modules.html#map
# (&#39;map&#39;, &#39;map_50&#39;, &#39;map_75&#39;, &#39;map_small&#39;, &#39;map_medium&#39;, &#39;map_large&#39;
#  &#39;mar_1&#39;, &#39;mar_10&#39;, &#39;mar_100&#39;, &#39;mar_small&#39;, &#39;mar_medium&#39;, &#39;mar_large&#39;)

map_value = &quot;map&quot;
confidence_threshold = 0.5
num_images = 4

# FP prediction
fp_model = ie.read_model(model=ir_path)
fp_compiled_model = ie.compile_model(model=fp_model, device_name=&quot;CPU&quot;)
input_layer_fp = fp_compiled_model.input(0)
output_layer_fp = fp_compiled_model.output(0)

# INT8 prediction
int8_model = ie.read_model(model=compressed_model_path)
int8_compiled_model = ie.compile_model(model=int8_model, device_name=&quot;CPU&quot;)
input_layer_int8 = int8_compiled_model.input(0)
output_layer_int8 = int8_compiled_model.output(0)

fig, axs = plt.subplots(nrows=num_images, ncols=3, figsize=(16, 14), squeeze=False)
for i in range(num_images):
    annotation, input_image, metadata = data_loader[i]
    image = cv2.cvtColor(
        src=cv2.imread(filename=metadata[&quot;filename&quot;]), code=cv2.COLOR_BGR2RGB
    )
    orig_image = image.copy()
    resized_image = cv2.resize(image, (input_width, input_height))
    target_annotation = annotation[1]

    fp_res = fp_compiled_model([input_image])[output_layer_fp]

    fp_metric = MAPMetric(map_value=map_value)
    fp_metric.update(output=[fp_res], target=[target_annotation])

    for item in fp_res[0, 0, ::]:
        _, _, conf, xmin, xmax, ymin, ymax = item
        if conf &gt; confidence_threshold:
            total_image = draw_boxes_on_image([xmin, xmax, ymin, ymax], image, &quot;red&quot;)

    axs[i, 1].imshow(total_image)

    int8_res = int8_compiled_model([input_image])[output_layer_int8]
    int8_metric = MAPMetric(map_value=map_value)
    int8_metric.update(output=[int8_res], target=[target_annotation])

    for item in int8_res[0, 0, ::]:
        _, _, conf, xmin, xmax, ymin, ymax = item
        if conf &gt; confidence_threshold:
            total_image = draw_boxes_on_image(
                [xmin, xmax, ymin, ymax], total_image, &quot;yellow&quot;
            )
            int8_image = draw_boxes_on_image(
                [xmin, xmax, ymin, ymax], orig_image, &quot;yellow&quot;
            )

    axs[i, 2].imshow(int8_image)

    # Annotation
    for annotation in target_annotation:
        total_image = draw_boxes_on_image(annotation[&quot;bbox&quot;], total_image, &quot;green&quot;)

    axs[i, 0].imshow(image)
    axs[i, 0].set_title(Path(metadata[&quot;filename&quot;]).stem)
    axs[i, 1].set_title(f&quot;FP32 mAP: {fp_metric.avg_value[map_value]:.3f}&quot;)
    axs[i, 2].set_title(f&quot;INT8 mAP: {int8_metric.avg_value[map_value]:.3f}&quot;)
    fig.suptitle(
        &quot;Annotated (green) and detected boxes on FP (red) and INT8 (yellow) model&quot;
    )
</pre></div>
</div>
<img alt="../_images/111-detection-quantization-with-output_19_0.png" src="../_images/111-detection-quantization-with-output_19_0.png" />
</section>
<section id="compare-the-size-of-the-original-and-quantized-models">
<h2>Compare the Size of the Original and Quantized Models<a class="headerlink" href="#compare-the-size-of-the-original-and-quantized-models" title="Permalink to this headline">¶</a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>original_model_size = Path(ir_path).with_suffix(&quot;.bin&quot;).stat().st_size / 1024
quantized_model_size = (
    Path(compressed_model_path).with_suffix(&quot;.bin&quot;).stat().st_size / 1024
)

print(f&quot;FP32 model size: {original_model_size:.2f} KB&quot;)
print(f&quot;INT8 model size: {quantized_model_size:.2f} KB&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">FP32</span> <span class="n">model</span> <span class="n">size</span><span class="p">:</span> <span class="mf">2823.60</span> <span class="n">KB</span>
<span class="n">INT8</span> <span class="n">model</span> <span class="n">size</span><span class="p">:</span> <span class="mf">806.62</span> <span class="n">KB</span>
</pre></div>
</div>
</section>
<section id="compare-performance-of-the-original-and-quantized-models">
<h2>Compare Performance of the Original and Quantized Models<a class="headerlink" href="#compare-performance-of-the-original-and-quantized-models" title="Permalink to this headline">¶</a></h2>
<p>To measure inference performance of the <code class="docutils literal notranslate"><span class="pre">FP16</span></code> and <code class="docutils literal notranslate"><span class="pre">INT8</span></code> models,
use the <a class="reference external" href="https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html">Benchmark
Tool</a>
in OpenVINO. It can be run in the notebook with the <code class="docutils literal notranslate"><span class="pre">!</span> <span class="pre">benchmark_app</span></code>
or <code class="docutils literal notranslate"><span class="pre">%sx</span> <span class="pre">benchmark_app</span></code> commands.</p>
<p>This tutorial uses a wrapper function from <a class="reference external" href="https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/utils/notebook_utils.ipynb">Notebook
Utils</a>.
It prints the <code class="docutils literal notranslate"><span class="pre">benchmark_app</span></code> command with the chosen parameters.</p>
<blockquote>
<div><p><strong>Note</strong>: For the most accurate performance estimation, it is
recommended to run <code class="docutils literal notranslate"><span class="pre">benchmark_app</span></code> in a terminal/command prompt
after closing other applications. Run <code class="docutils literal notranslate"><span class="pre">benchmark_app</span> <span class="pre">--help</span></code> to see
all command-line options.</p>
</div></blockquote>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># ! benchmark_app --help
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># benchmark_model??
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Benchmark FP16 model
benchmark_model(model_path=ir_path, device=&quot;CPU&quot;, seconds=15, api=&quot;async&quot;)
</pre></div>
</div>
<p><strong>Benchmark person-detection-retail-0013.xml with CPU for 15 seconds
with async inference</strong></p>
<p>Benchmark command:
<code class="docutils literal notranslate"><span class="pre">benchmark_app</span> <span class="pre">-m</span> <span class="pre">intel/person-detection-retail-0013/FP32/person-detection-retail-0013.xml</span> <span class="pre">-d</span> <span class="pre">CPU</span> <span class="pre">-t</span> <span class="pre">15</span> <span class="pre">-api</span> <span class="pre">async</span> <span class="pre">-b</span> <span class="pre">1</span> <span class="pre">-cdir</span> <span class="pre">model_cache</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Count</span><span class="p">:</span>          <span class="mi">5802</span> <span class="n">iterations</span>
<span class="n">Duration</span><span class="p">:</span>       <span class="mf">15012.29</span> <span class="n">ms</span>
<span class="n">Latency</span><span class="p">:</span>
    <span class="n">Median</span><span class="p">:</span>     <span class="mf">15.38</span> <span class="n">ms</span>
    <span class="n">AVG</span><span class="p">:</span>        <span class="mf">15.42</span> <span class="n">ms</span>
    <span class="n">MIN</span><span class="p">:</span>        <span class="mf">10.19</span> <span class="n">ms</span>
    <span class="n">MAX</span><span class="p">:</span>        <span class="mf">31.79</span> <span class="n">ms</span>
<span class="n">Throughput</span><span class="p">:</span> <span class="mf">386.48</span> <span class="n">FPS</span>

<span class="n">Device</span><span class="p">:</span> <span class="n">Intel</span><span class="p">(</span><span class="n">R</span><span class="p">)</span> <span class="n">Core</span><span class="p">(</span><span class="n">TM</span><span class="p">)</span> <span class="n">i9</span><span class="o">-</span><span class="mi">10920</span><span class="n">X</span> <span class="n">CPU</span> <span class="o">@</span> <span class="mf">3.50</span><span class="n">GHz</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Benchmark INT8 model
benchmark_model(model_path=compressed_model_path, device=&quot;CPU&quot;, seconds=15, api=&quot;async&quot;)
</pre></div>
</div>
<p><strong>Benchmark person-detection-retail-0013_mixed_DefaultQuantization.xml
with CPU for 15 seconds with async inference</strong></p>
<p>Benchmark command:
<code class="docutils literal notranslate"><span class="pre">benchmark_app</span> <span class="pre">-m</span> <span class="pre">optimized_model/person-detection-retail-0013_mixed_DefaultQuantization.xml</span> <span class="pre">-d</span> <span class="pre">CPU</span> <span class="pre">-t</span> <span class="pre">15</span> <span class="pre">-api</span> <span class="pre">async</span> <span class="pre">-b</span> <span class="pre">1</span> <span class="pre">-cdir</span> <span class="pre">model_cache</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Count</span><span class="p">:</span>          <span class="mi">13536</span> <span class="n">iterations</span>
<span class="n">Duration</span><span class="p">:</span>       <span class="mf">15017.31</span> <span class="n">ms</span>
<span class="n">Latency</span><span class="p">:</span>
    <span class="n">Median</span><span class="p">:</span>     <span class="mf">13.14</span> <span class="n">ms</span>
    <span class="n">AVG</span><span class="p">:</span>        <span class="mf">13.27</span> <span class="n">ms</span>
    <span class="n">MIN</span><span class="p">:</span>        <span class="mf">10.13</span> <span class="n">ms</span>
    <span class="n">MAX</span><span class="p">:</span>        <span class="mf">29.42</span> <span class="n">ms</span>
<span class="n">Throughput</span><span class="p">:</span> <span class="mf">901.36</span> <span class="n">FPS</span>

<span class="n">Device</span><span class="p">:</span> <span class="n">Intel</span><span class="p">(</span><span class="n">R</span><span class="p">)</span> <span class="n">Core</span><span class="p">(</span><span class="n">TM</span><span class="p">)</span> <span class="n">i9</span><span class="o">-</span><span class="mi">10920</span><span class="n">X</span> <span class="n">CPU</span> <span class="o">@</span> <span class="mf">3.50</span><span class="n">GHz</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Benchmark INT8 model on MULTI:CPU,GPU device (requires an Intel integrated GPU)
ie = Core()
if &quot;GPU&quot; in ie.available_devices:
    benchmark_model(
        model_path=compressed_model_path,
        device=&quot;MULTI:CPU,GPU&quot;,
        seconds=15,
        api=&quot;async&quot;,
        batch=4,
    )
</pre></div>
</div>
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  
    <a class='button bttn-sec button-size-l' id="prev-link" href="110-ct-segmentation-quantize-with-output.html" title="previous page">Prev</a>
    <a class='button bttn-sec button-size-l' id="next-link" href="112-pytorch-post-training-quantization-nncf-with-output.html" title="next page">Next</a>

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel®.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>