
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>INT8 Quantization with Post-training Optimization Tool (POT) in Simplified Mode tutorial &#8212; OpenVINO™  documentation</title>
    
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="../_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="../_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/tabs.css" type="text/css" />
    <script src="../_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="../_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="../_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/tabs.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/custom.js"></script>
    <script src="../_static/js/graphs.js"></script>
    <script src="../_static/js/graphs_ov_tf.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/target-highlight.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://docs.openvino.ai/latest/notebooks/114-quantization-simplified-mode-with-output.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Asynchronous Inference with OpenVINO™" href="115-async-api-with-output.html" />
    <link rel="prev" title="Quantization of Image Classification Models" href="113-image-classification-quantization-with-output.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/get-started-guide.html">
  Get Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/documentation.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api/api_reference.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../model_zoo.html">
  Model Zoo
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/resources.html">
  Resources
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/openvinotoolkit/openvino" rel="noopener" target="_blank" title="GitHub">
            <span><i class="sst-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="language-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">English</button>
  <div class="dropdown-menu" aria-labelledby="language-selector">
    
      
        <a class="dropdown-item font-weight-bold" href="/openvino-docs/index.html">English</a>
      
    
      
        <a  class="dropdown-item" href="/cn/openvino-docs/index.html">Chinese</a>
      
    
  </div>
</div>

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="../search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notebooks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks-installation.html">
   Installation of OpenVINO™ Notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="001-hello-world-with-output.html">
   Hello Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002-openvino-api-with-output.html">
   OpenVINO™ Runtime API Tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003-hello-segmentation-with-output.html">
   Hello Image Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="004-hello-detection-with-output.html">
   Hello Object Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="101-tensorflow-to-openvino-with-output.html">
   Convert a TensorFlow Model to OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="102-pytorch-onnx-to-openvino-with-output.html">
   Convert a PyTorch Model to ONNX and OpenVINO™ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="103-paddle-onnx-to-openvino-classification-with-output.html">
   Convert a PaddlePaddle Model to ONNX and OpenVINO™ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="104-model-tools-with-output.html">
   Working with Open Model Zoo Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="105-language-quantize-bert-with-output.html">
   Quantize NLP models with Post-Training Optimization Tool ​in OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="106-auto-device-with-output.html">
   Automatic Device Selection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="107-speech-recognition-quantization-with-output.html">
   Quantize Speech Recognition Models with OpenVINO™ Post-Training Optimization Tool ​
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-nncf-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="111-detection-quantization-with-output.html">
   Object Detection Quantization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="112-pytorch-post-training-quantization-nncf-with-output.html">
   Post-Training Quantization of PyTorch models with NNCF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="113-image-classification-quantization-with-output.html">
   Quantization of Image Classification Models
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   INT8 Quantization with Post-training Optimization Tool (POT) in Simplified Mode tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="115-async-api-with-output.html">
   Asynchronous Inference with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="201-vision-monodepth-with-output.html">
   Monodepth Estimation with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-image-with-output.html">
   Single Image Super Resolution with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-video-with-output.html">
   Video Super Resolution with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="203-meter-reader-with-output.html">
   Industrial Meter Reader
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="204-named-entity-recognition-with-output.html">
   Document Entity Extraction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="205-vision-background-removal-with-output.html">
   Image Background Removal with U^2-Net and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="206-vision-paddlegan-anime-with-output.html">
   Photos to Anime with PaddleGAN and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="207-vision-paddlegan-superresolution-with-output.html">
   Super Resolution with PaddleGAN and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="208-optical-character-recognition-with-output.html">
   Optical Character Recognition (OCR) with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="209-handwritten-ocr-with-output.html">
   Handwritten Chinese and Japanese OCR with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="210-ct-scan-live-inference-with-output.html">
   Live Inference and Benchmark CT-scan Data with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="211-speech-to-text-with-output.html">
   Speech to Text with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="212-onnx-style-transfer-with-output.html">
   Style Transfer on ONNX Models with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="214-vision-paddle-classification-with-output.html">
   PaddlePaddle Image Classification with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="215-image-inpainting-with-output.html">
   Image In-painting with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="216-license-plate-recognition-with-output.html">
   License Plate Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="217-vision-deblur-with-output.html">
   Deblur Photos with DeblurGAN-v2 and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="218-vehicle-detection-and-recognition-with-output.html">
   Vehicle Detection And Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="219-knowledge-graphs-conve-with-output.html">
   OpenVINO optimizations for Knowledge graphs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="220-yolov5-accuracy-check-and-quantization-with-output.html">
   Quantize the Ultralytics YOLOv5 model and check accuracy using the OpenVINO POT API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="221-machine-translation-with-output.html">
   Machine translation demo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="222-vision-image-colorization-with-output.html">
   Image Colorization with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="223-gpt2-text-prediction-with-output.html">
   GPT-2 Text Prediction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-pot-with-output.html">
   Post-Training Quantization with TensorFlow Classification Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-with-output.html">
   From Training to Deployment with TensorFlow and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="302-pytorch-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using PyTorch framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="305-tensorflow-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using TensorFlow Framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="401-object-detection-with-output.html">
   Live Object Detection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="402-pose-estimation-with-output.html">
   Live Human Pose Estimation with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="403-action-recognition-webcam-with-output.html">
   Human Action Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="405-paddle-ocr-webcam-with-output.html">
   PaddleOCR with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebook_utils-with-output.html">
   Notebook Utils
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prepare-the-calibration-dataset">
   Prepare the calibration dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prepare-the-model">
   Prepare the Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#compression-stage">
   Compression stage
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#compare-performance-of-the-original-and-quantized-models">
   Compare Performance of the Original and Quantized Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#demonstration-of-the-results">
   Demonstration of the results
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">

<div class="tocsection editthispage">
    <a href="None">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

            
                <div>
                  
  <section id="int8-quantization-with-post-training-optimization-tool-pot-in-simplified-mode-tutorial">
<h1>INT8 Quantization with Post-training Optimization Tool (POT) in Simplified Mode tutorial<a class="headerlink" href="#int8-quantization-with-post-training-optimization-tool-pot-in-simplified-mode-tutorial" title="Permalink to this headline">¶</a></h1>
<p>This tutorial shows how to quantize a
<a class="reference external" href="https://github.com/chenyaofo/pytorch-cifar-models">ResNet20</a> image
classification model, trained on
<a class="reference external" href="http://pytorch.org/vision/main/generated/torchvision.datasets.CIFAR10.html">CIFAR10</a>
dataset, using the Post-Training Optimization Tool (POT) in Simplified
Mode.</p>
<p>Simplified Mode is designed to make the data preparation step easier,
before model optimization. The mode is represented by an implementation
of the engine interface in the POT API in OpenVINO™. It enables reading
data from an arbitrary folder specified by the user. Currently,
Simplified Mode is available only for image data in PNG or JPEG formats,
stored in a single folder.</p>
<blockquote>
<div><p><strong>Note:</strong> This mode cannot be used with the accuracy-aware method. It
is not possible to control accuracy after optimization using this
mode. However, Simplified Mode can be useful for estimating
performance improvements when optimizing models.</p>
</div></blockquote>
<p>This tutorial includes the following steps:</p>
<ul class="simple">
<li><p>Downloading and saving the CIFAR10 dataset.</p></li>
<li><p>Preparing the model for quantization.</p></li>
<li><p>Compressing the prepared model.</p></li>
<li><p>Measuring and comparing the performance of the original and quantized
models.</p></li>
<li><p>Demonstrating the use of the quantized model for image
classification.</p></li>
</ul>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import os
from pathlib import Path
import warnings

import torch
from torchvision import transforms as T
from torchvision.datasets import CIFAR10

import matplotlib.pyplot as plt
import numpy as np

from openvino.runtime import Core, Tensor

warnings.filterwarnings(&quot;ignore&quot;)

# Set the data and model directories
MODEL_DIR = &#39;model&#39;
CALIB_DIR = &#39;calib&#39;
CIFAR_DIR = &#39;cifar&#39;
CALIB_SET_SIZE = 300
MODEL_NAME = &#39;resnet20&#39;

os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(CALIB_DIR, exist_ok=True)
</pre></div>
</div>
<section id="prepare-the-calibration-dataset">
<h2>Prepare the calibration dataset<a class="headerlink" href="#prepare-the-calibration-dataset" title="Permalink to this headline">¶</a></h2>
<p>The following steps are required to prepare the calibration dataset: -
Download the CIFAR10 dataset from <a class="reference external" href="https://pytorch.org/vision/stable/datasets.html">Torchvision.datasets
repository</a>. - Save
the selected number of elements from this dataset as <code class="docutils literal notranslate"><span class="pre">.png</span></code> images in
a separate folder.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>transform = T.Compose([T.ToTensor()])
dataset = CIFAR10(root=CIFAR_DIR, train=False, transform=transform, download=True)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Downloading</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">cs</span><span class="o">.</span><span class="n">toronto</span><span class="o">.</span><span class="n">edu</span><span class="o">/~</span><span class="n">kriz</span><span class="o">/</span><span class="n">cifar</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="n">python</span><span class="o">.</span><span class="n">tar</span><span class="o">.</span><span class="n">gz</span> <span class="n">to</span> <span class="n">cifar</span><span class="o">/</span><span class="n">cifar</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="n">python</span><span class="o">.</span><span class="n">tar</span><span class="o">.</span><span class="n">gz</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>0%|          | 0/170498071 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Extracting</span> <span class="n">cifar</span><span class="o">/</span><span class="n">cifar</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="n">python</span><span class="o">.</span><span class="n">tar</span><span class="o">.</span><span class="n">gz</span> <span class="n">to</span> <span class="n">cifar</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>pil_converter = T.ToPILImage(mode=&quot;RGB&quot;)

for idx, info in enumerate(dataset):
    im = info[0]
    if idx &gt;= CALIB_SET_SIZE:
        break
    label = info[1]
    pil_converter(im.squeeze(0)).save(Path(CALIB_DIR) / f&#39;{label}_{idx}.png&#39;)
</pre></div>
</div>
</section>
<section id="prepare-the-model">
<h2>Prepare the Model<a class="headerlink" href="#prepare-the-model" title="Permalink to this headline">¶</a></h2>
<p>Model preparation includes the following steps: - Download PyTorch model
from Torchvision repository. - Convert the model to ONNX format. - Run
Model Optimizer to convert ONNX to OpenVINO Intermediate Representation
(OpenVINO IR).</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model = torch.hub.load(&quot;chenyaofo/pytorch-cifar-models&quot;, &quot;cifar10_resnet20&quot;, pretrained=True)
dummy_input = torch.randn(1, 3, 32, 32)

onnx_model_path = Path(MODEL_DIR) / &#39;{}.onnx&#39;.format(MODEL_NAME)
ir_model_xml = onnx_model_path.with_suffix(&#39;.xml&#39;)
ir_model_bin = onnx_model_path.with_suffix(&#39;.bin&#39;)

torch.onnx.export(model, dummy_input, onnx_model_path)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Using</span> <span class="n">cache</span> <span class="n">found</span> <span class="ow">in</span> <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/.</span><span class="n">cache</span><span class="o">/</span><span class="n">torch</span><span class="o">/</span><span class="n">hub</span><span class="o">/</span><span class="n">chenyaofo_pytorch</span><span class="o">-</span><span class="n">cifar</span><span class="o">-</span><span class="n">models_master</span>
</pre></div>
</div>
<p>Now, convert this model into the OpenVINO IR using Model Optimizer:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>!mo --framework=onnx --data_type=FP32 --input_shape=[1,3,32,32] -m $onnx_model_path  --output_dir $MODEL_DIR
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Model</span> <span class="n">Optimizer</span> <span class="n">arguments</span><span class="p">:</span>
<span class="n">Common</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="o">-</span> <span class="n">Path</span> <span class="n">to</span> <span class="n">the</span> <span class="n">Input</span> <span class="n">Model</span><span class="p">:</span>  <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">114</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">simplified</span><span class="o">-</span><span class="n">mode</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">resnet20</span><span class="o">.</span><span class="n">onnx</span>
    <span class="o">-</span> <span class="n">Path</span> <span class="k">for</span> <span class="n">generated</span> <span class="n">IR</span><span class="p">:</span>    <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">114</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">simplified</span><span class="o">-</span><span class="n">mode</span><span class="o">/</span><span class="n">model</span>
    <span class="o">-</span> <span class="n">IR</span> <span class="n">output</span> <span class="n">name</span><span class="p">:</span>   <span class="n">resnet20</span>
    <span class="o">-</span> <span class="n">Log</span> <span class="n">level</span><span class="p">:</span>    <span class="n">ERROR</span>
    <span class="o">-</span> <span class="n">Batch</span><span class="p">:</span>    <span class="n">Not</span> <span class="n">specified</span><span class="p">,</span> <span class="n">inherited</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">model</span>
    <span class="o">-</span> <span class="n">Input</span> <span class="n">layers</span><span class="p">:</span>     <span class="n">Not</span> <span class="n">specified</span><span class="p">,</span> <span class="n">inherited</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">model</span>
    <span class="o">-</span> <span class="n">Output</span> <span class="n">layers</span><span class="p">:</span>    <span class="n">Not</span> <span class="n">specified</span><span class="p">,</span> <span class="n">inherited</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">model</span>
    <span class="o">-</span> <span class="n">Input</span> <span class="n">shapes</span><span class="p">:</span>     <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">]</span>
    <span class="o">-</span> <span class="n">Source</span> <span class="n">layout</span><span class="p">:</span>    <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Target</span> <span class="n">layout</span><span class="p">:</span>    <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Layout</span><span class="p">:</span>   <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Mean</span> <span class="n">values</span><span class="p">:</span>  <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Scale</span> <span class="n">values</span><span class="p">:</span>     <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Scale</span> <span class="n">factor</span><span class="p">:</span>     <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Precision</span> <span class="n">of</span> <span class="n">IR</span><span class="p">:</span>  <span class="n">FP32</span>
    <span class="o">-</span> <span class="n">Enable</span> <span class="n">fusing</span><span class="p">:</span>    <span class="kc">True</span>
    <span class="o">-</span> <span class="n">User</span> <span class="n">transformations</span><span class="p">:</span>     <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Reverse</span> <span class="nb">input</span> <span class="n">channels</span><span class="p">:</span>   <span class="kc">False</span>
    <span class="o">-</span> <span class="n">Enable</span> <span class="n">IR</span> <span class="n">generation</span> <span class="k">for</span> <span class="n">fixed</span> <span class="nb">input</span> <span class="n">shape</span><span class="p">:</span>   <span class="kc">False</span>
    <span class="o">-</span> <span class="n">Use</span> <span class="n">the</span> <span class="n">transformations</span> <span class="n">config</span> <span class="n">file</span><span class="p">:</span>  <span class="kc">None</span>
<span class="n">Advanced</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="o">-</span> <span class="n">Force</span> <span class="n">the</span> <span class="n">usage</span> <span class="n">of</span> <span class="n">legacy</span> <span class="n">Frontend</span> <span class="n">of</span> <span class="n">Model</span> <span class="n">Optimizer</span> <span class="k">for</span> <span class="n">model</span> <span class="n">conversion</span> <span class="n">into</span> <span class="n">IR</span><span class="p">:</span>   <span class="kc">False</span>
    <span class="o">-</span> <span class="n">Force</span> <span class="n">the</span> <span class="n">usage</span> <span class="n">of</span> <span class="n">new</span> <span class="n">Frontend</span> <span class="n">of</span> <span class="n">Model</span> <span class="n">Optimizer</span> <span class="k">for</span> <span class="n">model</span> <span class="n">conversion</span> <span class="n">into</span> <span class="n">IR</span><span class="p">:</span>  <span class="kc">False</span>
<span class="n">OpenVINO</span> <span class="n">runtime</span> <span class="n">found</span> <span class="ow">in</span><span class="p">:</span>  <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/.</span><span class="n">venv</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">openvino</span>
<span class="n">OpenVINO</span> <span class="n">runtime</span> <span class="n">version</span><span class="p">:</span>   <span class="mf">2022.1.0</span><span class="o">-</span><span class="mi">7019</span><span class="o">-</span><span class="n">cdb9bec7210</span><span class="o">-</span><span class="n">releases</span><span class="o">/</span><span class="mi">2022</span><span class="o">/</span><span class="mi">1</span>
<span class="n">Model</span> <span class="n">Optimizer</span> <span class="n">version</span><span class="p">:</span>    <span class="mf">2022.1.0</span><span class="o">-</span><span class="mi">7019</span><span class="o">-</span><span class="n">cdb9bec7210</span><span class="o">-</span><span class="n">releases</span><span class="o">/</span><span class="mi">2022</span><span class="o">/</span><span class="mi">1</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">Generated</span> <span class="n">IR</span> <span class="n">version</span> <span class="mi">11</span> <span class="n">model</span><span class="o">.</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">XML</span> <span class="n">file</span><span class="p">:</span> <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">114</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">simplified</span><span class="o">-</span><span class="n">mode</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">resnet20</span><span class="o">.</span><span class="n">xml</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">BIN</span> <span class="n">file</span><span class="p">:</span> <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">114</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">simplified</span><span class="o">-</span><span class="n">mode</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">resnet20</span><span class="o">.</span><span class="n">bin</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">Total</span> <span class="n">execution</span> <span class="n">time</span><span class="p">:</span> <span class="mf">0.38</span> <span class="n">seconds</span><span class="o">.</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">Memory</span> <span class="n">consumed</span><span class="p">:</span> <span class="mi">73</span> <span class="n">MB</span><span class="o">.</span>
<span class="n">It</span><span class="s1">&#39;s been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&amp;source=prod&amp;campid=ww_2022_bu_IOTG_OpenVINO-2022-1&amp;content=upg_all&amp;medium=organic or on the GitHub*</span>
<span class="p">[</span> <span class="n">INFO</span> <span class="p">]</span> <span class="n">The</span> <span class="n">model</span> <span class="n">was</span> <span class="n">converted</span> <span class="n">to</span> <span class="n">IR</span> <span class="n">v11</span><span class="p">,</span> <span class="n">the</span> <span class="n">latest</span> <span class="n">model</span> <span class="nb">format</span> <span class="n">that</span> <span class="n">corresponds</span> <span class="n">to</span> <span class="n">the</span> <span class="n">source</span> <span class="n">DL</span> <span class="n">framework</span> <span class="nb">input</span><span class="o">/</span><span class="n">output</span> <span class="nb">format</span><span class="o">.</span> <span class="n">While</span> <span class="n">IR</span> <span class="n">v11</span> <span class="ow">is</span> <span class="n">backwards</span> <span class="n">compatible</span> <span class="k">with</span> <span class="n">OpenVINO</span> <span class="n">Inference</span> <span class="n">Engine</span> <span class="n">API</span> <span class="n">v1</span><span class="mf">.0</span><span class="p">,</span> <span class="n">please</span> <span class="n">use</span> <span class="n">API</span> <span class="n">v2</span><span class="mf">.0</span> <span class="p">(</span><span class="k">as</span> <span class="n">of</span> <span class="mf">2022.1</span><span class="p">)</span> <span class="n">to</span> <span class="n">take</span> <span class="n">advantage</span> <span class="n">of</span> <span class="n">the</span> <span class="n">latest</span> <span class="n">improvements</span> <span class="ow">in</span> <span class="n">IR</span> <span class="n">v11</span><span class="o">.</span>
<span class="n">Find</span> <span class="n">more</span> <span class="n">information</span> <span class="n">about</span> <span class="n">API</span> <span class="n">v2</span><span class="mf">.0</span> <span class="ow">and</span> <span class="n">IR</span> <span class="n">v11</span> <span class="n">at</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">docs</span><span class="o">.</span><span class="n">openvino</span><span class="o">.</span><span class="n">ai</span>
</pre></div>
</div>
</section>
<section id="compression-stage">
<h2>Compression stage<a class="headerlink" href="#compression-stage" title="Permalink to this headline">¶</a></h2>
<p>Compress the model with the following command:</p>
<p><code class="docutils literal notranslate"><span class="pre">pot</span> <span class="pre">-q</span> <span class="pre">default</span> <span class="pre">-m</span> <span class="pre">&lt;path_to_xml&gt;</span> <span class="pre">-w</span> <span class="pre">&lt;path_to_bin&gt;</span> <span class="pre">--engine</span> <span class="pre">simplified</span> <span class="pre">--data-source</span> <span class="pre">&lt;path_to_data&gt;</span></code></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>!pot -q default -m $ir_model_xml -w $ir_model_bin --engine simplified --data-source $CALIB_DIR --output-dir compressed --direct-dump --name $MODEL_NAME
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">INFO</span><span class="p">:</span><span class="n">openvino</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">pot</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">run</span><span class="p">:</span><span class="n">Output</span> <span class="n">log</span> <span class="nb">dir</span><span class="p">:</span> <span class="n">compressed</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">openvino</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">pot</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">run</span><span class="p">:</span><span class="n">Creating</span> <span class="n">pipeline</span><span class="p">:</span>
 <span class="n">Algorithm</span><span class="p">:</span> <span class="n">DefaultQuantization</span>
 <span class="n">Parameters</span><span class="p">:</span>
    <span class="n">preset</span>                     <span class="p">:</span> <span class="n">performance</span>
    <span class="n">stat_subset_size</span>           <span class="p">:</span> <span class="mi">300</span>
    <span class="n">target_device</span>              <span class="p">:</span> <span class="n">ANY</span>
    <span class="n">model_type</span>                 <span class="p">:</span> <span class="kc">None</span>
    <span class="n">dump_intermediate_model</span>    <span class="p">:</span> <span class="kc">False</span>
    <span class="n">inplace_statistics</span>         <span class="p">:</span> <span class="kc">True</span>
    <span class="n">exec_log_dir</span>               <span class="p">:</span> <span class="n">compressed</span>
 <span class="o">===========================================================================</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">openvino</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">pot</span><span class="o">.</span><span class="n">data_loaders</span><span class="o">.</span><span class="n">image_loader</span><span class="p">:</span><span class="n">Layout</span> <span class="n">value</span> <span class="ow">is</span> <span class="nb">set</span> <span class="p">[</span><span class="n">N</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">W</span><span class="p">]</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">openvino</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">pot</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">pipeline</span><span class="p">:</span><span class="n">Inference</span> <span class="n">Engine</span> <span class="n">version</span><span class="p">:</span>                <span class="mf">2022.1.0</span><span class="o">-</span><span class="mi">7019</span><span class="o">-</span><span class="n">cdb9bec7210</span><span class="o">-</span><span class="n">releases</span><span class="o">/</span><span class="mi">2022</span><span class="o">/</span><span class="mi">1</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">openvino</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">pot</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">pipeline</span><span class="p">:</span><span class="n">Model</span> <span class="n">Optimizer</span> <span class="n">version</span><span class="p">:</span>                 <span class="mf">2022.1.0</span><span class="o">-</span><span class="mi">7019</span><span class="o">-</span><span class="n">cdb9bec7210</span><span class="o">-</span><span class="n">releases</span><span class="o">/</span><span class="mi">2022</span><span class="o">/</span><span class="mi">1</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">openvino</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">pot</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">pipeline</span><span class="p">:</span><span class="n">Post</span><span class="o">-</span><span class="n">Training</span> <span class="n">Optimization</span> <span class="n">Tool</span> <span class="n">version</span><span class="p">:</span> <span class="mf">2022.1.0</span><span class="o">-</span><span class="mi">7019</span><span class="o">-</span><span class="n">cdb9bec7210</span><span class="o">-</span><span class="n">releases</span><span class="o">/</span><span class="mi">2022</span><span class="o">/</span><span class="mi">1</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">openvino</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">pot</span><span class="o">.</span><span class="n">statistics</span><span class="o">.</span><span class="n">collector</span><span class="p">:</span><span class="n">Start</span> <span class="n">computing</span> <span class="n">statistics</span> <span class="k">for</span> <span class="n">algorithms</span> <span class="p">:</span> <span class="n">DefaultQuantization</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">openvino</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">pot</span><span class="o">.</span><span class="n">statistics</span><span class="o">.</span><span class="n">collector</span><span class="p">:</span><span class="n">Computing</span> <span class="n">statistics</span> <span class="n">finished</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">openvino</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">pot</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">pipeline</span><span class="p">:</span><span class="n">Start</span> <span class="n">algorithm</span><span class="p">:</span> <span class="n">DefaultQuantization</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">openvino</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">pot</span><span class="o">.</span><span class="n">algorithms</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">default</span><span class="o">.</span><span class="n">algorithm</span><span class="p">:</span><span class="n">Start</span> <span class="n">computing</span> <span class="n">statistics</span> <span class="k">for</span> <span class="n">algorithm</span> <span class="p">:</span> <span class="n">ActivationChannelAlignment</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">openvino</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">pot</span><span class="o">.</span><span class="n">algorithms</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">default</span><span class="o">.</span><span class="n">algorithm</span><span class="p">:</span><span class="n">Computing</span> <span class="n">statistics</span> <span class="n">finished</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">openvino</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">pot</span><span class="o">.</span><span class="n">algorithms</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">default</span><span class="o">.</span><span class="n">algorithm</span><span class="p">:</span><span class="n">Start</span> <span class="n">computing</span> <span class="n">statistics</span> <span class="k">for</span> <span class="n">algorithms</span> <span class="p">:</span> <span class="n">MinMaxQuantization</span><span class="p">,</span><span class="n">FastBiasCorrection</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">openvino</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">pot</span><span class="o">.</span><span class="n">algorithms</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">default</span><span class="o">.</span><span class="n">algorithm</span><span class="p">:</span><span class="n">Computing</span> <span class="n">statistics</span> <span class="n">finished</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">openvino</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">pot</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">pipeline</span><span class="p">:</span><span class="n">Finished</span><span class="p">:</span> <span class="n">DefaultQuantization</span>
 <span class="o">===========================================================================</span>
</pre></div>
</div>
</section>
<section id="compare-performance-of-the-original-and-quantized-models">
<h2>Compare Performance of the Original and Quantized Models<a class="headerlink" href="#compare-performance-of-the-original-and-quantized-models" title="Permalink to this headline">¶</a></h2>
<p>Finally, measure the inference performance of the <code class="docutils literal notranslate"><span class="pre">FP32</span></code> and <code class="docutils literal notranslate"><span class="pre">INT8</span></code>
models, using <a class="reference external" href="https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html">Benchmark
Tool</a>
- an inference performance measurement tool in OpenVINO.</p>
<blockquote>
<div><p><strong>Note</strong>: For more accurate performance, it is recommended to run
benchmark_app in a terminal/command prompt after closing other
applications. Run <code class="docutils literal notranslate"><span class="pre">benchmark_app</span> <span class="pre">-m</span> <span class="pre">model.xml</span> <span class="pre">-d</span> <span class="pre">CPU</span></code> to benchmark
async inference on CPU for one minute. Change CPU to GPU to benchmark
on GPU. Run <code class="docutils literal notranslate"><span class="pre">benchmark_app</span> <span class="pre">--help</span></code> to see an overview of all
command-line options.</p>
</div></blockquote>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>optimized_model_path = Path(&#39;compressed/optimized&#39;)
optimized_model_xml = optimized_model_path / &#39;{}.xml&#39;.format(MODEL_NAME)
optimized_model_bin = optimized_model_path / &#39;{}.bin&#39;.format(MODEL_NAME)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Inference FP32 model (OpenVINO IR)
!benchmark_app -m $ir_model_xml -d CPU -api async
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[Step 1/11] Parsing and validating input arguments
[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README.
[Step 2/11] Loading OpenVINO
[ WARNING ] PerformanceMode was not explicitly specified in command line. Device CPU performance hint will be set to THROUGHPUT.
[ INFO ] OpenVINO:
         API version............. 2022.1.0-7019-cdb9bec7210-releases/2022/1
[ INFO ] Device info
         CPU
         openvino_intel_cpu_plugin version 2022.1
         Build................... 2022.1.0-7019-cdb9bec7210-releases/2022/1

[Step 3/11] Setting device configuration
[ WARNING ] -nstreams default value is determined automatically for CPU device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README.
[Step 4/11] Reading network files
[ INFO ] Read model took 5.77 ms
[Step 5/11] Resizing network to match image sizes and given batch
[ INFO ] Network batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model input &#39;input.1&#39; precision u8, dimensions ([N,C,H,W]): 1 3 32 32
[ INFO ] Model output &#39;208&#39; precision f32, dimensions ([...]): 1 10
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 45.16 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] DEVICE: CPU
[ INFO ]   AVAILABLE_DEVICES  , [&#39;&#39;]
[ INFO ]   RANGE_FOR_ASYNC_INFER_REQUESTS  , (1, 1, 1)
[ INFO ]   RANGE_FOR_STREAMS  , (1, 24)
[ INFO ]   FULL_DEVICE_NAME  , Intel(R) Core(TM) i9-10920X CPU @ 3.50GHz
[ INFO ]   OPTIMIZATION_CAPABILITIES  , [&#39;WINOGRAD&#39;, &#39;FP32&#39;, &#39;FP16&#39;, &#39;INT8&#39;, &#39;BIN&#39;, &#39;EXPORT_IMPORT&#39;]
[ INFO ]   CACHE_DIR  ,
[ INFO ]   NUM_STREAMS  , 6
[ INFO ]   INFERENCE_NUM_THREADS  , 0
[ INFO ]   PERF_COUNT  , False
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS  , 0
[Step 9/11] Creating infer requests and preparing input data
[ INFO ] Create 6 infer requests took 1.36 ms
[ WARNING ] No input files were given for input &#39;input.1&#39;!. This input will be filled with random values!
[ INFO ] Fill input &#39;input.1&#39; with random values
[Step 10/11] Measuring performance (Start inference asynchronously, 6 inference requests using 6 streams for CPU, inference only: True, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 3.33 ms
[Step 11/11] Dumping statistics report
Count:          885936 iterations
Duration:       60000.50 ms
Latency:
    Median:     0.38 ms
    AVG:        0.39 ms
    MIN:        0.26 ms
    MAX:        7.24 ms
Throughput: 14765.48 FPS
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Inference INT8 model (OpenVINO IR)
!benchmark_app -m $optimized_model_xml -d CPU -api async
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[Step 1/11] Parsing and validating input arguments
[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README.
[Step 2/11] Loading OpenVINO
[ WARNING ] PerformanceMode was not explicitly specified in command line. Device CPU performance hint will be set to THROUGHPUT.
[ INFO ] OpenVINO:
         API version............. 2022.1.0-7019-cdb9bec7210-releases/2022/1
[ INFO ] Device info
         CPU
         openvino_intel_cpu_plugin version 2022.1
         Build................... 2022.1.0-7019-cdb9bec7210-releases/2022/1

[Step 3/11] Setting device configuration
[ WARNING ] -nstreams default value is determined automatically for CPU device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README.
[Step 4/11] Reading network files
[ INFO ] Read model took 7.75 ms
[Step 5/11] Resizing network to match image sizes and given batch
[ INFO ] Network batch size: 1
[Step 6/11] Configuring input of the model
[ INFO ] Model input &#39;input.1&#39; precision u8, dimensions ([N,C,H,W]): 1 3 32 32
[ INFO ] Model output &#39;208&#39; precision f32, dimensions ([...]): 1 10
[Step 7/11] Loading the model to the device
[ INFO ] Compile model took 69.51 ms
[Step 8/11] Querying optimal runtime parameters
[ INFO ] DEVICE: CPU
[ INFO ]   AVAILABLE_DEVICES  , [&#39;&#39;]
[ INFO ]   RANGE_FOR_ASYNC_INFER_REQUESTS  , (1, 1, 1)
[ INFO ]   RANGE_FOR_STREAMS  , (1, 24)
[ INFO ]   FULL_DEVICE_NAME  , Intel(R) Core(TM) i9-10920X CPU @ 3.50GHz
[ INFO ]   OPTIMIZATION_CAPABILITIES  , [&#39;WINOGRAD&#39;, &#39;FP32&#39;, &#39;FP16&#39;, &#39;INT8&#39;, &#39;BIN&#39;, &#39;EXPORT_IMPORT&#39;]
[ INFO ]   CACHE_DIR  ,
[ INFO ]   NUM_STREAMS  , 6
[ INFO ]   INFERENCE_NUM_THREADS  , 0
[ INFO ]   PERF_COUNT  , False
[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS  , 0
[Step 9/11] Creating infer requests and preparing input data
[ INFO ] Create 6 infer requests took 1.75 ms
[ WARNING ] No input files were given for input &#39;input.1&#39;!. This input will be filled with random values!
[ INFO ] Fill input &#39;input.1&#39; with random values
[Step 10/11] Measuring performance (Start inference asynchronously, 6 inference requests using 6 streams for CPU, inference only: True, limits: 60000 ms duration)
[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).
[ INFO ] First inference took 0.66 ms
[Step 11/11] Dumping statistics report
Count:          1360668 iterations
Duration:       60000.23 ms
Latency:
    Median:     0.24 ms
    AVG:        0.25 ms
    MIN:        0.17 ms
    MAX:        8.31 ms
Throughput: 22677.71 FPS
</pre></div>
</div>
</section>
<section id="demonstration-of-the-results">
<h2>Demonstration of the results<a class="headerlink" href="#demonstration-of-the-results" title="Permalink to this headline">¶</a></h2>
<p>This section demonstrates how to use the compressed model by running the
optimized model on a subset of images from the CIFAR10 dataset and shows
predictions, using the model.</p>
<p>The first step is to load the model:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>ie = Core()

compiled_model = ie.compile_model(str(optimized_model_xml), &quot;AUTO&quot;)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Define all possible labels from the CIFAR10 dataset.
labels_names = [&quot;airplane&quot;, &quot;automobile&quot;, &quot;bird&quot;, &quot;cat&quot;, &quot;deer&quot;, &quot;dog&quot;, &quot;frog&quot;, &quot;horse&quot;, &quot;ship&quot;, &quot;truck&quot;]
all_images = []
all_labels = []

# Get all images and their labels.
for batch in dataset:
    all_images.append(torch.unsqueeze(batch[0], 0))
    all_labels.append(batch[1])
</pre></div>
</div>
<p>The code below defines the function that shows the images and their
labels, using the indexes and two lists created in the previous step:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def plot_pictures(indexes: list, images=all_images, labels=all_labels):
    &quot;&quot;&quot;Plot images with the specified indexes.
    :param indexes: a list of indexes of images to be displayed.
    :param images: a list of images from the dataset.
    :param labels: a list of labels for each image.
    &quot;&quot;&quot;
    num_pics = len(indexes)
    _, axarr = plt.subplots(1, num_pics)
    for idx, im_idx in enumerate(indexes):
        assert idx &lt; 10000, &#39;Cannot get such index, there are only 10000&#39;
        pic = np.rollaxis(images[im_idx].squeeze().numpy(), 0, 3)
        axarr[idx].imshow(pic)
        axarr[idx].set_title(labels_names[labels[im_idx]])
</pre></div>
</div>
<p>Use the code below, to define a function that uses the optimized model
to obtain predictions for the selected images:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def infer_on_images(net, indexes: list, images=all_images):
    &quot;&quot;&quot; Inference model on a set of images.
    :param net: model on which do inference
    :param indexes: a list of indexes of images to infer on.
    :param images: a list of images from the dataset.
    &quot;&quot;&quot;
    predicted_labels = []
    infer_request = net.create_infer_request()
    for idx in indexes:
        assert idx &lt; 10000, &#39;Cannot get such index, there are only 10000&#39;
        input_tensor = Tensor(array=images[idx].detach().numpy(), shared_memory=True)
        infer_request.set_input_tensor(input_tensor)
        infer_request.start_async()
        infer_request.wait()
        output = infer_request.get_output_tensor()
        result = list(output.data)
        result = labels_names[np.argmax(result[0])]
        predicted_labels.append(result)
    return predicted_labels
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>indexes_to_infer = [0, 1, 2]  # to plot specify indexes

plot_pictures(indexes_to_infer)

results_quanized = infer_on_images(compiled_model, indexes_to_infer)

print(f&quot;Image labels using the quantized model : {results_quanized}.&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Image</span> <span class="n">labels</span> <span class="n">using</span> <span class="n">the</span> <span class="n">quantized</span> <span class="n">model</span> <span class="p">:</span> <span class="p">[</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;ship&#39;</span><span class="p">,</span> <span class="s1">&#39;ship&#39;</span><span class="p">]</span><span class="o">.</span>
</pre></div>
</div>
<img alt="../_images/114-quantization-simplified-mode-with-output_22_1.png" src="../_images/114-quantization-simplified-mode-with-output_22_1.png" />
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  
    <a class='button bttn-sec button-size-l' id="prev-link" href="113-image-classification-quantization-with-output.html" title="previous page">Prev</a>
    <a class='button bttn-sec button-size-l' id="next-link" href="115-async-api-with-output.html" title="next page">Next</a>

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel®.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>