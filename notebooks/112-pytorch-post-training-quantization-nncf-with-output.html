
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Post-Training Quantization of PyTorch models with NNCF &#8212; OpenVINOâ„¢  documentation</title>
    
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="../_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="../_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/tabs.css" type="text/css" />
    <script src="../_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="../_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="../_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/tabs.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/custom.js"></script>
    <script src="../_static/js/graphs.js"></script>
    <script src="../_static/js/graphs_ov_tf.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/target-highlight.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://docs.openvino.ai/latest/notebooks/112-pytorch-post-training-quantization-nncf-with-output.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Quantization of Image Classification Models" href="113-image-classification-quantization-with-output.html" />
    <link rel="prev" title="Object Detection Quantization" href="111-detection-quantization-with-output.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/get-started-guide.html">
  Get Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/documentation.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api/api_reference.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../model_zoo.html">
  Model Zoo
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/resources.html">
  Resources
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/openvinotoolkit/openvino" rel="noopener" target="_blank" title="GitHub">
            <span><i class="sst-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="language-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">English</button>
  <div class="dropdown-menu" aria-labelledby="language-selector">
    
      
        <a class="dropdown-item font-weight-bold" href="/openvino-docs/index.html">English</a>
      
    
      
        <a  class="dropdown-item" href="/cn/openvino-docs/index.html">Chinese</a>
      
    
  </div>
</div>

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="../search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notebooks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks-installation.html">
   Installation of OpenVINOâ„¢ Notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="001-hello-world-with-output.html">
   Hello Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002-openvino-api-with-output.html">
   OpenVINOâ„¢ Runtime API Tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003-hello-segmentation-with-output.html">
   Hello Image Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="004-hello-detection-with-output.html">
   Hello Object Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="101-tensorflow-to-openvino-with-output.html">
   Convert a TensorFlow Model to OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="102-pytorch-onnx-to-openvino-with-output.html">
   Convert a PyTorch Model to ONNX and OpenVINOâ„¢ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="103-paddle-onnx-to-openvino-classification-with-output.html">
   Convert a PaddlePaddle Model to ONNX and OpenVINOâ„¢ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="104-model-tools-with-output.html">
   Working with Open Model Zoo Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="105-language-quantize-bert-with-output.html">
   Quantize NLP models with Post-Training Optimization Tool â€‹in OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="106-auto-device-with-output.html">
   Automatic Device Selection with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="107-speech-recognition-quantization-with-output.html">
   Quantize Speech Recognition Models with OpenVINOâ„¢ Post-Training Optimization Tool â€‹
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-nncf-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="111-detection-quantization-with-output.html">
   Object Detection Quantization
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Post-Training Quantization of PyTorch models with NNCF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="113-image-classification-quantization-with-output.html">
   Quantization of Image Classification Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="114-quantization-simplified-mode-with-output.html">
   INT8 Quantization with Post-training Optimization Tool (POT) in Simplified Mode tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="115-async-api-with-output.html">
   Asynchronous Inference with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="201-vision-monodepth-with-output.html">
   Monodepth Estimation with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-image-with-output.html">
   Single Image Super Resolution with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-video-with-output.html">
   Video Super Resolution with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="203-meter-reader-with-output.html">
   Industrial Meter Reader
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="204-named-entity-recognition-with-output.html">
   Document Entity Extraction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="205-vision-background-removal-with-output.html">
   Image Background Removal with U^2-Net and OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="206-vision-paddlegan-anime-with-output.html">
   Photos to Anime with PaddleGAN and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="207-vision-paddlegan-superresolution-with-output.html">
   Super Resolution with PaddleGAN and OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="208-optical-character-recognition-with-output.html">
   Optical Character Recognition (OCR) with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="209-handwritten-ocr-with-output.html">
   Handwritten Chinese and Japanese OCR with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="210-ct-scan-live-inference-with-output.html">
   Live Inference and Benchmark CT-scan Data with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="211-speech-to-text-with-output.html">
   Speech to Text with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="212-onnx-style-transfer-with-output.html">
   Style Transfer on ONNX Models with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="214-vision-paddle-classification-with-output.html">
   PaddlePaddle Image Classification with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="215-image-inpainting-with-output.html">
   Image In-painting with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="216-license-plate-recognition-with-output.html">
   License Plate Recognition with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="217-vision-deblur-with-output.html">
   Deblur Photos with DeblurGAN-v2 and OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="218-vehicle-detection-and-recognition-with-output.html">
   Vehicle Detection And Recognition with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="219-knowledge-graphs-conve-with-output.html">
   OpenVINO optimizations for Knowledge graphs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="220-yolov5-accuracy-check-and-quantization-with-output.html">
   Quantize the Ultralytics YOLOv5 model and check accuracy using the OpenVINO POT API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="221-machine-translation-with-output.html">
   Machine translation demo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="222-vision-image-colorization-with-output.html">
   Image Colorization with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="223-gpt2-text-prediction-with-output.html">
   GPT-2 Text Prediction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-pot-with-output.html">
   Post-Training Quantization with TensorFlow Classification Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-with-output.html">
   From Training to Deployment with TensorFlow and OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="302-pytorch-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using PyTorch framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="305-tensorflow-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using TensorFlow Framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="401-object-detection-with-output.html">
   Live Object Detection with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="402-pose-estimation-with-output.html">
   Live Human Pose Estimation with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="403-action-recognition-webcam-with-output.html">
   Human Action Recognition with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="405-paddle-ocr-webcam-with-output.html">
   PaddleOCR with OpenVINOâ„¢
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebook_utils-with-output.html">
   Notebook Utils
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preparations">
   Preparations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preparing-model-files">
     Preparing model files
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#imports">
     Imports
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#settings">
     Settings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#download-and-prepare-tiny-imagenet-dataset">
     Download and Prepare Tiny ImageNet dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#helpers-classes-and-functions">
     Helpers classes and functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#validation-function">
     Validation function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-and-load-original-uncompressed-model">
     Create and load original uncompressed model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-train-and-validation-dataloaders">
     Create train and validation dataloaders
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i-evaluate-the-loaded-model">
   I. Evaluate the loaded model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ii-create-and-initialize-quantization">
   II. Create and initialize quantization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iii-convert-onnx-models-to-openvino-intermediate-representation-openvino-ir">
   III. Convert ONNX models to OpenVINO Intermediate Representation (OpenVINO IR)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iv-compare-perfomance-of-int8-model-and-fp32-model-in-openvino">
   IV. Compare perfomance of INT8 model and FP32 model in OpenVINO
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">

<div class="tocsection editthispage">
    <a href="None">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

            
                <div>
                  
  <section id="post-training-quantization-of-pytorch-models-with-nncf">
<h1>Post-Training Quantization of PyTorch models with NNCF<a class="headerlink" href="#post-training-quantization-of-pytorch-models-with-nncf" title="Permalink to this headline">Â¶</a></h1>
<p>The goal of this tutorial is to demonstrate how to use the NNCF (Neural
Network Compression Framework) 8-bit quantization in post-training mode
(without the fine-tuning pipeline) to optimize a PyTorch model for the
high-speed inference via OpenVINOâ„¢ Toolkit. The optimization process
contains the following steps:</p>
<ol class="arabic simple">
<li><p>Evaluate the original model.</p></li>
<li><p>Transform the original model to a quantized one.</p></li>
<li><p>Export optimized and original models to ONNX and then to OpenVINO IR.</p></li>
<li><p>Compare performance of the obtained <code class="docutils literal notranslate"><span class="pre">FP32</span></code> and <code class="docutils literal notranslate"><span class="pre">INT8</span></code> models.</p></li>
</ol>
<p>This tutorial uses a ResNet-50 model, pre-trained on Tiny ImageNet,
which contains 100000 images of 200 classes (500 for each class)
downsized to 64Ã—64 colored images. The tutorial will demonstrate that
only a tiny part of the dataset is needed for the post-training
quantization, not demanding the fine-tuning of the model.</p>
<blockquote>
<div><p>NOTE: This notebook requires that a C++ compiler is accessible on the
default binary search path of the OS you are running the notebook.</p>
</div></blockquote>
<section id="preparations">
<h2>Preparations<a class="headerlink" href="#preparations" title="Permalink to this headline">Â¶</a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># On Windows, this script adds the directory that contains cl.exe to the PATH to enable PyTorch to find the
# required C++ tools. This code assumes that Visual Studio 2019 is installed in the default
# directory. If you have a different C++ compiler, add the correct path to os.environ[&quot;PATH&quot;]
# directly.

# Adding the path to os.environ[&quot;LIB&quot;] is not always required - it depends on the system configuration.

import sys

if sys.platform == &quot;win32&quot;:
    import distutils.command.build_ext
    import os
    from pathlib import Path

    VS_INSTALL_DIR = r&quot;C:/Program Files (x86)/Microsoft Visual Studio&quot;
    cl_paths = sorted(list(Path(VS_INSTALL_DIR).glob(&quot;**/Hostx86/x64/cl.exe&quot;)))
    if len(cl_paths) == 0:
        raise ValueError(
            &quot;Cannot find Visual Studio. This notebook requires C++. If you installed &quot;
            &quot;a C++ compiler, please add the directory that contains cl.exe to &quot;
            &quot;`os.environ[&#39;PATH&#39;]`&quot;
        )
    else:
        # If multiple versions of MSVC are installed, get the most recent one.
        cl_path = cl_paths[-1]
        vs_dir = str(cl_path.parent)
        os.environ[&quot;PATH&quot;] += f&quot;{os.pathsep}{vs_dir}&quot;
        # The code for finding the library dirs is from
        # https://stackoverflow.com/questions/47423246/get-pythons-lib-path
        d = distutils.core.Distribution()
        b = distutils.command.build_ext.build_ext(d)
        b.finalize_options()
        os.environ[&quot;LIB&quot;] = os.pathsep.join(b.library_dirs)
        print(f&quot;Added {vs_dir} to PATH&quot;)
</pre></div>
</div>
<section id="preparing-model-files">
<h3>Preparing model files<a class="headerlink" href="#preparing-model-files" title="Permalink to this headline">Â¶</a></h3>
<blockquote>
<div><p><strong>Note</strong>: All NNCF logging messages below ERROR level (INFO and
WARNING) are disabled to simplify the tutorial. For production use,
it is recommended to enable logging by removing
<code class="docutils literal notranslate"><span class="pre">set_log_level(logging.ERROR)</span></code>.</p>
</div></blockquote>
</section>
<section id="imports">
<h3>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">Â¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import logging
import os
import sys
import time
import warnings
import zipfile
from pathlib import Path
from typing import List, Tuple

import torch
import torch.nn as nn
import torch.utils.data
import torchvision.datasets as datasets
import torchvision.models as models
import torchvision.transforms as transforms
from nncf import NNCFConfig  # Important - should be imported directly after torch
from nncf.common.utils.logger import set_log_level

set_log_level(logging.ERROR)  # Disables all NNCF info and warning messages
from nncf.torch import create_compressed_model, register_default_init_args
from openvino.runtime import Core
from torch.jit import TracerWarning

sys.path.append(&quot;../utils&quot;)
from notebook_utils import download_file
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/.</span><span class="n">venv</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">nncf</span><span class="o">/</span><span class="n">torch</span><span class="o">/</span><span class="fm">__init__</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">23</span><span class="p">:</span> <span class="ne">UserWarning</span><span class="p">:</span> <span class="n">NNCF</span> <span class="n">provides</span> <span class="n">best</span> <span class="n">results</span> <span class="k">with</span> <span class="n">torch</span><span class="o">==</span><span class="mf">1.9.1</span><span class="p">,</span> <span class="k">while</span> <span class="n">current</span> <span class="n">torch</span> <span class="n">version</span> <span class="ow">is</span> <span class="mf">1.8.1</span><span class="o">+</span><span class="n">cpu</span> <span class="o">-</span> <span class="n">consider</span> <span class="n">switching</span> <span class="n">to</span> <span class="n">torch</span><span class="o">==</span><span class="mf">1.9.1</span>
  <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;NNCF provides best results with torch==</span><span class="si">{bkc}</span><span class="s2">, &quot;</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">No</span> <span class="n">CUDA</span> <span class="n">runtime</span> <span class="ow">is</span> <span class="n">found</span><span class="p">,</span> <span class="n">using</span> <span class="n">CUDA_HOME</span><span class="o">=</span><span class="s1">&#39;/usr/local/cuda&#39;</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/.</span><span class="n">venv</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">nncf</span><span class="o">/</span><span class="n">torch</span><span class="o">/</span><span class="n">dynamic_graph</span><span class="o">/</span><span class="n">patch_pytorch</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">163</span><span class="p">:</span> <span class="ne">UserWarning</span><span class="p">:</span> <span class="n">Not</span> <span class="n">patching</span> <span class="n">unique_dim</span> <span class="n">since</span> <span class="n">it</span> <span class="ow">is</span> <span class="n">missing</span> <span class="ow">in</span> <span class="n">this</span> <span class="n">version</span> <span class="n">of</span> <span class="n">PyTorch</span>
  <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Not patching </span><span class="si">{}</span><span class="s2"> since it is missing in this version of PyTorch&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">op_name</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="settings">
<h3>Settings<a class="headerlink" href="#settings" title="Permalink to this headline">Â¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
print(f&quot;Using {device} device&quot;)

MODEL_DIR = Path(&quot;model&quot;)
OUTPUT_DIR = Path(&quot;output&quot;)
BASE_MODEL_NAME = &quot;resnet50&quot;
IMAGE_SIZE = [64, 64]

OUTPUT_DIR.mkdir(exist_ok=True)
MODEL_DIR.mkdir(exist_ok=True)

# Paths where PyTorch, ONNX and OpenVINO IR models will be stored.
fp32_checkpoint_filename = Path(BASE_MODEL_NAME + &quot;_fp32&quot;).with_suffix(&quot;.pth&quot;)
fp32_onnx_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME + &quot;_fp32&quot;)).with_suffix(&quot;.onnx&quot;)
fp32_ir_path = fp32_onnx_path.with_suffix(&quot;.xml&quot;)
int8_onnx_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME + &quot;_int8&quot;)).with_suffix(&quot;.onnx&quot;)
int8_ir_path = int8_onnx_path.with_suffix(&quot;.xml&quot;)


fp32_pth_url = &quot;https://storage.openvinotoolkit.org/repositories/nncf/openvino_notebook_ckpts/304_resnet50_fp32.pth&quot;
download_file(fp32_pth_url, directory=MODEL_DIR, filename=fp32_checkpoint_filename)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Using</span> <span class="n">cpu</span> <span class="n">device</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>model/resnet50_fp32.pth:   0%|          | 0.00/91.5M [00:00&lt;?, ?B/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PosixPath</span><span class="p">(</span><span class="s1">&#39;/opt/home/k8sworker/cibuilds/ov-notebook/OVNotebookOps-231/.workspace/scm/ov-notebook/notebooks/112-pytorch-post-training-quantization-nncf/model/resnet50_fp32.pth&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="download-and-prepare-tiny-imagenet-dataset">
<h3>Download and Prepare Tiny ImageNet dataset<a class="headerlink" href="#download-and-prepare-tiny-imagenet-dataset" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p>100k images of shape 3x64x64,</p></li>
<li><p>200 different classes: snake, spider, cat, truck, grasshopper, gull,
etc.</p></li>
</ul>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def download_tiny_imagenet_200(
    output_dir: Path,
    url: str = &quot;http://cs231n.stanford.edu/tiny-imagenet-200.zip&quot;,
    tarname: str = &quot;tiny-imagenet-200.zip&quot;,
):
    archive_path = output_dir / tarname
    download_file(url, directory=output_dir, filename=tarname)
    zip_ref = zipfile.ZipFile(archive_path, &quot;r&quot;)
    zip_ref.extractall(path=output_dir)
    zip_ref.close()
    print(f&quot;Successfully downloaded and extracted dataset to: {output_dir}&quot;)


def create_validation_dir(dataset_dir: Path):
    VALID_DIR = dataset_dir / &quot;val&quot;
    val_img_dir = VALID_DIR / &quot;images&quot;

    fp = open(VALID_DIR / &quot;val_annotations.txt&quot;, &quot;r&quot;)
    data = fp.readlines()

    val_img_dict = {}
    for line in data:
        words = line.split(&quot;\t&quot;)
        val_img_dict[words[0]] = words[1]
    fp.close()

    for img, folder in val_img_dict.items():
        newpath = val_img_dir / folder
        if not newpath.exists():
            os.makedirs(newpath)
        if (val_img_dir / img).exists():
            os.rename(val_img_dir / img, newpath / img)


DATASET_DIR = OUTPUT_DIR / &quot;tiny-imagenet-200&quot;
if not DATASET_DIR.exists():
    download_tiny_imagenet_200(OUTPUT_DIR)
    create_validation_dir(DATASET_DIR)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>output/tiny-imagenet-200.zip:   0%|          | 0.00/237M [00:00&lt;?, ?B/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Successfully</span> <span class="n">downloaded</span> <span class="ow">and</span> <span class="n">extracted</span> <span class="n">dataset</span> <span class="n">to</span><span class="p">:</span> <span class="n">output</span>
</pre></div>
</div>
</section>
<section id="helpers-classes-and-functions">
<h3>Helpers classes and functions<a class="headerlink" href="#helpers-classes-and-functions" title="Permalink to this headline">Â¶</a></h3>
<p>The code below will help to count accuracy and visualize validation
process.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class AverageMeter(object):
    &quot;&quot;&quot;Computes and stores the average and current value&quot;&quot;&quot;

    def __init__(self, name: str, fmt: str = &quot;:f&quot;):
        self.name = name
        self.fmt = fmt
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val: float, n: int = 1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

    def __str__(self):
        fmtstr = &quot;{name} {val&quot; + self.fmt + &quot;} ({avg&quot; + self.fmt + &quot;})&quot;
        return fmtstr.format(**self.__dict__)


class ProgressMeter(object):
    &quot;&quot;&quot;Displays the progress of validation process&quot;&quot;&quot;

    def __init__(self, num_batches: int, meters: List[AverageMeter], prefix: str = &quot;&quot;):
        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)
        self.meters = meters
        self.prefix = prefix

    def display(self, batch: int):
        entries = [self.prefix + self.batch_fmtstr.format(batch)]
        entries += [str(meter) for meter in self.meters]
        print(&quot;\t&quot;.join(entries))

    def _get_batch_fmtstr(self, num_batches: int):
        num_digits = len(str(num_batches // 1))
        fmt = &quot;{:&quot; + str(num_digits) + &quot;d}&quot;
        return &quot;[&quot; + fmt + &quot;/&quot; + fmt.format(num_batches) + &quot;]&quot;


def accuracy(output: torch.Tensor, target: torch.Tensor, topk: Tuple[int] = (1,)):
    &quot;&quot;&quot;Computes the accuracy over the k top predictions for the specified values of k&quot;&quot;&quot;
    with torch.no_grad():
        maxk = max(topk)
        batch_size = target.size(0)

        _, pred = output.topk(maxk, 1, True, True)
        pred = pred.t()
        correct = pred.eq(target.view(1, -1).expand_as(pred))

        res = []
        for k in topk:
            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)
            res.append(correct_k.mul_(100.0 / batch_size))
        return res
</pre></div>
</div>
</section>
<section id="validation-function">
<h3>Validation function<a class="headerlink" href="#validation-function" title="Permalink to this headline">Â¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def validate(val_loader: torch.utils.data.DataLoader, model: torch.nn.Module):
    &quot;&quot;&quot;Compute the metrics using data from val_loader for the model&quot;&quot;&quot;
    batch_time = AverageMeter(&quot;Time&quot;, &quot;:3.3f&quot;)
    top1 = AverageMeter(&quot;Acc@1&quot;, &quot;:2.2f&quot;)
    top5 = AverageMeter(&quot;Acc@5&quot;, &quot;:2.2f&quot;)
    progress = ProgressMeter(len(val_loader), [batch_time, top1, top5], prefix=&quot;Test: &quot;)

    # Switch to evaluate mode.
    model.eval()
    model.to(device)

    with torch.no_grad():
        end = time.time()
        for i, (images, target) in enumerate(val_loader):
            images = images.to(device)
            target = target.to(device)

            # Compute the output.
            output = model(images)

            # Measure accuracy and record loss.
            acc1, acc5 = accuracy(output, target, topk=(1, 5))
            top1.update(acc1[0], images.size(0))
            top5.update(acc5[0], images.size(0))

            # Measure elapsed time.
            batch_time.update(time.time() - end)
            end = time.time()

            print_frequency = 10
            if i % print_frequency == 0:
                progress.display(i)

        print(
            &quot; * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}&quot;.format(top1=top1, top5=top5)
        )
    return top1.avg
</pre></div>
</div>
</section>
<section id="create-and-load-original-uncompressed-model">
<h3>Create and load original uncompressed model<a class="headerlink" href="#create-and-load-original-uncompressed-model" title="Permalink to this headline">Â¶</a></h3>
<p>ResNet-50 from the <a class="reference external" href="https://github.com/pytorch/vision">torchivision
repository</a> is pre-trained on
ImageNet with more prediction classes than Tiny ImageNet, so the model
is adjusted by swapping the last FC layer to one with fewer output
values.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def create_model(model_path: Path):
    &quot;&quot;&quot;Creates the ResNet-50 model and loads the pretrained weights&quot;&quot;&quot;
    model = models.resnet50()
    # Update the last FC layer for Tiny ImageNet number of classes.
    NUM_CLASSES = 200
    model.fc = nn.Linear(in_features=2048, out_features=NUM_CLASSES, bias=True)
    model.to(device)
    if model_path.exists():
        checkpoint = torch.load(str(model_path), map_location=&quot;cpu&quot;)
        model.load_state_dict(checkpoint[&quot;state_dict&quot;], strict=True)
    else:
        raise RuntimeError(&quot;There is no checkpoint to load&quot;)
    return model


model = create_model(MODEL_DIR / fp32_checkpoint_filename)
</pre></div>
</div>
</section>
<section id="create-train-and-validation-dataloaders">
<h3>Create train and validation dataloaders<a class="headerlink" href="#create-train-and-validation-dataloaders" title="Permalink to this headline">Â¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def create_dataloaders(batch_size: int = 128):
    &quot;&quot;&quot;Creates train dataloader that is used for quantization initialization and validation dataloader for computing the model accruacy&quot;&quot;&quot;
    train_dir = DATASET_DIR / &quot;train&quot;
    val_dir = DATASET_DIR / &quot;val&quot; / &quot;images&quot;
    normalize = transforms.Normalize(
        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
    )
    train_dataset = datasets.ImageFolder(
        train_dir,
        transforms.Compose(
            [
                transforms.Resize(IMAGE_SIZE),
                transforms.ToTensor(),
                normalize,
            ]
        ),
    )
    val_dataset = datasets.ImageFolder(
        val_dir,
        transforms.Compose(
            [transforms.Resize(IMAGE_SIZE), transforms.ToTensor(), normalize]
        ),
    )

    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        sampler=None,
    )

    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True,
    )
    return train_loader, val_loader


train_loader, val_loader = create_dataloaders()
</pre></div>
</div>
<p>Now that the preparation of the training and validation pipelines, and
the model files within this notebook is done, it is time to perform
actual post-training quantization with NNCF.</p>
</section>
</section>
<section id="i-evaluate-the-loaded-model">
<h2>I. Evaluate the loaded model<a class="headerlink" href="#i-evaluate-the-loaded-model" title="Permalink to this headline">Â¶</a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>acc1 = validate(val_loader, model)
print(f&quot;Test accuracy of FP32 model: {acc1:.3f}&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Test</span><span class="p">:</span> <span class="p">[</span> <span class="mi">0</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.429</span> <span class="p">(</span><span class="mf">0.429</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">81.25</span> <span class="p">(</span><span class="mf">81.25</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">92.19</span> <span class="p">(</span><span class="mf">92.19</span><span class="p">)</span>
<span class="n">Test</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.310</span> <span class="p">(</span><span class="mf">0.313</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">56.25</span> <span class="p">(</span><span class="mf">66.97</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">86.72</span> <span class="p">(</span><span class="mf">87.50</span><span class="p">)</span>
<span class="n">Test</span><span class="p">:</span> <span class="p">[</span><span class="mi">20</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.282</span> <span class="p">(</span><span class="mf">0.302</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">67.97</span> <span class="p">(</span><span class="mf">64.29</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">85.16</span> <span class="p">(</span><span class="mf">87.35</span><span class="p">)</span>
<span class="n">Test</span><span class="p">:</span> <span class="p">[</span><span class="mi">30</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.287</span> <span class="p">(</span><span class="mf">0.299</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">53.12</span> <span class="p">(</span><span class="mf">62.37</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">77.34</span> <span class="p">(</span><span class="mf">85.33</span><span class="p">)</span>
<span class="n">Test</span><span class="p">:</span> <span class="p">[</span><span class="mi">40</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.294</span> <span class="p">(</span><span class="mf">0.297</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">67.19</span> <span class="p">(</span><span class="mf">60.86</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">90.62</span> <span class="p">(</span><span class="mf">84.51</span><span class="p">)</span>
<span class="n">Test</span><span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.299</span> <span class="p">(</span><span class="mf">0.297</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">60.16</span> <span class="p">(</span><span class="mf">60.80</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">88.28</span> <span class="p">(</span><span class="mf">84.42</span><span class="p">)</span>
<span class="n">Test</span><span class="p">:</span> <span class="p">[</span><span class="mi">60</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.292</span> <span class="p">(</span><span class="mf">0.297</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">66.41</span> <span class="p">(</span><span class="mf">60.46</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">86.72</span> <span class="p">(</span><span class="mf">83.79</span><span class="p">)</span>
<span class="n">Test</span><span class="p">:</span> <span class="p">[</span><span class="mi">70</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.269</span> <span class="p">(</span><span class="mf">0.296</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">52.34</span> <span class="p">(</span><span class="mf">60.21</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">80.47</span> <span class="p">(</span><span class="mf">83.33</span><span class="p">)</span>
 <span class="o">*</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">60.740</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">83.960</span>
<span class="n">Test</span> <span class="n">accuracy</span> <span class="n">of</span> <span class="n">FP32</span> <span class="n">model</span><span class="p">:</span> <span class="mf">60.740</span>
</pre></div>
</div>
<p>Export the <code class="docutils literal notranslate"><span class="pre">FP32</span></code> model to the ONNX, which is supported by OpenVINO
Toolkit, to benchmark it in comparison with the <code class="docutils literal notranslate"><span class="pre">INT8</span></code> model.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>dummy_input = torch.randn(1, 3, *IMAGE_SIZE).to(device)
torch.onnx.export(model, dummy_input, fp32_onnx_path, opset_version=10)
print(f&quot;FP32 ONNX model was exported to {fp32_onnx_path}.&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">FP32</span> <span class="n">ONNX</span> <span class="n">model</span> <span class="n">was</span> <span class="n">exported</span> <span class="n">to</span> <span class="n">output</span><span class="o">/</span><span class="n">resnet50_fp32</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span>
</pre></div>
</div>
</section>
<section id="ii-create-and-initialize-quantization">
<h2>II. Create and initialize quantization<a class="headerlink" href="#ii-create-and-initialize-quantization" title="Permalink to this headline">Â¶</a></h2>
<p>NNCF enables post-training quantization by adding the quantization
layers into the model graph and then using a subset of the training
dataset to initialize the parameters of these additional quantization
layers. The framework is designed so that modifications to your original
training code are minor. Quantization is the simplest scenario and
requires only 3 modifications.</p>
<ol class="arabic simple">
<li><p>Configure the NNCF parameters to specify compression:</p></li>
</ol>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>nncf_config_dict = {
    &quot;input_info&quot;: {&quot;sample_size&quot;: [1, 3, *IMAGE_SIZE]},
    &quot;log_dir&quot;: str(OUTPUT_DIR),
    &quot;compression&quot;: {
        &quot;algorithm&quot;: &quot;quantization&quot;,
        &quot;initializer&quot;: {
            &quot;range&quot;: {&quot;num_init_samples&quot;: 15000},
            &quot;batchnorm_adaptation&quot;: {&quot;num_bn_adaptation_samples&quot;: 4000},
        },
    },
}

nncf_config = NNCFConfig.from_dict(nncf_config_dict)
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Provide a data loader to initialize the values of quantization ranges
and determine which activation should be signed or unsigned from the
collected statistics, using a given number of samples.</p></li>
</ol>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>nncf_config = register_default_init_args(nncf_config, train_loader)
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Create a quantized model from a pre-trained <code class="docutils literal notranslate"><span class="pre">FP32</span></code> model and a
configuration object.</p></li>
</ol>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>compression_ctrl, model = create_compressed_model(model, nncf_config)
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Evaluate the new model on the validation set after initialization of
quantization. The accuracy should be close to the accuracy of the
floating-point <code class="docutils literal notranslate"><span class="pre">FP32</span></code> model for a simple case like the one being
demonstrated now.</p></li>
</ol>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>acc1 = validate(val_loader, model)
print(f&quot;Accuracy of initialized INT8 model: {acc1:.3f}&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Test</span><span class="p">:</span> <span class="p">[</span> <span class="mi">0</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.847</span> <span class="p">(</span><span class="mf">0.847</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">82.03</span> <span class="p">(</span><span class="mf">82.03</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">92.19</span> <span class="p">(</span><span class="mf">92.19</span><span class="p">)</span>
<span class="n">Test</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.389</span> <span class="p">(</span><span class="mf">0.435</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">57.81</span> <span class="p">(</span><span class="mf">68.25</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">87.50</span> <span class="p">(</span><span class="mf">87.78</span><span class="p">)</span>
<span class="n">Test</span><span class="p">:</span> <span class="p">[</span><span class="mi">20</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.406</span> <span class="p">(</span><span class="mf">0.415</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">68.75</span> <span class="p">(</span><span class="mf">64.40</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">85.94</span> <span class="p">(</span><span class="mf">87.28</span><span class="p">)</span>
<span class="n">Test</span><span class="p">:</span> <span class="p">[</span><span class="mi">30</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.390</span> <span class="p">(</span><span class="mf">0.407</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">52.34</span> <span class="p">(</span><span class="mf">62.12</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">75.00</span> <span class="p">(</span><span class="mf">84.90</span><span class="p">)</span>
<span class="n">Test</span><span class="p">:</span> <span class="p">[</span><span class="mi">40</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.389</span> <span class="p">(</span><span class="mf">0.404</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">66.41</span> <span class="p">(</span><span class="mf">60.79</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">88.28</span> <span class="p">(</span><span class="mf">84.01</span><span class="p">)</span>
<span class="n">Test</span><span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.392</span> <span class="p">(</span><span class="mf">0.403</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">59.38</span> <span class="p">(</span><span class="mf">60.74</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">85.94</span> <span class="p">(</span><span class="mf">84.02</span><span class="p">)</span>
<span class="n">Test</span><span class="p">:</span> <span class="p">[</span><span class="mi">60</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.391</span> <span class="p">(</span><span class="mf">0.401</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">67.19</span> <span class="p">(</span><span class="mf">60.44</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">85.16</span> <span class="p">(</span><span class="mf">83.27</span><span class="p">)</span>
<span class="n">Test</span><span class="p">:</span> <span class="p">[</span><span class="mi">70</span><span class="o">/</span><span class="mi">79</span><span class="p">]</span>   <span class="n">Time</span> <span class="mf">0.380</span> <span class="p">(</span><span class="mf">0.400</span><span class="p">)</span>  <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">54.69</span> <span class="p">(</span><span class="mf">60.20</span><span class="p">)</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">79.69</span> <span class="p">(</span><span class="mf">82.92</span><span class="p">)</span>
 <span class="o">*</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">1</span> <span class="mf">60.740</span> <span class="n">Acc</span><span class="o">@</span><span class="mi">5</span> <span class="mf">83.490</span>
<span class="n">Accuracy</span> <span class="n">of</span> <span class="n">initialized</span> <span class="n">INT8</span> <span class="n">model</span><span class="p">:</span> <span class="mf">60.740</span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>Export an <code class="docutils literal notranslate"><span class="pre">INT8</span></code> model to ONNX</p></li>
</ol>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>warnings.filterwarnings(&quot;ignore&quot;, category=TracerWarning)  # Ignore export warnings
warnings.filterwarnings(&quot;ignore&quot;, category=UserWarning)
compression_ctrl.export_model(int8_onnx_path)
print(f&quot;INT8 ONNX model exported to {int8_onnx_path}.&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">INT8</span> <span class="n">ONNX</span> <span class="n">model</span> <span class="n">exported</span> <span class="n">to</span> <span class="n">output</span><span class="o">/</span><span class="n">resnet50_int8</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span>
</pre></div>
</div>
</section>
<section id="iii-convert-onnx-models-to-openvino-intermediate-representation-openvino-ir">
<h2>III. Convert ONNX models to OpenVINO Intermediate Representation (OpenVINO IR)<a class="headerlink" href="#iii-convert-onnx-models-to-openvino-intermediate-representation-openvino-ir" title="Permalink to this headline">Â¶</a></h2>
<p>Use Model Optimizer to convert the ONNX model to OpenVINO IR, with
<code class="docutils literal notranslate"><span class="pre">FP16</span></code> precision. The models are saved to the current directory. Then,
add the mean values to the model and scale the output with the standard
deviation by <code class="docutils literal notranslate"><span class="pre">--mean_values</span></code> and <code class="docutils literal notranslate"><span class="pre">--scale_values</span></code> arguments. It is
not necessary to normalize input data before propagating it through the
network with these options.</p>
<p>For more information about Model Optimizer, refer to the <a class="reference external" href="https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html">Model
Optimizer Developer
Guide</a>.</p>
<p>Executing the following command may take a while. There may be some
errors or warnings in the output. When Model Optimizer converts the
model to OpenVINO IR successfully, the last lines of the output will
include: <code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">SUCCESS</span> <span class="pre">]</span> <span class="pre">Generated</span> <span class="pre">IR</span> <span class="pre">version</span> <span class="pre">11</span> <span class="pre">model</span></code>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>input_shape = [1, 3, *IMAGE_SIZE]
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>if not fp32_ir_path.exists():
    !mo --input_model &quot;$fp32_onnx_path&quot; --input_shape &quot;$input_shape&quot; --mean_values &quot;[123.675, 116.28 , 103.53]&quot; --scale_values &quot;[58.395, 57.12 , 57.375]&quot; --data_type FP16 --output_dir &quot;$OUTPUT_DIR&quot;
    assert fp32_ir_path.exists(), &quot;The OpenVINO IR of FP32 model wasn&#39;t created&quot;
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Model</span> <span class="n">Optimizer</span> <span class="n">arguments</span><span class="p">:</span>
<span class="n">Common</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="o">-</span> <span class="n">Path</span> <span class="n">to</span> <span class="n">the</span> <span class="n">Input</span> <span class="n">Model</span><span class="p">:</span>  <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">112</span><span class="o">-</span><span class="n">pytorch</span><span class="o">-</span><span class="n">post</span><span class="o">-</span><span class="n">training</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">nncf</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">resnet50_fp32</span><span class="o">.</span><span class="n">onnx</span>
    <span class="o">-</span> <span class="n">Path</span> <span class="k">for</span> <span class="n">generated</span> <span class="n">IR</span><span class="p">:</span>    <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">112</span><span class="o">-</span><span class="n">pytorch</span><span class="o">-</span><span class="n">post</span><span class="o">-</span><span class="n">training</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">nncf</span><span class="o">/</span><span class="n">output</span>
    <span class="o">-</span> <span class="n">IR</span> <span class="n">output</span> <span class="n">name</span><span class="p">:</span>   <span class="n">resnet50_fp32</span>
    <span class="o">-</span> <span class="n">Log</span> <span class="n">level</span><span class="p">:</span>    <span class="n">ERROR</span>
    <span class="o">-</span> <span class="n">Batch</span><span class="p">:</span>    <span class="n">Not</span> <span class="n">specified</span><span class="p">,</span> <span class="n">inherited</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">model</span>
    <span class="o">-</span> <span class="n">Input</span> <span class="n">layers</span><span class="p">:</span>     <span class="n">Not</span> <span class="n">specified</span><span class="p">,</span> <span class="n">inherited</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">model</span>
    <span class="o">-</span> <span class="n">Output</span> <span class="n">layers</span><span class="p">:</span>    <span class="n">Not</span> <span class="n">specified</span><span class="p">,</span> <span class="n">inherited</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">model</span>
    <span class="o">-</span> <span class="n">Input</span> <span class="n">shapes</span><span class="p">:</span>     <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
    <span class="o">-</span> <span class="n">Source</span> <span class="n">layout</span><span class="p">:</span>    <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Target</span> <span class="n">layout</span><span class="p">:</span>    <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Layout</span><span class="p">:</span>   <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Mean</span> <span class="n">values</span><span class="p">:</span>  <span class="p">[</span><span class="mf">123.675</span><span class="p">,</span> <span class="mf">116.28</span> <span class="p">,</span> <span class="mf">103.53</span><span class="p">]</span>
    <span class="o">-</span> <span class="n">Scale</span> <span class="n">values</span><span class="p">:</span>     <span class="p">[</span><span class="mf">58.395</span><span class="p">,</span> <span class="mf">57.12</span> <span class="p">,</span> <span class="mf">57.375</span><span class="p">]</span>
    <span class="o">-</span> <span class="n">Scale</span> <span class="n">factor</span><span class="p">:</span>     <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Precision</span> <span class="n">of</span> <span class="n">IR</span><span class="p">:</span>  <span class="n">FP16</span>
    <span class="o">-</span> <span class="n">Enable</span> <span class="n">fusing</span><span class="p">:</span>    <span class="kc">True</span>
    <span class="o">-</span> <span class="n">User</span> <span class="n">transformations</span><span class="p">:</span>     <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Reverse</span> <span class="nb">input</span> <span class="n">channels</span><span class="p">:</span>   <span class="kc">False</span>
    <span class="o">-</span> <span class="n">Enable</span> <span class="n">IR</span> <span class="n">generation</span> <span class="k">for</span> <span class="n">fixed</span> <span class="nb">input</span> <span class="n">shape</span><span class="p">:</span>   <span class="kc">False</span>
    <span class="o">-</span> <span class="n">Use</span> <span class="n">the</span> <span class="n">transformations</span> <span class="n">config</span> <span class="n">file</span><span class="p">:</span>  <span class="kc">None</span>
<span class="n">Advanced</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="o">-</span> <span class="n">Force</span> <span class="n">the</span> <span class="n">usage</span> <span class="n">of</span> <span class="n">legacy</span> <span class="n">Frontend</span> <span class="n">of</span> <span class="n">Model</span> <span class="n">Optimizer</span> <span class="k">for</span> <span class="n">model</span> <span class="n">conversion</span> <span class="n">into</span> <span class="n">IR</span><span class="p">:</span>   <span class="kc">False</span>
    <span class="o">-</span> <span class="n">Force</span> <span class="n">the</span> <span class="n">usage</span> <span class="n">of</span> <span class="n">new</span> <span class="n">Frontend</span> <span class="n">of</span> <span class="n">Model</span> <span class="n">Optimizer</span> <span class="k">for</span> <span class="n">model</span> <span class="n">conversion</span> <span class="n">into</span> <span class="n">IR</span><span class="p">:</span>  <span class="kc">False</span>
<span class="n">OpenVINO</span> <span class="n">runtime</span> <span class="n">found</span> <span class="ow">in</span><span class="p">:</span>  <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/.</span><span class="n">venv</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">openvino</span>
<span class="n">OpenVINO</span> <span class="n">runtime</span> <span class="n">version</span><span class="p">:</span>   <span class="mf">2022.1.0</span><span class="o">-</span><span class="mi">7019</span><span class="o">-</span><span class="n">cdb9bec7210</span><span class="o">-</span><span class="n">releases</span><span class="o">/</span><span class="mi">2022</span><span class="o">/</span><span class="mi">1</span>
<span class="n">Model</span> <span class="n">Optimizer</span> <span class="n">version</span><span class="p">:</span>    <span class="mf">2022.1.0</span><span class="o">-</span><span class="mi">7019</span><span class="o">-</span><span class="n">cdb9bec7210</span><span class="o">-</span><span class="n">releases</span><span class="o">/</span><span class="mi">2022</span><span class="o">/</span><span class="mi">1</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">Generated</span> <span class="n">IR</span> <span class="n">version</span> <span class="mi">11</span> <span class="n">model</span><span class="o">.</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">XML</span> <span class="n">file</span><span class="p">:</span> <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">112</span><span class="o">-</span><span class="n">pytorch</span><span class="o">-</span><span class="n">post</span><span class="o">-</span><span class="n">training</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">nncf</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">resnet50_fp32</span><span class="o">.</span><span class="n">xml</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">BIN</span> <span class="n">file</span><span class="p">:</span> <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">112</span><span class="o">-</span><span class="n">pytorch</span><span class="o">-</span><span class="n">post</span><span class="o">-</span><span class="n">training</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">nncf</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">resnet50_fp32</span><span class="o">.</span><span class="n">bin</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">Total</span> <span class="n">execution</span> <span class="n">time</span><span class="p">:</span> <span class="mf">0.77</span> <span class="n">seconds</span><span class="o">.</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">Memory</span> <span class="n">consumed</span><span class="p">:</span> <span class="mi">268</span> <span class="n">MB</span><span class="o">.</span>
<span class="n">It</span><span class="s1">&#39;s been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&amp;source=prod&amp;campid=ww_2022_bu_IOTG_OpenVINO-2022-1&amp;content=upg_all&amp;medium=organic or on the GitHub*</span>
<span class="p">[</span> <span class="n">INFO</span> <span class="p">]</span> <span class="n">The</span> <span class="n">model</span> <span class="n">was</span> <span class="n">converted</span> <span class="n">to</span> <span class="n">IR</span> <span class="n">v11</span><span class="p">,</span> <span class="n">the</span> <span class="n">latest</span> <span class="n">model</span> <span class="nb">format</span> <span class="n">that</span> <span class="n">corresponds</span> <span class="n">to</span> <span class="n">the</span> <span class="n">source</span> <span class="n">DL</span> <span class="n">framework</span> <span class="nb">input</span><span class="o">/</span><span class="n">output</span> <span class="nb">format</span><span class="o">.</span> <span class="n">While</span> <span class="n">IR</span> <span class="n">v11</span> <span class="ow">is</span> <span class="n">backwards</span> <span class="n">compatible</span> <span class="k">with</span> <span class="n">OpenVINO</span> <span class="n">Inference</span> <span class="n">Engine</span> <span class="n">API</span> <span class="n">v1</span><span class="mf">.0</span><span class="p">,</span> <span class="n">please</span> <span class="n">use</span> <span class="n">API</span> <span class="n">v2</span><span class="mf">.0</span> <span class="p">(</span><span class="k">as</span> <span class="n">of</span> <span class="mf">2022.1</span><span class="p">)</span> <span class="n">to</span> <span class="n">take</span> <span class="n">advantage</span> <span class="n">of</span> <span class="n">the</span> <span class="n">latest</span> <span class="n">improvements</span> <span class="ow">in</span> <span class="n">IR</span> <span class="n">v11</span><span class="o">.</span>
<span class="n">Find</span> <span class="n">more</span> <span class="n">information</span> <span class="n">about</span> <span class="n">API</span> <span class="n">v2</span><span class="mf">.0</span> <span class="ow">and</span> <span class="n">IR</span> <span class="n">v11</span> <span class="n">at</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">docs</span><span class="o">.</span><span class="n">openvino</span><span class="o">.</span><span class="n">ai</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>if not int8_ir_path.exists():
    !mo --input_model &quot;$int8_onnx_path&quot; --input_shape &quot;$input_shape&quot; --mean_values &quot;[123.675, 116.28 , 103.53]&quot; --scale_values &quot;[58.395, 57.12 , 57.375]&quot; --data_type FP16 --output_dir &quot;$OUTPUT_DIR&quot;
    assert int8_ir_path.exists(), &quot;The OpenVINO IR of INT8 model wasn&#39;t created&quot;
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Model</span> <span class="n">Optimizer</span> <span class="n">arguments</span><span class="p">:</span>
<span class="n">Common</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="o">-</span> <span class="n">Path</span> <span class="n">to</span> <span class="n">the</span> <span class="n">Input</span> <span class="n">Model</span><span class="p">:</span>  <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">112</span><span class="o">-</span><span class="n">pytorch</span><span class="o">-</span><span class="n">post</span><span class="o">-</span><span class="n">training</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">nncf</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">resnet50_int8</span><span class="o">.</span><span class="n">onnx</span>
    <span class="o">-</span> <span class="n">Path</span> <span class="k">for</span> <span class="n">generated</span> <span class="n">IR</span><span class="p">:</span>    <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">112</span><span class="o">-</span><span class="n">pytorch</span><span class="o">-</span><span class="n">post</span><span class="o">-</span><span class="n">training</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">nncf</span><span class="o">/</span><span class="n">output</span>
    <span class="o">-</span> <span class="n">IR</span> <span class="n">output</span> <span class="n">name</span><span class="p">:</span>   <span class="n">resnet50_int8</span>
    <span class="o">-</span> <span class="n">Log</span> <span class="n">level</span><span class="p">:</span>    <span class="n">ERROR</span>
    <span class="o">-</span> <span class="n">Batch</span><span class="p">:</span>    <span class="n">Not</span> <span class="n">specified</span><span class="p">,</span> <span class="n">inherited</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">model</span>
    <span class="o">-</span> <span class="n">Input</span> <span class="n">layers</span><span class="p">:</span>     <span class="n">Not</span> <span class="n">specified</span><span class="p">,</span> <span class="n">inherited</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">model</span>
    <span class="o">-</span> <span class="n">Output</span> <span class="n">layers</span><span class="p">:</span>    <span class="n">Not</span> <span class="n">specified</span><span class="p">,</span> <span class="n">inherited</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">model</span>
    <span class="o">-</span> <span class="n">Input</span> <span class="n">shapes</span><span class="p">:</span>     <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
    <span class="o">-</span> <span class="n">Source</span> <span class="n">layout</span><span class="p">:</span>    <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Target</span> <span class="n">layout</span><span class="p">:</span>    <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Layout</span><span class="p">:</span>   <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Mean</span> <span class="n">values</span><span class="p">:</span>  <span class="p">[</span><span class="mf">123.675</span><span class="p">,</span> <span class="mf">116.28</span> <span class="p">,</span> <span class="mf">103.53</span><span class="p">]</span>
    <span class="o">-</span> <span class="n">Scale</span> <span class="n">values</span><span class="p">:</span>     <span class="p">[</span><span class="mf">58.395</span><span class="p">,</span> <span class="mf">57.12</span> <span class="p">,</span> <span class="mf">57.375</span><span class="p">]</span>
    <span class="o">-</span> <span class="n">Scale</span> <span class="n">factor</span><span class="p">:</span>     <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Precision</span> <span class="n">of</span> <span class="n">IR</span><span class="p">:</span>  <span class="n">FP16</span>
    <span class="o">-</span> <span class="n">Enable</span> <span class="n">fusing</span><span class="p">:</span>    <span class="kc">True</span>
    <span class="o">-</span> <span class="n">User</span> <span class="n">transformations</span><span class="p">:</span>     <span class="n">Not</span> <span class="n">specified</span>
    <span class="o">-</span> <span class="n">Reverse</span> <span class="nb">input</span> <span class="n">channels</span><span class="p">:</span>   <span class="kc">False</span>
    <span class="o">-</span> <span class="n">Enable</span> <span class="n">IR</span> <span class="n">generation</span> <span class="k">for</span> <span class="n">fixed</span> <span class="nb">input</span> <span class="n">shape</span><span class="p">:</span>   <span class="kc">False</span>
    <span class="o">-</span> <span class="n">Use</span> <span class="n">the</span> <span class="n">transformations</span> <span class="n">config</span> <span class="n">file</span><span class="p">:</span>  <span class="kc">None</span>
<span class="n">Advanced</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="o">-</span> <span class="n">Force</span> <span class="n">the</span> <span class="n">usage</span> <span class="n">of</span> <span class="n">legacy</span> <span class="n">Frontend</span> <span class="n">of</span> <span class="n">Model</span> <span class="n">Optimizer</span> <span class="k">for</span> <span class="n">model</span> <span class="n">conversion</span> <span class="n">into</span> <span class="n">IR</span><span class="p">:</span>   <span class="kc">False</span>
    <span class="o">-</span> <span class="n">Force</span> <span class="n">the</span> <span class="n">usage</span> <span class="n">of</span> <span class="n">new</span> <span class="n">Frontend</span> <span class="n">of</span> <span class="n">Model</span> <span class="n">Optimizer</span> <span class="k">for</span> <span class="n">model</span> <span class="n">conversion</span> <span class="n">into</span> <span class="n">IR</span><span class="p">:</span>  <span class="kc">False</span>
<span class="n">OpenVINO</span> <span class="n">runtime</span> <span class="n">found</span> <span class="ow">in</span><span class="p">:</span>  <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/.</span><span class="n">venv</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">openvino</span>
<span class="n">OpenVINO</span> <span class="n">runtime</span> <span class="n">version</span><span class="p">:</span>   <span class="mf">2022.1.0</span><span class="o">-</span><span class="mi">7019</span><span class="o">-</span><span class="n">cdb9bec7210</span><span class="o">-</span><span class="n">releases</span><span class="o">/</span><span class="mi">2022</span><span class="o">/</span><span class="mi">1</span>
<span class="n">Model</span> <span class="n">Optimizer</span> <span class="n">version</span><span class="p">:</span>    <span class="mf">2022.1.0</span><span class="o">-</span><span class="mi">7019</span><span class="o">-</span><span class="n">cdb9bec7210</span><span class="o">-</span><span class="n">releases</span><span class="o">/</span><span class="mi">2022</span><span class="o">/</span><span class="mi">1</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">Generated</span> <span class="n">IR</span> <span class="n">version</span> <span class="mi">11</span> <span class="n">model</span><span class="o">.</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">XML</span> <span class="n">file</span><span class="p">:</span> <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">112</span><span class="o">-</span><span class="n">pytorch</span><span class="o">-</span><span class="n">post</span><span class="o">-</span><span class="n">training</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">nncf</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">resnet50_int8</span><span class="o">.</span><span class="n">xml</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">BIN</span> <span class="n">file</span><span class="p">:</span> <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">k8sworker</span><span class="o">/</span><span class="n">cibuilds</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">OVNotebookOps</span><span class="o">-</span><span class="mi">231</span><span class="o">/.</span><span class="n">workspace</span><span class="o">/</span><span class="n">scm</span><span class="o">/</span><span class="n">ov</span><span class="o">-</span><span class="n">notebook</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="mi">112</span><span class="o">-</span><span class="n">pytorch</span><span class="o">-</span><span class="n">post</span><span class="o">-</span><span class="n">training</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">nncf</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">resnet50_int8</span><span class="o">.</span><span class="n">bin</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">Total</span> <span class="n">execution</span> <span class="n">time</span><span class="p">:</span> <span class="mf">1.61</span> <span class="n">seconds</span><span class="o">.</span>
<span class="p">[</span> <span class="n">SUCCESS</span> <span class="p">]</span> <span class="n">Memory</span> <span class="n">consumed</span><span class="p">:</span> <span class="mi">270</span> <span class="n">MB</span><span class="o">.</span>
<span class="n">It</span><span class="s1">&#39;s been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&amp;source=prod&amp;campid=ww_2022_bu_IOTG_OpenVINO-2022-1&amp;content=upg_all&amp;medium=organic or on the GitHub*</span>
<span class="p">[</span> <span class="n">INFO</span> <span class="p">]</span> <span class="n">The</span> <span class="n">model</span> <span class="n">was</span> <span class="n">converted</span> <span class="n">to</span> <span class="n">IR</span> <span class="n">v11</span><span class="p">,</span> <span class="n">the</span> <span class="n">latest</span> <span class="n">model</span> <span class="nb">format</span> <span class="n">that</span> <span class="n">corresponds</span> <span class="n">to</span> <span class="n">the</span> <span class="n">source</span> <span class="n">DL</span> <span class="n">framework</span> <span class="nb">input</span><span class="o">/</span><span class="n">output</span> <span class="nb">format</span><span class="o">.</span> <span class="n">While</span> <span class="n">IR</span> <span class="n">v11</span> <span class="ow">is</span> <span class="n">backwards</span> <span class="n">compatible</span> <span class="k">with</span> <span class="n">OpenVINO</span> <span class="n">Inference</span> <span class="n">Engine</span> <span class="n">API</span> <span class="n">v1</span><span class="mf">.0</span><span class="p">,</span> <span class="n">please</span> <span class="n">use</span> <span class="n">API</span> <span class="n">v2</span><span class="mf">.0</span> <span class="p">(</span><span class="k">as</span> <span class="n">of</span> <span class="mf">2022.1</span><span class="p">)</span> <span class="n">to</span> <span class="n">take</span> <span class="n">advantage</span> <span class="n">of</span> <span class="n">the</span> <span class="n">latest</span> <span class="n">improvements</span> <span class="ow">in</span> <span class="n">IR</span> <span class="n">v11</span><span class="o">.</span>
<span class="n">Find</span> <span class="n">more</span> <span class="n">information</span> <span class="n">about</span> <span class="n">API</span> <span class="n">v2</span><span class="mf">.0</span> <span class="ow">and</span> <span class="n">IR</span> <span class="n">v11</span> <span class="n">at</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">docs</span><span class="o">.</span><span class="n">openvino</span><span class="o">.</span><span class="n">ai</span>
</pre></div>
</div>
</section>
<section id="iv-compare-perfomance-of-int8-model-and-fp32-model-in-openvino">
<h2>IV. Compare perfomance of INT8 model and FP32 model in OpenVINO<a class="headerlink" href="#iv-compare-perfomance-of-int8-model-and-fp32-model-in-openvino" title="Permalink to this headline">Â¶</a></h2>
<p>Finally, measure the inference performance of the <code class="docutils literal notranslate"><span class="pre">FP32</span></code> and <code class="docutils literal notranslate"><span class="pre">INT8</span></code>
models, using <a class="reference external" href="https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html">Benchmark
Tool</a>
- an inference performance measurement tool in OpenVINO. By default,
Benchmark Tool runs inference for 60 seconds in asynchronous mode on
CPU. It returns inference speed as latency (milliseconds per image) and
throughput (frames per second) values.</p>
<blockquote>
<div><p><strong>Note</strong>: This notebook runs benchmark_app for 15 seconds to give a
quick indication of performance. For more accurate performance, it is
recommended to run benchmark_app in a terminal/command prompt after
closing other applications. Run <code class="docutils literal notranslate"><span class="pre">benchmark_app</span> <span class="pre">-m</span> <span class="pre">model.xml</span> <span class="pre">-d</span> <span class="pre">CPU</span></code>
to benchmark async inference on CPU for one minute. Change CPU to GPU
to benchmark on GPU. Run <code class="docutils literal notranslate"><span class="pre">benchmark_app</span> <span class="pre">--help</span></code> to see an overview
of all command-line options.</p>
</div></blockquote>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def parse_benchmark_output(benchmark_output: str):
    &quot;&quot;&quot;Prints the output from benchmark_app in human-readable format&quot;&quot;&quot;
    parsed_output = [line for line in benchmark_output if not (line.startswith(r&quot;[&quot;) or line.startswith(&quot;  &quot;) or line == &quot;&quot;)]
    print(*parsed_output, sep=&#39;\n&#39;)


print(&#39;Benchmark FP32 model (OpenVINO IR)&#39;)
benchmark_output = ! benchmark_app -m &quot;$fp32_ir_path&quot; -d CPU -api async -t 15
parse_benchmark_output(benchmark_output)

print(&#39;Benchmark INT8 model (OpenVINO IR)&#39;)
benchmark_output = ! benchmark_app -m &quot;$int8_ir_path&quot; -d CPU -api async -t 15
parse_benchmark_output(benchmark_output)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Benchmark</span> <span class="n">FP32</span> <span class="n">model</span> <span class="p">(</span><span class="n">OpenVINO</span> <span class="n">IR</span><span class="p">)</span>
<span class="n">Count</span><span class="p">:</span>          <span class="mi">16272</span> <span class="n">iterations</span>
<span class="n">Duration</span><span class="p">:</span>       <span class="mf">15007.45</span> <span class="n">ms</span>
<span class="n">Latency</span><span class="p">:</span>
<span class="n">Throughput</span><span class="p">:</span> <span class="mf">1084.26</span> <span class="n">FPS</span>
<span class="n">Benchmark</span> <span class="n">INT8</span> <span class="n">model</span> <span class="p">(</span><span class="n">OpenVINO</span> <span class="n">IR</span><span class="p">)</span>
<span class="n">Count</span><span class="p">:</span>          <span class="mi">47796</span> <span class="n">iterations</span>
<span class="n">Duration</span><span class="p">:</span>       <span class="mf">15002.23</span> <span class="n">ms</span>
<span class="n">Latency</span><span class="p">:</span>
<span class="n">Throughput</span><span class="p">:</span> <span class="mf">3185.93</span> <span class="n">FPS</span>
</pre></div>
</div>
<p>Show CPU Information for reference:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>ie = Core()
ie.get_property(&quot;CPU&quot;, &quot;FULL_DEVICE_NAME&quot;)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;Intel(R) Core(TM) i9-10920X CPU @ 3.50GHz&#39;</span>
</pre></div>
</div>
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  
    <a class='button bttn-sec button-size-l' id="prev-link" href="111-detection-quantization-with-output.html" title="previous page">Prev</a>
    <a class='button bttn-sec button-size-l' id="next-link" href="113-image-classification-quantization-with-output.html" title="next page">Next</a>

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, IntelÂ®.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>